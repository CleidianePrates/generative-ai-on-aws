{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune LLaMA 2 on Amazon SageMaker\n",
    "\n",
    "In this sagemaker example, we are going to learn how to fine-tune [LLaMA 2](https://huggingface.co/meta-llama/Llama-2-70b-hf) using [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314). [LLaMA 2](https://huggingface.co/meta-llama/Llama-2-70b-hf) is the next version of the [LLaMA](https://arxiv.org/abs/2302.13971). Compared to the V1 model, it is trained on more data - 2T tokens and supports context length window upto 4K tokens. Learn more about LLaMa 2 in the [\"\"]() blog post.\n",
    "\n",
    "QLoRA is an efficient finetuning technique that quantizes a pretrained language model to 4 bits and attaches small “Low-Rank Adapters” which are fine-tuned. This enables fine-tuning of models with up to 65 billion parameters on a single GPU; despite its efficiency, QLoRA matches the performance of full-precision fine-tuning and achieves state-of-the-art results on language tasks.\n",
    "\n",
    "In our example, we are going to leverage Hugging Face [Transformers](https://huggingface.co/docs/transformers/index), [Accelerate](https://huggingface.co/docs/accelerate/index), and [PEFT](https://github.com/huggingface/peft). \n",
    "\n",
    "In Detail you will learn how to:\n",
    "1. Setup Development Environment\n",
    "2. Load and prepare the dataset\n",
    "3. Fine-Tune LLaMA 13B with QLoRA on Amazon SageMaker\n",
    "4. Deploy Fine-tuned LLM on Amazon SageMaker\n",
    "\n",
    "### Quick intro: PEFT or Parameter Efficient Fine-tuning\n",
    "\n",
    "[PEFT](https://github.com/huggingface/peft), or Parameter Efficient Fine-tuning, is a new open-source library from Hugging Face to enable efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters. PEFT currently includes techniques for:\n",
    "\n",
    "- (Q)LoRA: [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685.pdf)\n",
    "- Prefix Tuning: [P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/pdf/2110.07602.pdf)\n",
    "- P-Tuning: [GPT Understands, Too](https://arxiv.org/pdf/2103.10385.pdf)\n",
    "- Prompt Tuning: [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)\n",
    "- IA3: [Infused Adapter by Inhibiting and Amplifying Inner Activations](https://arxiv.org/abs/2205.05638)\n",
    "\n",
    "\n",
    "\n",
    "### Access LLaMA 2\n",
    "\n",
    "Before we can start training we have to make sure that we accepted the license of [llama 2](https://huggingface.co/meta-llama/Llama-2-70b-hf) to be able to use it. You can accept the license by clicking on the Agree and access repository button on the model page at: \n",
    "* [LLaMa 7B](https://huggingface.co/meta-llama/Llama-2-7b-hf)\n",
    "* [LLaMa 13B](https://huggingface.co/meta-llama/Llama-2-13b-hf)\n",
    "* [LLaMa 70B](https://huggingface.co/meta-llama/Llama-2-70b-hf)\n",
    "\n",
    "## 1. Setup Development Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install \"transformers==4.31.0\" \"datasets[s3]==2.13.0\" sagemaker --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access any LLaMA 2 asset we need to login into our hugging face account. We can do this by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !huggingface-cli login --token YOUR_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.25.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::079002598131:role/service-role/AmazonSageMaker-ExecutionRole-20220804T150518\n",
      "sagemaker bucket: sagemaker-us-east-1-079002598131\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and prepare the dataset\n",
    "\n",
    "we will use the [dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k) an open source dataset of instruction-following records generated by thousands of Databricks employees in several of the behavioral categories outlined in the [InstructGPT paper](https://arxiv.org/abs/2203.02155), including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization.\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"instruction\": \"What is world of warcraft\",\n",
    "  \"context\": \"\",\n",
    "  \"response\": \"World of warcraft is a massive online multi player role playing game. It was released in 2004 by bizarre entertainment\"\n",
    "}\n",
    "```\n",
    "\n",
    "To load the `samsum` dataset, we use the `load_dataset()` method from the 🤗 Datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/root/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 15011\n",
      "{'instruction': 'What is a mitochondrion?', 'context': '', 'response': 'A mitochondrion is an organelle found in the cells of most eukaryotes, such as animals, plants and fungi. Mitochondria have a double membrane structure and use aerobic respiration to generate adenosine triphosphate (ATP), which is used throughout the cell as a source of chemical energy. They were discovered by Albert von Kölliker in 1857  in the voluntary muscles of insects. The term mitochondrion was coined by Carl Benda in 1898. The mitochondrion is popularly nicknamed the \"powerhouse of the cell\", a phrase coined by Philip Siekevitz in a 1957 article of the same name.\\n\\nSome cells in some multicellular organisms lack mitochondria (for example, mature mammalian red blood cells). A large number of unicellular organisms, such as microsporidia, parabasalids and diplomonads, have reduced or transformed their mitochondria into other structures. One eukaryote, Monocercomonoides, is known to have completely lost its mitochondria, and one multicellular organism, Henneguya salminicola, is known to have retained mitochondrion-related organelles in association with a complete loss of their mitochondrial genome.\\n\\nMitochondria are commonly between 0.75 and 3 μm2 in cross section, but vary considerably in size and structure. Unless specifically stained, they are not visible. In addition to supplying cellular energy, mitochondria are involved in other tasks, such as signaling, cellular differentiation, and cell death, as well as maintaining control of the cell cycle and cell growth. Mitochondrial biogenesis is in turn temporally coordinated with these cellular processes. Mitochondria have been implicated in several human disorders and conditions, such as mitochondrial diseases, cardiac dysfunction, heart failure and autism.\\n\\nThe number of mitochondria in a cell can vary widely by organism, tissue, and cell type. A mature red blood cell has no mitochondria, whereas a liver cell can have more than 2000. The mitochondrion is composed of compartments that carry out specialized functions. These compartments or regions include the outer membrane, intermembrane space, inner membrane, cristae, and matrix.\\n\\nAlthough most of a eukaryotic cell\\'s DNA is contained in the cell nucleus, the mitochondrion has its own genome (\"mitogenome\") that is substantially similar to bacterial genomes. This finding has led to general acceptance of the endosymbiotic hypothesis - that free-living prokaryotic ancestors of modern mitochondria permanently fused with eukaryotic cells in the distant past, evolving such that modern animals, plants, fungi, and other eukaryotes are able to respire to generate cellular energy.', 'category': 'open_qa'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "print(dataset[randrange(len(dataset))])\n",
    "# dataset size: 15011\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instruct tune our model we need to convert our structured examples into a collection of tasks described via instructions. We define a `formatting_function` that takes a sample and returns a string with our format instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dolly(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    context = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
    "    response = f\"### Answer\\n{sample['response']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets test our formatting function on a random example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "How is data engineering different from data science?\n",
      "\n",
      "### Answer\n",
      "Data Engineering occurs before data science in the data lifecycle. Data can be generated from various sources and this raw data may be structured, semi structured or even unstructured. To make sense of this data, data has to be ingested and transformed. This is when data engineering comes into picture. Data engineering includes the process of extracting the raw data, ingesting data into a system like a data lake, transforming the data for business needs and finally loading it into a table. This is commonly known as ELT (Extract-Load-Transform). ETL (Extract-Transform-Load) is also possible where transformation happens before the loading stage.\n",
      "Data Science and Analytics is the next step in the data lifecycle. Once the data needed for specific business need is available, data scientists use this data to run machine learning models to find the most accurate model. This data is available to the data scientists in the form of tables. Data analysts also use this data to do some exploratory analysis and create dashboards. \n",
      "In essence, the data lifecycle would look as follows:\n",
      "Raw data from different sources -> Data Engineering -> Data Science and Analytics\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_dolly(dataset[randrange(len(dataset))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, to formatting our samples we also want to pack multiple samples to one sequence to have a more efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#model_id = \"meta-llama/Llama-2-13b-hf\" # sharded weights, gated\n",
    "model_id = \"NousResearch/Llama-2-7b-hf\" # not gated, TODO: Try 13b\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some helper functions to pack our samples into sequences of a given length and then tokenize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-a9e8aa13e41a7fc8.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-9286c08bce910be1.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "Extract the ingredients required to prepare guacamole from the text. Separate them with a comma.\n",
      "\n",
      "### Context\n",
      "Guacamole is traditionally made by mashing peeled, ripe avocados and salt with a molcajete y tejolote (mortar and pestle). Recipes often call for lime juice, cilantro, onions, and jalapeños. Some non-traditional recipes may call for sour cream, tomatoes, basil, or peas. Due to the presence of polyphenol oxidase in the cells of avocado, exposure to oxygen in the air causes an enzymatic reaction and develops melanoidin pigment, turning the sauce brown. This result is generally considered unappetizing, and there are several methods (some anecdotal) that are used to counter this effect, such as storing the guacamole in an air-tight container or wrapping tightly in plastic to limit the surface area exposed to the air.\n",
      "\n",
      "### Answer\n",
      "Here are the ingredients from the paragraph to prepare Guacamole: Ripe avocados, salt, lime juice, cilantro, onions, and jalapeños. While these are the most commonly used, some non-traditional recipes may use sour cream, tomatoes, basil, or peas.</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-75f16e9e4c95140b.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 1591\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_dolly(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "# apply prompt template per sample\n",
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
    "\n",
    "# print random sample\n",
    "print(dataset[randint(0, len(dataset))][\"text\"])\n",
    "\n",
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "# tokenize and chunk dataset\n",
    "lm_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(lm_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we processed the datasets we are going to use the new [FileSystem integration](https://huggingface.co/docs/datasets/filesystems) to upload our dataset to S3. We are using the `sess.default_bucket()`, adjust this if you want to store the dataset in a different S3 bucket. We will use the S3 path later in our training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1591 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded data to:\n",
      "training dataset to: s3://sagemaker-us-east-1-079002598131/processed/llama/dolly/train\n"
     ]
    }
   ],
   "source": [
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/processed/llama/dolly/train'\n",
    "lm_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(\"uploaded data to:\")\n",
    "print(f\"training dataset to: {training_input_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-Tune LLaMA 13B with QLoRA on Amazon SageMaker\n",
    "\n",
    "We are going to use the recently introduced method in the paper \"[QLoRA: Quantization-aware Low-Rank Adapter Tuning for Language Generation](https://arxiv.org/abs/2106.09685)\" by Tim Dettmers et al. QLoRA is a new technique to reduce the memory footprint of large language models during finetuning, without sacrificing performance. The TL;DR; of how QLoRA works is: \n",
    "\n",
    "* Quantize the pretrained model to 4 bits and freezing it.\n",
    "* Attach small, trainable adapter layers. (LoRA)\n",
    "* Finetune only the adapter layers, while using the frozen quantized model for context.\n",
    "\n",
    "We prepared a [run_clm_qlora.py](./scripts/run_clm_qlora.py), which implements QLora using PEFT to train our model. The script also merges the LoRA weights into the model weights after training. That way you can use the model as a normal model without any additional code. The model will be temporally offloaded to disk, if it is too large to fit into memory.\n",
    "\n",
    "In order to create a sagemaker training job we need an `HuggingFace` Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. The Estimator manages the infrastructure use. \n",
    "SagMaker takes care of starting and managing all the required ec2 instances for us, provides the correct huggingface container, uploads the provided scripts and downloads the data from our S3 bucket into the container at `/opt/ml/input/data`. Then, it starts the training job by running.\n",
    "\n",
    "### Harwarde requirements\n",
    "\n",
    "We also ran several experiments to determine, which instance type can be used for the different model sizes. The following table shows the results of our experiments. The table shows the instance type, model size, context length, and max batch size. \n",
    "\n",
    "| Model        | Instance Type     | Max Batch Size | Context Length |\n",
    "|--------------|-------------------|----------------|----------------|\n",
    "| [LLama 7B]() | `(ml.)g5.4xlarge` | `3`            | `2048`         |\n",
    "| [LLama 13B]() | `(ml.)g5.4xlarge` | `2`            | `2048`         |\n",
    "| [LLama 70B]() | `(ml.)p4d.24xlarge` | `1++` (need to test more configs)            | `2048`         |\n",
    "\n",
    "\n",
    "> You can also use `g5.2xlarge` instead of the `g5.4xlarge` instance type, but then it is not possible to use `merge_weights` parameter, since to merge the LoRA weights into the model weights, the model needs to fit into memory. But you could save the adapter weights and merge them using [merge_adapter_weights.py](./scripts/merge_adapter_weights.py) after training.\n",
    "\n",
    "_Note: We plan to extend this list in the future. feel free to contribute your setup!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'huggingface-qlora-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,                             # pre-trained model\n",
    "  'dataset_path': '/opt/ml/input/data/training',    # path where sagemaker will save training dataset\n",
    "  'epochs': 3,                                      # number of training epochs\n",
    "  'per_device_train_batch_size': 2,                 # batch size for training\n",
    "  'lr': 2e-4,                                       # learning rate used during training\n",
    "#  'hf_token': HfFolder.get_token(),                 # huggingface token to access llama 2\n",
    "  'merge_weights': True,                            # wether to merge LoRA into the model (needs more memory)\n",
    "}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_clm_qlora.py',      # train script\n",
    "    source_dir           = 'scripts',         # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.p4d.24xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.28',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.0',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start our training job, with the `.fit()` method passing our S3 path to the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-qlora-2023-08-31-00-58-09-2023-08-31-00-58-10-028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-31 00:58:10 Starting - Starting the training job...\n",
      "2023-08-31 00:58:20 Pending - Training job waiting for capacity...\n",
      "2023-08-31 00:58:57 Pending - Preparing the instances for training........................\n",
      "2023-08-31 01:02:59 Downloading - Downloading input data...\n",
      "2023-08-31 01:03:19 Training - Downloading the training image..................\n",
      "2023-08-31 01:06:35 Training - Training image download completed. Training in progress........\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-08-31 01:07:45,287 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-08-31 01:07:45,342 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-31 01:07:45,350 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-08-31 01:07:45,351 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-08-31 01:07:46,728 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.31.0 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.4/7.4 MB 81.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting peft==0.4.0 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.4.0-py3-none-any.whl (72 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.9/72.9 kB 23.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting accelerate==0.21.0 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.21.0-py3-none-any.whl (244 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 244.2/244.2 kB 60.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes==0.40.2 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.40.2-py3-none-any.whl (92.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.5/92.5 MB 22.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.1 (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 85.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers>=0.13.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (0.13.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (3.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (0.14.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (2023.5.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 2)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 2)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0->-r requirements.txt (line 1)) (2023.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0->-r requirements.txt (line 1)) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 2)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 2)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 2)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 1)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 1)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 1)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 2)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 2)) (1.3.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: safetensors, bitsandbytes, transformers, accelerate, peft\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.21.0 bitsandbytes-0.40.2 peft-0.4.0 safetensors-0.3.3 transformers-4.31.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.1.2 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-08-31 01:07:57,188 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-08-31 01:07:57,188 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-08-31 01:07:57,270 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-31 01:07:57,334 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-31 01:07:57,397 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-31 01:07:57,406 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"dataset_path\": \"/opt/ml/input/data/training\",\n",
      "        \"epochs\": 3,\n",
      "        \"lr\": 0.0002,\n",
      "        \"merge_weights\": true,\n",
      "        \"model_id\": \"NousResearch/Llama-2-7b-hf\",\n",
      "        \"per_device_train_batch_size\": 2\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-qlora-2023-08-31-00-58-09-2023-08-31-00-58-10-028\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-079002598131/huggingface-qlora-2023-08-31-00-58-09-2023-08-31-00-58-10-028/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_clm_qlora\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_clm_qlora.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"dataset_path\":\"/opt/ml/input/data/training\",\"epochs\":3,\"lr\":0.0002,\"merge_weights\":true,\"model_id\":\"NousResearch/Llama-2-7b-hf\",\"per_device_train_batch_size\":2}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_clm_qlora.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_clm_qlora\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-079002598131/huggingface-qlora-2023-08-31-00-58-09-2023-08-31-00-58-10-028/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"dataset_path\":\"/opt/ml/input/data/training\",\"epochs\":3,\"lr\":0.0002,\"merge_weights\":true,\"model_id\":\"NousResearch/Llama-2-7b-hf\",\"per_device_train_batch_size\":2},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-qlora-2023-08-31-00-58-09-2023-08-31-00-58-10-028\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-079002598131/huggingface-qlora-2023-08-31-00-58-09-2023-08-31-00-58-10-028/source/sourcedir.tar.gz\",\"module_name\":\"run_clm_qlora\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_clm_qlora.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--dataset_path\",\"/opt/ml/input/data/training\",\"--epochs\",\"3\",\"--lr\",\"0.0002\",\"--merge_weights\",\"True\",\"--model_id\",\"NousResearch/Llama-2-7b-hf\",\"--per_device_train_batch_size\",\"2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_PATH=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=3\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.0002\u001b[0m\n",
      "\u001b[34mSM_HP_MERGE_WEIGHTS=true\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=NousResearch/Llama-2-7b-hf\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 run_clm_qlora.py --dataset_path /opt/ml/input/data/training --epochs 3 --lr 0.0002 --merge_weights True --model_id NousResearch/Llama-2-7b-hf --per_device_train_batch_size 2\u001b[0m\n",
      "\u001b[34m2023-08-31 01:07:57,433 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json: 100%|██████████| 583/583 [00:00<00:00, 5.30MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)fetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)fetensors.index.json: 100%|██████████| 26.8k/26.8k [00:00<00:00, 168MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   0%|          | 31.5M/9.98G [00:00<00:44, 224MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   1%|          | 62.9M/9.98G [00:00<00:42, 235MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   1%|          | 94.4M/9.98G [00:00<00:40, 244MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   1%|▏         | 126M/9.98G [00:00<00:44, 223MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   2%|▏         | 157M/9.98G [00:00<00:44, 222MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   2%|▏         | 189M/9.98G [00:00<00:45, 217MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   2%|▏         | 220M/9.98G [00:00<00:42, 232MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   3%|▎         | 252M/9.98G [00:01<00:38, 251MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   3%|▎         | 283M/9.98G [00:01<00:36, 264MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   3%|▎         | 325M/9.98G [00:01<00:32, 295MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   4%|▎         | 357M/9.98G [00:01<00:33, 289MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   4%|▍         | 388M/9.98G [00:01<00:32, 295MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   4%|▍         | 419M/9.98G [00:01<00:32, 295MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   5%|▍         | 461M/9.98G [00:01<00:29, 320MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   5%|▌         | 503M/9.98G [00:01<00:35, 270MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   5%|▌         | 535M/9.98G [00:02<00:35, 266MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   6%|▌         | 577M/9.98G [00:02<00:33, 284MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   6%|▌         | 619M/9.98G [00:02<00:32, 286MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   7%|▋         | 650M/9.98G [00:02<00:34, 269MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   7%|▋         | 682M/9.98G [00:02<00:34, 273MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   7%|▋         | 724M/9.98G [00:02<00:32, 287MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   8%|▊         | 755M/9.98G [00:02<00:33, 279MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   8%|▊         | 786M/9.98G [00:02<00:35, 262MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   8%|▊         | 818M/9.98G [00:03<00:40, 227MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   9%|▊         | 849M/9.98G [00:03<00:40, 225MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   9%|▉         | 881M/9.98G [00:03<00:38, 238MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   9%|▉         | 923M/9.98G [00:03<00:34, 265MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  10%|▉         | 954M/9.98G [00:03<00:36, 247MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  10%|▉         | 996M/9.98G [00:03<00:34, 260MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  10%|█         | 1.03G/9.98G [00:03<00:35, 253MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  11%|█         | 1.07G/9.98G [00:04<00:31, 280MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  11%|█         | 1.10G/9.98G [00:04<00:31, 285MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  11%|█▏        | 1.14G/9.98G [00:04<00:29, 301MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  12%|█▏        | 1.18G/9.98G [00:04<00:26, 327MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  12%|█▏        | 1.24G/9.98G [00:04<00:23, 365MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  13%|█▎        | 1.28G/9.98G [00:04<00:23, 375MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  13%|█▎        | 1.32G/9.98G [00:04<00:23, 372MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  14%|█▎        | 1.36G/9.98G [00:04<00:25, 336MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  14%|█▍        | 1.41G/9.98G [00:05<00:26, 319MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  15%|█▍        | 1.45G/9.98G [00:05<00:29, 291MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  15%|█▍        | 1.48G/9.98G [00:05<00:33, 251MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  15%|█▌        | 1.51G/9.98G [00:05<00:37, 226MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  16%|█▌        | 1.55G/9.98G [00:05<00:33, 255MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  16%|█▌        | 1.58G/9.98G [00:05<00:35, 239MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  16%|█▋        | 1.64G/9.98G [00:05<00:29, 281MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  17%|█▋        | 1.67G/9.98G [00:06<00:29, 278MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  17%|█▋        | 1.70G/9.98G [00:06<00:30, 272MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  17%|█▋        | 1.73G/9.98G [00:06<00:35, 232MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  18%|█▊        | 1.77G/9.98G [00:06<00:32, 251MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  18%|█▊        | 1.80G/9.98G [00:06<00:30, 264MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  18%|█▊        | 1.85G/9.98G [00:06<00:28, 287MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  19%|█▉        | 1.88G/9.98G [00:06<00:27, 293MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  19%|█▉        | 1.91G/9.98G [00:06<00:27, 295MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  20%|█▉        | 1.96G/9.98G [00:07<00:25, 317MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  20%|██        | 2.00G/9.98G [00:07<00:24, 320MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  20%|██        | 2.04G/9.98G [00:07<00:25, 307MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  21%|██        | 2.08G/9.98G [00:07<00:26, 297MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  21%|██        | 2.12G/9.98G [00:07<00:25, 306MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  22%|██▏       | 2.16G/9.98G [00:07<00:23, 326MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  22%|██▏       | 2.20G/9.98G [00:07<00:22, 342MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  22%|██▏       | 2.24G/9.98G [00:08<00:25, 309MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  23%|██▎       | 2.29G/9.98G [00:08<00:26, 295MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  23%|██▎       | 2.32G/9.98G [00:08<00:26, 284MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  24%|██▎       | 2.36G/9.98G [00:08<00:27, 280MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  24%|██▍       | 2.39G/9.98G [00:08<00:28, 269MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  24%|██▍       | 2.42G/9.98G [00:08<00:27, 278MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  25%|██▍       | 2.45G/9.98G [00:08<00:31, 237MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  25%|██▍       | 2.49G/9.98G [00:09<00:30, 246MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  25%|██▌       | 2.52G/9.98G [00:09<00:28, 259MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  26%|██▌       | 2.55G/9.98G [00:09<00:36, 205MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  26%|██▌       | 2.58G/9.98G [00:09<00:34, 217MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  26%|██▌       | 2.61G/9.98G [00:09<00:32, 226MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  27%|██▋       | 2.65G/9.98G [00:09<00:28, 261MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  27%|██▋       | 2.69G/9.98G [00:09<00:26, 271MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  27%|██▋       | 2.73G/9.98G [00:09<00:26, 275MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  28%|██▊       | 2.77G/9.98G [00:10<00:23, 302MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  28%|██▊       | 2.82G/9.98G [00:10<00:21, 338MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  29%|██▊       | 2.86G/9.98G [00:10<00:20, 350MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  29%|██▉       | 2.90G/9.98G [00:10<00:24, 294MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  29%|██▉       | 2.94G/9.98G [00:10<00:25, 278MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  30%|██▉       | 2.97G/9.98G [00:10<00:27, 258MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  30%|███       | 3.00G/9.98G [00:10<00:29, 238MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  30%|███       | 3.03G/9.98G [00:11<00:27, 252MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  31%|███       | 3.07G/9.98G [00:11<00:24, 279MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  31%|███       | 3.10G/9.98G [00:11<00:25, 269MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  32%|███▏      | 3.15G/9.98G [00:11<00:22, 306MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  32%|███▏      | 3.19G/9.98G [00:11<00:23, 288MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  32%|███▏      | 3.23G/9.98G [00:11<00:23, 290MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  33%|███▎      | 3.26G/9.98G [00:11<00:24, 272MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  33%|███▎      | 3.29G/9.98G [00:12<00:29, 227MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  33%|███▎      | 3.32G/9.98G [00:12<00:28, 233MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  34%|███▎      | 3.36G/9.98G [00:12<00:32, 207MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  34%|███▍      | 3.39G/9.98G [00:12<00:32, 203MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  34%|███▍      | 3.43G/9.98G [00:12<00:26, 243MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  35%|███▍      | 3.46G/9.98G [00:12<00:25, 256MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  35%|███▍      | 3.49G/9.98G [00:13<00:39, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  35%|███▌      | 3.52G/9.98G [00:13<00:37, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  36%|███▌      | 3.55G/9.98G [00:13<00:35, 182MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  36%|███▌      | 3.59G/9.98G [00:13<00:35, 182MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  36%|███▋      | 3.62G/9.98G [00:13<00:31, 205MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  37%|███▋      | 3.66G/9.98G [00:13<00:27, 231MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  37%|███▋      | 3.70G/9.98G [00:13<00:24, 253MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  38%|███▊      | 3.74G/9.98G [00:14<00:22, 274MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  38%|███▊      | 3.77G/9.98G [00:14<00:25, 242MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  38%|███▊      | 3.81G/9.98G [00:14<00:24, 255MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  38%|███▊      | 3.84G/9.98G [00:14<00:23, 257MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  39%|███▉      | 3.88G/9.98G [00:14<00:22, 266MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  39%|███▉      | 3.91G/9.98G [00:14<00:22, 272MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  40%|███▉      | 3.94G/9.98G [00:14<00:22, 269MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  40%|███▉      | 3.97G/9.98G [00:14<00:21, 275MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  40%|████      | 4.01G/9.98G [00:15<00:23, 253MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  40%|████      | 4.04G/9.98G [00:15<00:23, 254MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  41%|████      | 4.08G/9.98G [00:15<00:21, 280MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  41%|████      | 4.11G/9.98G [00:15<00:21, 270MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  42%|████▏     | 4.15G/9.98G [00:15<00:20, 280MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  42%|████▏     | 4.18G/9.98G [00:15<00:20, 276MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  42%|████▏     | 4.22G/9.98G [00:15<00:21, 263MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  43%|████▎     | 4.25G/9.98G [00:16<00:21, 266MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  43%|████▎     | 4.29G/9.98G [00:16<00:20, 275MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  43%|████▎     | 4.32G/9.98G [00:16<00:21, 266MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  44%|████▎     | 4.36G/9.98G [00:16<00:19, 283MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  44%|████▍     | 4.39G/9.98G [00:16<00:19, 281MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  44%|████▍     | 4.42G/9.98G [00:16<00:19, 280MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  45%|████▍     | 4.46G/9.98G [00:16<00:28, 196MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  45%|████▍     | 4.49G/9.98G [00:17<00:28, 194MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  45%|████▌     | 4.52G/9.98G [00:17<00:26, 206MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  46%|████▌     | 4.55G/9.98G [00:17<00:29, 186MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  46%|████▌     | 4.58G/9.98G [00:17<00:27, 198MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  46%|████▌     | 4.61G/9.98G [00:17<00:24, 222MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  47%|████▋     | 4.65G/9.98G [00:17<00:23, 225MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  47%|████▋     | 4.68G/9.98G [00:17<00:21, 244MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  47%|████▋     | 4.71G/9.98G [00:18<00:20, 252MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  48%|████▊     | 4.74G/9.98G [00:18<00:35, 149MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  48%|████▊     | 4.76G/9.98G [00:18<00:33, 155MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  48%|████▊     | 4.80G/9.98G [00:18<00:26, 192MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  48%|████▊     | 4.83G/9.98G [00:18<00:24, 206MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  49%|████▉     | 4.87G/9.98G [00:19<00:27, 187MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  49%|████▉     | 4.90G/9.98G [00:19<00:24, 203MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  49%|████▉     | 4.93G/9.98G [00:19<00:22, 225MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  50%|████▉     | 4.96G/9.98G [00:19<00:20, 244MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  50%|█████     | 4.99G/9.98G [00:19<00:19, 254MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  50%|█████     | 5.02G/9.98G [00:19<00:19, 254MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  51%|█████     | 5.05G/9.98G [00:19<00:19, 250MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  51%|█████     | 5.10G/9.98G [00:19<00:17, 282MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  51%|█████▏    | 5.13G/9.98G [00:19<00:17, 283MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  52%|█████▏    | 5.16G/9.98G [00:20<00:17, 281MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  52%|█████▏    | 5.19G/9.98G [00:20<00:18, 266MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  52%|█████▏    | 5.23G/9.98G [00:20<00:16, 282MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  53%|█████▎    | 5.26G/9.98G [00:20<00:17, 263MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  53%|█████▎    | 5.30G/9.98G [00:20<00:19, 238MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  53%|█████▎    | 5.33G/9.98G [00:20<00:18, 254MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  54%|█████▎    | 5.36G/9.98G [00:20<00:21, 217MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  54%|█████▍    | 5.40G/9.98G [00:21<00:20, 225MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  54%|█████▍    | 5.43G/9.98G [00:21<00:20, 223MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  55%|█████▍    | 5.46G/9.98G [00:21<00:19, 236MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  55%|█████▌    | 5.49G/9.98G [00:21<00:24, 180MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  55%|█████▌    | 5.54G/9.98G [00:21<00:20, 219MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  56%|█████▌    | 5.58G/9.98G [00:21<00:18, 243MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  56%|█████▌    | 5.61G/9.98G [00:22<00:18, 233MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  57%|█████▋    | 5.65G/9.98G [00:22<00:16, 268MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  57%|█████▋    | 5.68G/9.98G [00:22<00:16, 257MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  57%|█████▋    | 5.73G/9.98G [00:22<00:15, 271MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  58%|█████▊    | 5.76G/9.98G [00:22<00:15, 266MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  58%|█████▊    | 5.79G/9.98G [00:22<00:17, 246MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  58%|█████▊    | 5.82G/9.98G [00:22<00:16, 259MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  59%|█████▉    | 5.86G/9.98G [00:22<00:15, 271MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  59%|█████▉    | 5.89G/9.98G [00:23<00:14, 278MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  59%|█████▉    | 5.93G/9.98G [00:23<00:13, 294MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  60%|█████▉    | 5.97G/9.98G [00:23<00:17, 232MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  60%|██████    | 6.00G/9.98G [00:23<00:17, 227MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  60%|██████    | 6.03G/9.98G [00:23<00:16, 235MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  61%|██████    | 6.06G/9.98G [00:23<00:15, 252MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  61%|██████    | 6.10G/9.98G [00:23<00:13, 284MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  61%|██████▏   | 6.13G/9.98G [00:23<00:13, 276MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  62%|██████▏   | 6.17G/9.98G [00:24<00:13, 276MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  62%|██████▏   | 6.20G/9.98G [00:24<00:13, 283MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  62%|██████▏   | 6.23G/9.98G [00:24<00:13, 285MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  63%|██████▎   | 6.27G/9.98G [00:24<00:12, 302MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  63%|██████▎   | 6.31G/9.98G [00:24<00:11, 310MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  64%|██████▎   | 6.34G/9.98G [00:24<00:12, 288MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  64%|██████▍   | 6.38G/9.98G [00:24<00:12, 283MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  64%|██████▍   | 6.41G/9.98G [00:24<00:12, 287MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  65%|██████▍   | 6.45G/9.98G [00:25<00:12, 287MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  65%|██████▍   | 6.48G/9.98G [00:25<00:13, 252MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  65%|██████▌   | 6.51G/9.98G [00:25<00:13, 264MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  66%|██████▌   | 6.54G/9.98G [00:25<00:12, 271MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  66%|██████▌   | 6.57G/9.98G [00:25<00:13, 249MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  66%|██████▌   | 6.61G/9.98G [00:25<00:15, 219MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  67%|██████▋   | 6.64G/9.98G [00:25<00:14, 236MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  67%|██████▋   | 6.67G/9.98G [00:26<00:13, 246MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  67%|██████▋   | 6.70G/9.98G [00:26<00:13, 252MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  67%|██████▋   | 6.73G/9.98G [00:26<00:12, 265MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  68%|██████▊   | 6.77G/9.98G [00:26<00:10, 297MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  68%|██████▊   | 6.82G/9.98G [00:26<00:10, 308MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  69%|██████▊   | 6.86G/9.98G [00:26<00:10, 309MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  69%|██████▉   | 6.89G/9.98G [00:26<00:11, 269MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  69%|██████▉   | 6.92G/9.98G [00:27<00:17, 176MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  70%|██████▉   | 6.95G/9.98G [00:27<00:19, 153MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  70%|██████▉   | 6.98G/9.98G [00:27<00:16, 177MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  70%|███████   | 7.01G/9.98G [00:27<00:15, 196MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  71%|███████   | 7.05G/9.98G [00:27<00:14, 201MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  71%|███████   | 7.08G/9.98G [00:27<00:13, 210MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  71%|███████▏  | 7.11G/9.98G [00:27<00:12, 232MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  72%|███████▏  | 7.15G/9.98G [00:28<00:10, 262MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  72%|███████▏  | 7.18G/9.98G [00:28<00:11, 234MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  72%|███████▏  | 7.21G/9.98G [00:28<00:12, 219MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  73%|███████▎  | 7.25G/9.98G [00:28<00:12, 219MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  73%|███████▎  | 7.28G/9.98G [00:28<00:11, 239MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  73%|███████▎  | 7.31G/9.98G [00:28<00:10, 249MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  74%|███████▎  | 7.34G/9.98G [00:28<00:10, 250MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  74%|███████▍  | 7.38G/9.98G [00:29<00:09, 278MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  74%|███████▍  | 7.42G/9.98G [00:29<00:08, 302MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  75%|███████▍  | 7.46G/9.98G [00:29<00:08, 291MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  75%|███████▌  | 7.49G/9.98G [00:29<00:08, 290MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  75%|███████▌  | 7.52G/9.98G [00:29<00:08, 293MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  76%|███████▌  | 7.56G/9.98G [00:29<00:07, 319MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  76%|███████▌  | 7.60G/9.98G [00:29<00:07, 309MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  77%|███████▋  | 7.63G/9.98G [00:29<00:08, 293MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  77%|███████▋  | 7.68G/9.98G [00:29<00:07, 314MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  77%|███████▋  | 7.72G/9.98G [00:30<00:07, 321MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  78%|███████▊  | 7.76G/9.98G [00:30<00:08, 263MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  78%|███████▊  | 7.79G/9.98G [00:30<00:08, 273MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  78%|███████▊  | 7.82G/9.98G [00:30<00:07, 270MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  79%|███████▉  | 7.87G/9.98G [00:30<00:06, 311MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  79%|███████▉  | 7.92G/9.98G [00:30<00:07, 266MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  80%|███████▉  | 7.96G/9.98G [00:31<00:07, 286MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  80%|████████  | 8.00G/9.98G [00:31<00:06, 296MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  81%|████████  | 8.03G/9.98G [00:31<00:06, 285MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  81%|████████  | 8.06G/9.98G [00:31<00:06, 278MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  81%|████████  | 8.10G/9.98G [00:31<00:07, 248MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  81%|████████▏ | 8.13G/9.98G [00:31<00:09, 188MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  82%|████████▏ | 8.16G/9.98G [00:31<00:08, 203MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  82%|████████▏ | 8.19G/9.98G [00:32<00:09, 188MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  82%|████████▏ | 8.22G/9.98G [00:32<00:08, 199MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  83%|████████▎ | 8.25G/9.98G [00:32<00:08, 214MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  83%|████████▎ | 8.28G/9.98G [00:32<00:07, 233MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  83%|████████▎ | 8.33G/9.98G [00:32<00:06, 267MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  84%|████████▍ | 8.37G/9.98G [00:32<00:05, 275MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  84%|████████▍ | 8.40G/9.98G [00:32<00:06, 255MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  85%|████████▍ | 8.45G/9.98G [00:33<00:05, 294MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  85%|████████▌ | 8.48G/9.98G [00:33<00:05, 295MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  85%|████████▌ | 8.51G/9.98G [00:33<00:05, 261MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  86%|████████▌ | 8.56G/9.98G [00:33<00:05, 275MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  86%|████████▌ | 8.60G/9.98G [00:33<00:04, 279MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  87%|████████▋ | 8.63G/9.98G [00:33<00:04, 285MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  87%|████████▋ | 8.67G/9.98G [00:33<00:04, 309MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  87%|████████▋ | 8.71G/9.98G [00:33<00:03, 323MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  88%|████████▊ | 8.76G/9.98G [00:34<00:03, 325MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  88%|████████▊ | 8.80G/9.98G [00:34<00:03, 303MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  88%|████████▊ | 8.83G/9.98G [00:34<00:03, 303MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  89%|████████▉ | 8.87G/9.98G [00:34<00:03, 297MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  89%|████████▉ | 8.90G/9.98G [00:34<00:03, 300MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  90%|████████▉ | 8.93G/9.98G [00:34<00:03, 296MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  90%|████████▉ | 8.97G/9.98G [00:34<00:03, 290MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  90%|█████████ | 9.00G/9.98G [00:34<00:03, 286MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  91%|█████████ | 9.04G/9.98G [00:35<00:03, 311MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  91%|█████████ | 9.08G/9.98G [00:35<00:02, 329MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  91%|█████████▏| 9.12G/9.98G [00:35<00:02, 328MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  92%|█████████▏| 9.16G/9.98G [00:35<00:02, 339MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  92%|█████████▏| 9.21G/9.98G [00:35<00:02, 311MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  93%|█████████▎| 9.25G/9.98G [00:35<00:02, 329MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  93%|█████████▎| 9.29G/9.98G [00:35<00:02, 332MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  94%|█████████▎| 9.33G/9.98G [00:35<00:01, 342MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  94%|█████████▍| 9.37G/9.98G [00:36<00:01, 324MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  94%|█████████▍| 9.42G/9.98G [00:36<00:01, 320MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  95%|█████████▍| 9.46G/9.98G [00:36<00:01, 321MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  95%|█████████▌| 9.50G/9.98G [00:36<00:01, 335MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  96%|█████████▌| 9.55G/9.98G [00:36<00:01, 367MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  96%|█████████▌| 9.59G/9.98G [00:36<00:01, 341MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  97%|█████████▋| 9.64G/9.98G [00:36<00:01, 327MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  97%|█████████▋| 9.68G/9.98G [00:37<00:01, 263MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  97%|█████████▋| 9.71G/9.98G [00:37<00:01, 254MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  98%|█████████▊| 9.74G/9.98G [00:37<00:00, 267MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  98%|█████████▊| 9.78G/9.98G [00:37<00:00, 271MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  98%|█████████▊| 9.81G/9.98G [00:37<00:00, 255MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  99%|█████████▊| 9.85G/9.98G [00:37<00:00, 257MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  99%|█████████▉| 9.88G/9.98G [00:37<00:00, 269MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  99%|█████████▉| 9.91G/9.98G [00:37<00:00, 279MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors: 100%|█████████▉| 9.94G/9.98G [00:38<00:00, 271MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors: 100%|██████████| 9.98G/9.98G [00:38<00:00, 262MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:38<00:38, 38.18s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   1%|          | 41.9M/3.50G [00:00<00:10, 345MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   2%|▏         | 83.9M/3.50G [00:00<00:09, 371MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   4%|▎         | 126M/3.50G [00:00<00:10, 312MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   5%|▍         | 168M/3.50G [00:00<00:12, 261MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   6%|▌         | 199M/3.50G [00:00<00:12, 260MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   7%|▋         | 231M/3.50G [00:00<00:14, 228MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   8%|▊         | 273M/3.50G [00:01<00:12, 251MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   9%|▊         | 304M/3.50G [00:01<00:12, 264MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  10%|▉         | 336M/3.50G [00:01<00:12, 252MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  10%|█         | 367M/3.50G [00:01<00:12, 254MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  11%|█▏        | 398M/3.50G [00:01<00:13, 230MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  12%|█▏        | 430M/3.50G [00:01<00:13, 228MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  13%|█▎        | 461M/3.50G [00:01<00:13, 230MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  14%|█▍        | 503M/3.50G [00:01<00:11, 261MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  15%|█▌        | 535M/3.50G [00:02<00:11, 260MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  16%|█▌        | 566M/3.50G [00:02<00:11, 266MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  17%|█▋        | 598M/3.50G [00:02<00:10, 273MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  18%|█▊        | 640M/3.50G [00:02<00:09, 309MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  19%|█▉        | 682M/3.50G [00:02<00:10, 264MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  20%|██        | 713M/3.50G [00:02<00:11, 245MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  22%|██▏       | 755M/3.50G [00:02<00:10, 273MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  23%|██▎       | 797M/3.50G [00:03<00:09, 291MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  24%|██▍       | 839M/3.50G [00:03<00:08, 300MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  25%|██▍       | 870M/3.50G [00:03<00:09, 277MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  26%|██▌       | 902M/3.50G [00:03<00:09, 268MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  27%|██▋       | 933M/3.50G [00:03<00:09, 271MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  28%|██▊       | 965M/3.50G [00:03<00:09, 281MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  28%|██▊       | 996M/3.50G [00:03<00:08, 285MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  30%|██▉       | 1.04G/3.50G [00:03<00:08, 296MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  31%|███       | 1.07G/3.50G [00:03<00:08, 297MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  31%|███▏      | 1.10G/3.50G [00:04<00:09, 250MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  32%|███▏      | 1.13G/3.50G [00:04<00:10, 234MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  33%|███▎      | 1.16G/3.50G [00:04<00:11, 210MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  34%|███▍      | 1.21G/3.50G [00:04<00:09, 237MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  35%|███▌      | 1.24G/3.50G [00:04<00:09, 247MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  36%|███▌      | 1.27G/3.50G [00:04<00:11, 194MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  37%|███▋      | 1.30G/3.50G [00:05<00:11, 187MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  38%|███▊      | 1.32G/3.50G [00:05<00:11, 190MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  39%|███▊      | 1.35G/3.50G [00:05<00:10, 209MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  40%|███▉      | 1.38G/3.50G [00:05<00:09, 216MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  40%|████      | 1.42G/3.50G [00:05<00:09, 226MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  41%|████▏     | 1.45G/3.50G [00:05<00:08, 240MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  42%|████▏     | 1.48G/3.50G [00:05<00:07, 254MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  43%|████▎     | 1.52G/3.50G [00:05<00:07, 273MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  44%|████▍     | 1.55G/3.50G [00:06<00:07, 276MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  45%|████▌     | 1.58G/3.50G [00:06<00:06, 285MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  47%|████▋     | 1.64G/3.50G [00:06<00:06, 309MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  48%|████▊     | 1.67G/3.50G [00:06<00:06, 289MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  49%|████▉     | 1.71G/3.50G [00:06<00:06, 290MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  50%|████▉     | 1.74G/3.50G [00:06<00:06, 253MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  51%|█████     | 1.78G/3.50G [00:06<00:06, 271MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  52%|█████▏    | 1.81G/3.50G [00:07<00:06, 270MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  53%|█████▎    | 1.85G/3.50G [00:07<00:06, 260MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  54%|█████▎    | 1.88G/3.50G [00:07<00:06, 233MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  55%|█████▍    | 1.91G/3.50G [00:07<00:07, 223MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  55%|█████▌    | 1.94G/3.50G [00:07<00:06, 236MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  56%|█████▋    | 1.97G/3.50G [00:07<00:06, 252MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  57%|█████▋    | 2.00G/3.50G [00:07<00:05, 267MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  58%|█████▊    | 2.03G/3.50G [00:07<00:05, 275MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  59%|█████▉    | 2.07G/3.50G [00:08<00:05, 264MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  60%|█████▉    | 2.10G/3.50G [00:08<00:05, 262MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  61%|██████    | 2.14G/3.50G [00:08<00:04, 272MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  62%|██████▏   | 2.17G/3.50G [00:08<00:04, 275MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  63%|██████▎   | 2.20G/3.50G [00:08<00:06, 208MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  64%|██████▍   | 2.23G/3.50G [00:08<00:07, 174MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  64%|██████▍   | 2.25G/3.50G [00:09<00:06, 178MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  65%|██████▌   | 2.29G/3.50G [00:09<00:06, 198MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  67%|██████▋   | 2.33G/3.50G [00:09<00:04, 241MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  67%|██████▋   | 2.36G/3.50G [00:09<00:04, 237MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  69%|██████▊   | 2.40G/3.50G [00:09<00:04, 270MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  69%|██████▉   | 2.43G/3.50G [00:09<00:04, 240MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  70%|███████   | 2.46G/3.50G [00:09<00:04, 255MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  71%|███████▏  | 2.50G/3.50G [00:09<00:03, 258MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  72%|███████▏  | 2.53G/3.50G [00:10<00:03, 246MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  73%|███████▎  | 2.56G/3.50G [00:10<00:04, 194MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  74%|███████▍  | 2.59G/3.50G [00:10<00:05, 169MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  75%|███████▍  | 2.61G/3.50G [00:10<00:05, 151MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  75%|███████▌  | 2.63G/3.50G [00:11<00:07, 123MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  76%|███████▌  | 2.65G/3.50G [00:11<00:06, 124MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  76%|███████▋  | 2.67G/3.50G [00:11<00:05, 138MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  77%|███████▋  | 2.71G/3.50G [00:11<00:04, 168MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  78%|███████▊  | 2.73G/3.50G [00:11<00:04, 176MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  79%|███████▉  | 2.76G/3.50G [00:11<00:03, 206MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  80%|███████▉  | 2.79G/3.50G [00:11<00:03, 213MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  81%|████████  | 2.82G/3.50G [00:11<00:03, 196MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  81%|████████▏ | 2.85G/3.50G [00:12<00:02, 222MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  83%|████████▎ | 2.89G/3.50G [00:12<00:02, 256MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  84%|████████▎ | 2.93G/3.50G [00:12<00:02, 261MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  85%|████████▍ | 2.97G/3.50G [00:12<00:01, 275MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  86%|████████▌ | 3.00G/3.50G [00:12<00:01, 280MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  87%|████████▋ | 3.04G/3.50G [00:12<00:01, 295MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  88%|████████▊ | 3.07G/3.50G [00:12<00:01, 290MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  89%|████████▊ | 3.10G/3.50G [00:12<00:01, 293MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  90%|████████▉ | 3.14G/3.50G [00:12<00:01, 279MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  90%|█████████ | 3.17G/3.50G [00:13<00:01, 191MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  92%|█████████▏| 3.21G/3.50G [00:13<00:01, 225MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  93%|█████████▎| 3.24G/3.50G [00:13<00:01, 228MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  93%|█████████▎| 3.27G/3.50G [00:13<00:00, 235MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  94%|█████████▍| 3.30G/3.50G [00:13<00:00, 230MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  95%|█████████▌| 3.33G/3.50G [00:13<00:00, 235MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  96%|█████████▌| 3.37G/3.50G [00:14<00:00, 247MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  97%|█████████▋| 3.40G/3.50G [00:14<00:00, 246MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  98%|█████████▊| 3.44G/3.50G [00:14<00:00, 192MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  99%|█████████▉| 3.48G/3.50G [00:14<00:00, 229MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors: 100%|██████████| 3.50G/3.50G [00:14<00:00, 238MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:52<00:00, 24.38s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:52<00:00, 26.45s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.97s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.88s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.04s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)neration_config.json:   0%|          | 0.00/179 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)neration_config.json: 100%|██████████| 179/179 [00:00<00:00, 2.00MB/s]\u001b[0m\n",
      "\u001b[34mFound 7 modules to quantize: ['up_proj', 'v_proj', 'k_proj', 'o_proj', 'down_proj', 'gate_proj', 'q_proj']\u001b[0m\n",
      "\u001b[34mtrainable params: 159,907,840 || all params: 3,660,320,768 || trainable%: 4.368683788535114\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m0%|          | 0/2388 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 1/2388 [00:03<2:30:44,  3.79s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 2/2388 [00:06<2:07:13,  3.20s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 3/2388 [00:09<1:59:35,  3.01s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 4/2388 [00:12<1:55:58,  2.92s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 5/2388 [00:14<1:53:59,  2.87s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 6/2388 [00:17<1:52:45,  2.84s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 7/2388 [00:20<1:51:56,  2.82s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 8/2388 [00:23<1:51:23,  2.81s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 9/2388 [00:26<1:51:00,  2.80s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 10/2388 [00:28<1:50:44,  2.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6601, 'learning_rate': 0.00019916247906197655, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m0%|          | 10/2388 [00:28<1:50:44,  2.79s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 11/2388 [00:31<1:50:33,  2.79s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 12/2388 [00:34<1:50:23,  2.79s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 13/2388 [00:37<1:50:15,  2.79s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 14/2388 [00:39<1:50:10,  2.78s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 15/2388 [00:42<1:50:06,  2.78s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 16/2388 [00:45<1:50:02,  2.78s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 17/2388 [00:48<1:49:59,  2.78s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 18/2388 [00:51<1:49:55,  2.78s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 19/2388 [00:53<1:49:51,  2.78s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 20/2388 [00:56<1:49:48,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5604, 'learning_rate': 0.0001983249581239531, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m1%|          | 20/2388 [00:56<1:49:48,  2.78s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 21/2388 [00:59<1:49:45,  2.78s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 22/2388 [01:02<1:49:43,  2.78s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 23/2388 [01:04<1:49:40,  2.78s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 24/2388 [01:07<1:49:37,  2.78s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 25/2388 [01:10<1:49:33,  2.78s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 26/2388 [01:13<1:49:33,  2.78s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 27/2388 [01:16<1:49:31,  2.78s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 28/2388 [01:18<1:49:27,  2.78s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 29/2388 [01:21<1:49:23,  2.78s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 30/2388 [01:24<1:49:19,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4998, 'learning_rate': 0.00019748743718592964, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m1%|▏         | 30/2388 [01:24<1:49:19,  2.78s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 31/2388 [01:27<1:49:16,  2.78s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 32/2388 [01:30<1:49:13,  2.78s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 33/2388 [01:32<1:49:10,  2.78s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 34/2388 [01:35<1:49:06,  2.78s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 35/2388 [01:38<1:49:03,  2.78s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 36/2388 [01:41<1:49:01,  2.78s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 37/2388 [01:43<1:48:58,  2.78s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 38/2388 [01:46<1:48:55,  2.78s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 39/2388 [01:49<1:48:53,  2.78s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 40/2388 [01:52<1:48:50,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4492, 'learning_rate': 0.0001966499162479062, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m2%|▏         | 40/2388 [01:52<1:48:50,  2.78s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 41/2388 [01:55<1:48:48,  2.78s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 42/2388 [01:57<1:48:44,  2.78s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 43/2388 [02:00<1:48:42,  2.78s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 44/2388 [02:03<1:48:39,  2.78s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 45/2388 [02:06<1:48:37,  2.78s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 46/2388 [02:08<1:48:34,  2.78s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 47/2388 [02:11<1:48:31,  2.78s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 48/2388 [02:14<1:48:31,  2.78s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 49/2388 [02:17<1:48:29,  2.78s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 50/2388 [02:20<1:48:25,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4626, 'learning_rate': 0.00019581239530988274, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m2%|▏         | 50/2388 [02:20<1:48:25,  2.78s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 51/2388 [02:22<1:48:22,  2.78s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 52/2388 [02:25<1:48:19,  2.78s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 53/2388 [02:28<1:48:15,  2.78s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 54/2388 [02:31<1:48:13,  2.78s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 55/2388 [02:34<1:48:09,  2.78s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 56/2388 [02:36<1:48:06,  2.78s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 57/2388 [02:39<1:48:03,  2.78s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 58/2388 [02:42<1:48:00,  2.78s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 59/2388 [02:45<1:47:57,  2.78s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 60/2388 [02:47<1:47:54,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3797, 'learning_rate': 0.0001949748743718593, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m3%|▎         | 60/2388 [02:47<1:47:54,  2.78s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 61/2388 [02:50<1:47:52,  2.78s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 62/2388 [02:53<1:47:49,  2.78s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 63/2388 [02:56<1:47:46,  2.78s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 64/2388 [02:59<1:47:43,  2.78s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 65/2388 [03:01<1:47:42,  2.78s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 66/2388 [03:04<1:47:39,  2.78s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 67/2388 [03:07<1:47:36,  2.78s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 68/2388 [03:10<1:47:33,  2.78s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 69/2388 [03:12<1:47:32,  2.78s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 70/2388 [03:15<1:47:31,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4137, 'learning_rate': 0.00019413735343383584, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m3%|▎         | 70/2388 [03:15<1:47:31,  2.78s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 71/2388 [03:18<1:47:28,  2.78s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 72/2388 [03:21<1:47:24,  2.78s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 73/2388 [03:24<1:47:20,  2.78s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 74/2388 [03:26<1:47:18,  2.78s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 75/2388 [03:29<1:47:14,  2.78s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 76/2388 [03:32<1:47:11,  2.78s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 77/2388 [03:35<1:47:08,  2.78s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 78/2388 [03:37<1:47:05,  2.78s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 79/2388 [03:40<1:47:03,  2.78s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 80/2388 [03:43<1:47:00,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4349, 'learning_rate': 0.0001932998324958124, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m3%|▎         | 80/2388 [03:43<1:47:00,  2.78s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 81/2388 [03:46<1:46:57,  2.78s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 82/2388 [03:49<1:46:54,  2.78s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 83/2388 [03:51<1:46:52,  2.78s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 84/2388 [03:54<1:46:49,  2.78s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 85/2388 [03:57<1:46:47,  2.78s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 86/2388 [04:00<1:46:44,  2.78s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 87/2388 [04:03<1:46:42,  2.78s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 88/2388 [04:05<1:46:40,  2.78s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 89/2388 [04:08<1:46:37,  2.78s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 90/2388 [04:11<1:46:35,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4205, 'learning_rate': 0.00019246231155778894, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m4%|▍         | 90/2388 [04:11<1:46:35,  2.78s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 91/2388 [04:14<1:46:33,  2.78s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 92/2388 [04:16<1:46:30,  2.78s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 93/2388 [04:19<1:46:27,  2.78s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 94/2388 [04:22<1:46:24,  2.78s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 95/2388 [04:25<1:46:22,  2.78s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 96/2388 [04:28<1:46:18,  2.78s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 97/2388 [04:30<1:46:14,  2.78s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 98/2388 [04:33<1:46:11,  2.78s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 99/2388 [04:36<1:46:08,  2.78s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 100/2388 [04:39<1:46:05,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3758, 'learning_rate': 0.0001916247906197655, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m4%|▍         | 100/2388 [04:39<1:46:05,  2.78s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 101/2388 [04:41<1:46:03,  2.78s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 102/2388 [04:44<1:45:59,  2.78s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 103/2388 [04:47<1:45:56,  2.78s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 104/2388 [04:50<1:45:53,  2.78s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 105/2388 [04:53<1:45:49,  2.78s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 106/2388 [04:55<1:45:47,  2.78s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 107/2388 [04:58<1:45:44,  2.78s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 108/2388 [05:01<1:45:42,  2.78s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 109/2388 [05:04<1:45:40,  2.78s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 110/2388 [05:07<1:45:37,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3356, 'learning_rate': 0.00019078726968174204, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m5%|▍         | 110/2388 [05:07<1:45:37,  2.78s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 111/2388 [05:09<1:45:34,  2.78s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 112/2388 [05:12<1:45:31,  2.78s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 113/2388 [05:15<1:45:29,  2.78s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 114/2388 [05:18<1:45:26,  2.78s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 115/2388 [05:20<1:45:23,  2.78s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 116/2388 [05:23<1:45:20,  2.78s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 117/2388 [05:26<1:45:16,  2.78s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 118/2388 [05:29<1:45:13,  2.78s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 119/2388 [05:32<1:45:11,  2.78s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 120/2388 [05:34<1:45:08,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4778, 'learning_rate': 0.0001899497487437186, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m5%|▌         | 120/2388 [05:34<1:45:08,  2.78s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 121/2388 [05:37<1:45:06,  2.78s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 122/2388 [05:40<1:45:03,  2.78s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 123/2388 [05:43<1:45:00,  2.78s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 124/2388 [05:45<1:44:57,  2.78s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 125/2388 [05:48<1:44:54,  2.78s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 126/2388 [05:51<1:44:52,  2.78s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 127/2388 [05:54<1:44:49,  2.78s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 128/2388 [05:57<1:44:46,  2.78s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 129/2388 [05:59<1:44:44,  2.78s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 130/2388 [06:02<1:44:42,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3621, 'learning_rate': 0.00018911222780569514, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m5%|▌         | 130/2388 [06:02<1:44:42,  2.78s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 131/2388 [06:05<1:44:39,  2.78s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 132/2388 [06:08<1:44:35,  2.78s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 133/2388 [06:11<1:44:33,  2.78s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 134/2388 [06:13<1:44:31,  2.78s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 135/2388 [06:16<1:44:28,  2.78s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 136/2388 [06:19<1:44:25,  2.78s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 137/2388 [06:22<1:44:22,  2.78s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 138/2388 [06:24<1:44:20,  2.78s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 139/2388 [06:27<1:44:18,  2.78s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 140/2388 [06:30<1:44:15,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3972, 'learning_rate': 0.0001882747068676717, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m6%|▌         | 140/2388 [06:30<1:44:15,  2.78s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 141/2388 [06:33<1:44:13,  2.78s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 142/2388 [06:36<1:44:10,  2.78s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 143/2388 [06:38<1:44:07,  2.78s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 144/2388 [06:41<1:44:04,  2.78s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 145/2388 [06:44<1:44:01,  2.78s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 146/2388 [06:47<1:43:58,  2.78s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 147/2388 [06:49<1:43:55,  2.78s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 148/2388 [06:52<1:43:52,  2.78s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 149/2388 [06:55<1:43:49,  2.78s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 150/2388 [06:58<1:43:47,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3476, 'learning_rate': 0.00018743718592964824, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m6%|▋         | 150/2388 [06:58<1:43:47,  2.78s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 151/2388 [07:01<1:43:45,  2.78s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 152/2388 [07:03<1:43:42,  2.78s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 153/2388 [07:06<1:43:39,  2.78s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 154/2388 [07:09<1:43:35,  2.78s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 155/2388 [07:12<1:43:34,  2.78s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 156/2388 [07:15<1:43:31,  2.78s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 157/2388 [07:17<1:43:28,  2.78s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 158/2388 [07:20<1:43:25,  2.78s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 159/2388 [07:23<1:43:22,  2.78s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 160/2388 [07:26<1:43:18,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3907, 'learning_rate': 0.0001865996649916248, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m7%|▋         | 160/2388 [07:26<1:43:18,  2.78s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 161/2388 [07:28<1:43:15,  2.78s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 162/2388 [07:31<1:43:12,  2.78s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 163/2388 [07:34<1:43:08,  2.78s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 164/2388 [07:37<1:43:05,  2.78s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 165/2388 [07:40<1:43:02,  2.78s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 166/2388 [07:42<1:43:00,  2.78s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 167/2388 [07:45<1:42:57,  2.78s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 168/2388 [07:48<1:42:54,  2.78s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 169/2388 [07:51<1:42:51,  2.78s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 170/2388 [07:53<1:42:48,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4423, 'learning_rate': 0.00018576214405360134, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m7%|▋         | 170/2388 [07:53<1:42:48,  2.78s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 171/2388 [07:56<1:42:46,  2.78s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 172/2388 [07:59<1:42:43,  2.78s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 173/2388 [08:02<1:42:42,  2.78s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 174/2388 [08:05<1:42:39,  2.78s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 175/2388 [08:07<1:42:36,  2.78s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 176/2388 [08:10<1:42:34,  2.78s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 177/2388 [08:13<1:42:33,  2.78s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 178/2388 [08:16<1:42:30,  2.78s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 179/2388 [08:19<1:42:28,  2.78s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 180/2388 [08:21<1:42:25,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4569, 'learning_rate': 0.0001849246231155779, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m8%|▊         | 180/2388 [08:21<1:42:25,  2.78s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 181/2388 [08:24<1:42:22,  2.78s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 182/2388 [08:27<1:42:19,  2.78s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 183/2388 [08:30<1:42:16,  2.78s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 184/2388 [08:32<1:42:13,  2.78s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 185/2388 [08:35<1:42:10,  2.78s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 186/2388 [08:38<1:42:07,  2.78s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 187/2388 [08:41<1:42:05,  2.78s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 188/2388 [08:44<1:42:02,  2.78s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 189/2388 [08:46<1:41:59,  2.78s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 190/2388 [08:49<1:41:56,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.354, 'learning_rate': 0.00018408710217755444, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m8%|▊         | 190/2388 [08:49<1:41:56,  2.78s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 191/2388 [08:52<1:41:54,  2.78s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 192/2388 [08:55<1:41:51,  2.78s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 193/2388 [08:57<1:41:48,  2.78s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 194/2388 [09:00<1:41:45,  2.78s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 195/2388 [09:03<1:41:43,  2.78s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 196/2388 [09:06<1:41:40,  2.78s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 197/2388 [09:09<1:41:36,  2.78s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 198/2388 [09:11<1:41:33,  2.78s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 199/2388 [09:14<1:41:31,  2.78s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 200/2388 [09:17<1:41:27,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4826, 'learning_rate': 0.000183249581239531, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m8%|▊         | 200/2388 [09:17<1:41:27,  2.78s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 201/2388 [09:20<1:41:24,  2.78s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 202/2388 [09:23<1:41:21,  2.78s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 203/2388 [09:25<1:41:18,  2.78s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 204/2388 [09:28<1:41:15,  2.78s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 205/2388 [09:31<1:41:12,  2.78s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 206/2388 [09:34<1:41:09,  2.78s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 207/2388 [09:36<1:41:06,  2.78s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 208/2388 [09:39<1:41:02,  2.78s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 209/2388 [09:42<1:41:00,  2.78s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 210/2388 [09:45<1:40:57,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3857, 'learning_rate': 0.00018241206030150754, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m9%|▉         | 210/2388 [09:45<1:40:57,  2.78s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 211/2388 [09:48<1:40:55,  2.78s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 212/2388 [09:50<1:40:52,  2.78s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 213/2388 [09:53<1:40:48,  2.78s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 214/2388 [09:56<1:40:46,  2.78s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 215/2388 [09:59<1:40:43,  2.78s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 216/2388 [10:01<1:40:42,  2.78s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 217/2388 [10:04<1:40:39,  2.78s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 218/2388 [10:07<1:40:36,  2.78s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 219/2388 [10:10<1:40:32,  2.78s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 220/2388 [10:13<1:40:31,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3459, 'learning_rate': 0.0001815745393634841, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m9%|▉         | 220/2388 [10:13<1:40:31,  2.78s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 221/2388 [10:15<1:40:29,  2.78s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 222/2388 [10:18<1:40:26,  2.78s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 223/2388 [10:21<1:40:22,  2.78s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 224/2388 [10:24<1:40:19,  2.78s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 225/2388 [10:26<1:40:16,  2.78s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 226/2388 [10:29<1:40:13,  2.78s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 227/2388 [10:32<1:40:12,  2.78s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 228/2388 [10:35<1:40:10,  2.78s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 229/2388 [10:38<1:40:07,  2.78s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 230/2388 [10:40<1:40:03,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3844, 'learning_rate': 0.00018073701842546063, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m10%|▉         | 230/2388 [10:40<1:40:03,  2.78s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 231/2388 [10:43<1:40:01,  2.78s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 232/2388 [10:46<1:39:58,  2.78s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 233/2388 [10:49<1:39:54,  2.78s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 234/2388 [10:52<1:39:51,  2.78s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 235/2388 [10:54<1:39:48,  2.78s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 236/2388 [10:57<1:39:45,  2.78s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 237/2388 [11:00<1:39:43,  2.78s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 238/2388 [11:03<1:39:40,  2.78s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 239/2388 [11:05<1:39:37,  2.78s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 240/2388 [11:08<1:39:34,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4086, 'learning_rate': 0.0001798994974874372, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m10%|█         | 240/2388 [11:08<1:39:34,  2.78s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 241/2388 [11:11<1:39:32,  2.78s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 242/2388 [11:14<1:39:30,  2.78s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 243/2388 [11:17<1:39:27,  2.78s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 244/2388 [11:19<1:39:24,  2.78s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 245/2388 [11:22<1:39:21,  2.78s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 246/2388 [11:25<1:39:17,  2.78s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 247/2388 [11:28<1:39:14,  2.78s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 248/2388 [11:30<1:39:12,  2.78s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 249/2388 [11:33<1:39:09,  2.78s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 250/2388 [11:36<1:39:06,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4691, 'learning_rate': 0.00017906197654941373, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m10%|█         | 250/2388 [11:36<1:39:06,  2.78s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 251/2388 [11:39<1:39:04,  2.78s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 252/2388 [11:42<1:39:01,  2.78s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 253/2388 [11:44<1:38:59,  2.78s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 254/2388 [11:47<1:38:57,  2.78s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 255/2388 [11:50<1:38:54,  2.78s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 256/2388 [11:53<1:38:51,  2.78s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 257/2388 [11:56<1:38:49,  2.78s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 258/2388 [11:58<1:38:46,  2.78s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 259/2388 [12:01<1:38:44,  2.78s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 260/2388 [12:04<1:38:41,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3864, 'learning_rate': 0.0001782244556113903, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m11%|█         | 260/2388 [12:04<1:38:41,  2.78s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 261/2388 [12:07<1:38:39,  2.78s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 262/2388 [12:09<1:38:36,  2.78s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 263/2388 [12:12<1:38:35,  2.78s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 264/2388 [12:15<1:38:31,  2.78s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 265/2388 [12:18<1:38:28,  2.78s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 266/2388 [12:21<1:38:26,  2.78s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 267/2388 [12:23<1:38:22,  2.78s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 268/2388 [12:26<1:38:19,  2.78s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 269/2388 [12:29<1:38:16,  2.78s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 270/2388 [12:32<1:38:14,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3245, 'learning_rate': 0.00017738693467336683, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m11%|█▏        | 270/2388 [12:32<1:38:14,  2.78s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 271/2388 [12:34<1:38:11,  2.78s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 272/2388 [12:37<1:38:07,  2.78s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 273/2388 [12:40<1:38:03,  2.78s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 274/2388 [12:43<1:38:00,  2.78s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 275/2388 [12:46<1:37:57,  2.78s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 276/2388 [12:48<1:37:54,  2.78s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 277/2388 [12:51<1:37:51,  2.78s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 278/2388 [12:54<1:37:48,  2.78s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 279/2388 [12:57<1:37:45,  2.78s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 280/2388 [12:59<1:37:43,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2255, 'learning_rate': 0.0001765494137353434, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 280/2388 [12:59<1:37:43,  2.78s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 281/2388 [13:02<1:37:41,  2.78s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 282/2388 [13:05<1:37:37,  2.78s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 283/2388 [13:08<1:37:35,  2.78s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 284/2388 [13:11<1:37:33,  2.78s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 285/2388 [13:13<1:37:31,  2.78s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 286/2388 [13:16<1:37:28,  2.78s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 287/2388 [13:19<1:37:25,  2.78s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 288/2388 [13:22<1:37:22,  2.78s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 289/2388 [13:25<1:37:18,  2.78s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 290/2388 [13:27<1:37:15,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4375, 'learning_rate': 0.00017571189279731993, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 290/2388 [13:27<1:37:15,  2.78s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 291/2388 [13:30<1:37:12,  2.78s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 292/2388 [13:33<1:37:10,  2.78s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 293/2388 [13:36<1:37:07,  2.78s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 294/2388 [13:38<1:37:04,  2.78s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 295/2388 [13:41<1:37:01,  2.78s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 296/2388 [13:44<1:36:58,  2.78s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 297/2388 [13:47<1:36:55,  2.78s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 298/2388 [13:50<1:36:52,  2.78s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 299/2388 [13:52<1:36:50,  2.78s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 300/2388 [13:55<1:36:47,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.395, 'learning_rate': 0.0001748743718592965, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 300/2388 [13:55<1:36:47,  2.78s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 301/2388 [13:58<1:36:45,  2.78s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 302/2388 [14:01<1:36:43,  2.78s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 303/2388 [14:03<1:36:41,  2.78s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 304/2388 [14:06<1:36:39,  2.78s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 305/2388 [14:09<1:36:35,  2.78s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 306/2388 [14:12<1:36:34,  2.78s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 307/2388 [14:15<1:36:31,  2.78s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 308/2388 [14:17<1:36:28,  2.78s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 309/2388 [14:20<1:36:25,  2.78s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 310/2388 [14:23<1:36:23,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4544, 'learning_rate': 0.00017403685092127303, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 310/2388 [14:23<1:36:23,  2.78s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 311/2388 [14:26<1:36:20,  2.78s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 312/2388 [14:29<1:36:17,  2.78s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 313/2388 [14:31<1:36:14,  2.78s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 314/2388 [14:34<1:36:11,  2.78s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 315/2388 [14:37<1:36:09,  2.78s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 316/2388 [14:40<1:36:05,  2.78s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 317/2388 [14:42<1:36:03,  2.78s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 318/2388 [14:45<1:36:00,  2.78s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 319/2388 [14:48<1:35:57,  2.78s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 320/2388 [14:51<1:35:54,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3567, 'learning_rate': 0.0001731993299832496, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 320/2388 [14:51<1:35:54,  2.78s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 321/2388 [14:54<1:35:51,  2.78s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 322/2388 [14:56<1:35:48,  2.78s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 323/2388 [14:59<1:35:45,  2.78s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 324/2388 [15:02<1:35:43,  2.78s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 325/2388 [15:05<1:35:39,  2.78s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 326/2388 [15:07<1:35:37,  2.78s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 327/2388 [15:10<1:35:34,  2.78s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 328/2388 [15:13<1:35:32,  2.78s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 329/2388 [15:16<1:35:29,  2.78s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 330/2388 [15:19<1:35:25,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3899, 'learning_rate': 0.00017236180904522613, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 330/2388 [15:19<1:35:25,  2.78s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 331/2388 [15:21<1:35:23,  2.78s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 332/2388 [15:24<1:35:19,  2.78s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 333/2388 [15:27<1:35:16,  2.78s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 334/2388 [15:30<1:35:13,  2.78s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 335/2388 [15:33<1:35:10,  2.78s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 336/2388 [15:35<1:35:07,  2.78s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 337/2388 [15:38<1:35:04,  2.78s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 338/2388 [15:41<1:35:02,  2.78s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 339/2388 [15:44<1:34:59,  2.78s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 340/2388 [15:46<1:34:57,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3832, 'learning_rate': 0.0001715242881072027, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 340/2388 [15:46<1:34:57,  2.78s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 341/2388 [15:49<1:34:55,  2.78s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 342/2388 [15:52<1:34:53,  2.78s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 343/2388 [15:55<1:34:50,  2.78s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 344/2388 [15:58<1:34:48,  2.78s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 345/2388 [16:00<1:34:46,  2.78s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 346/2388 [16:03<1:34:43,  2.78s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 347/2388 [16:06<1:34:40,  2.78s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 348/2388 [16:09<1:34:37,  2.78s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 349/2388 [16:11<1:34:34,  2.78s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 350/2388 [16:14<1:34:32,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4262, 'learning_rate': 0.00017068676716917923, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 350/2388 [16:14<1:34:32,  2.78s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 351/2388 [16:17<1:34:30,  2.78s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 352/2388 [16:20<1:34:26,  2.78s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 353/2388 [16:23<1:34:22,  2.78s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 354/2388 [16:25<1:34:19,  2.78s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 355/2388 [16:28<1:34:15,  2.78s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 356/2388 [16:31<1:34:12,  2.78s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 357/2388 [16:34<1:34:09,  2.78s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 358/2388 [16:37<1:34:06,  2.78s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 359/2388 [16:39<1:34:03,  2.78s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 360/2388 [16:42<1:34:00,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4656, 'learning_rate': 0.0001698492462311558, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 360/2388 [16:42<1:34:00,  2.78s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 361/2388 [16:45<1:33:58,  2.78s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 362/2388 [16:48<1:33:55,  2.78s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 363/2388 [16:50<1:33:52,  2.78s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 364/2388 [16:53<1:33:49,  2.78s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 365/2388 [16:56<1:33:46,  2.78s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 366/2388 [16:59<1:33:43,  2.78s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 367/2388 [17:02<1:33:42,  2.78s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 368/2388 [17:04<1:33:38,  2.78s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 369/2388 [17:07<1:33:36,  2.78s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 370/2388 [17:10<1:33:33,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4133, 'learning_rate': 0.00016901172529313233, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 370/2388 [17:10<1:33:33,  2.78s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 371/2388 [17:13<1:33:32,  2.78s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 372/2388 [17:15<1:33:28,  2.78s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 373/2388 [17:18<1:33:25,  2.78s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 374/2388 [17:21<1:33:22,  2.78s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 375/2388 [17:24<1:33:19,  2.78s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 376/2388 [17:27<1:33:16,  2.78s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 377/2388 [17:29<1:33:13,  2.78s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 378/2388 [17:32<1:33:10,  2.78s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 379/2388 [17:35<1:33:07,  2.78s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 380/2388 [17:38<1:33:04,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3612, 'learning_rate': 0.0001681742043551089, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 380/2388 [17:38<1:33:04,  2.78s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 381/2388 [17:40<1:33:02,  2.78s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 382/2388 [17:43<1:32:59,  2.78s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 383/2388 [17:46<1:32:56,  2.78s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 384/2388 [17:49<1:32:53,  2.78s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 385/2388 [17:52<1:34:01,  2.82s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 386/2388 [17:55<1:33:37,  2.81s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 387/2388 [17:57<1:33:20,  2.80s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 388/2388 [18:00<1:33:06,  2.79s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 389/2388 [18:03<1:32:57,  2.79s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 390/2388 [18:06<1:32:49,  2.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3696, 'learning_rate': 0.00016733668341708543, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m16%|█▋        | 390/2388 [18:06<1:32:49,  2.79s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 391/2388 [18:08<1:32:43,  2.79s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 392/2388 [18:11<1:32:38,  2.78s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 393/2388 [18:14<1:32:34,  2.78s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 394/2388 [18:17<1:32:30,  2.78s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 395/2388 [18:20<1:32:26,  2.78s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 396/2388 [18:22<1:32:22,  2.78s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 397/2388 [18:25<1:32:19,  2.78s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 398/2388 [18:28<1:32:16,  2.78s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 399/2388 [18:31<1:32:13,  2.78s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 400/2388 [18:33<1:32:09,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3812, 'learning_rate': 0.000166499162479062, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 400/2388 [18:33<1:32:09,  2.78s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 401/2388 [18:36<1:32:08,  2.78s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 402/2388 [18:39<1:32:06,  2.78s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 403/2388 [18:42<1:32:04,  2.78s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 404/2388 [18:45<1:32:00,  2.78s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 405/2388 [18:47<1:31:57,  2.78s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 406/2388 [18:50<1:31:53,  2.78s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 407/2388 [18:53<1:31:50,  2.78s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 408/2388 [18:56<1:31:48,  2.78s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 409/2388 [18:58<1:31:45,  2.78s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 410/2388 [19:01<1:31:43,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4695, 'learning_rate': 0.00016566164154103853, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 410/2388 [19:01<1:31:43,  2.78s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 411/2388 [19:04<1:31:40,  2.78s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 412/2388 [19:07<1:31:38,  2.78s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 413/2388 [19:10<1:31:34,  2.78s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 414/2388 [19:12<1:31:33,  2.78s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 415/2388 [19:15<1:31:30,  2.78s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 416/2388 [19:18<1:31:26,  2.78s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 417/2388 [19:21<1:31:23,  2.78s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 418/2388 [19:24<1:31:20,  2.78s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 419/2388 [19:26<1:31:17,  2.78s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 420/2388 [19:29<1:31:14,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3888, 'learning_rate': 0.0001648241206030151, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 420/2388 [19:29<1:31:14,  2.78s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 421/2388 [19:32<1:31:11,  2.78s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 422/2388 [19:35<1:31:08,  2.78s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 423/2388 [19:37<1:31:06,  2.78s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 424/2388 [19:40<1:31:03,  2.78s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 425/2388 [19:43<1:31:00,  2.78s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 426/2388 [19:46<1:30:57,  2.78s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 427/2388 [19:49<1:30:54,  2.78s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 428/2388 [19:51<1:30:52,  2.78s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 429/2388 [19:54<1:30:48,  2.78s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 430/2388 [19:57<1:30:46,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4168, 'learning_rate': 0.00016398659966499162, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 430/2388 [19:57<1:30:46,  2.78s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 431/2388 [20:00<1:30:43,  2.78s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 432/2388 [20:02<1:30:42,  2.78s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 433/2388 [20:05<1:30:39,  2.78s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 434/2388 [20:08<1:30:36,  2.78s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 435/2388 [20:11<1:30:34,  2.78s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 436/2388 [20:14<1:30:32,  2.78s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 437/2388 [20:16<1:30:29,  2.78s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 438/2388 [20:19<1:30:25,  2.78s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 439/2388 [20:22<1:30:22,  2.78s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 440/2388 [20:25<1:30:19,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3435, 'learning_rate': 0.0001631490787269682, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 440/2388 [20:25<1:30:19,  2.78s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 441/2388 [20:28<1:30:17,  2.78s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 442/2388 [20:30<1:30:14,  2.78s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 443/2388 [20:33<1:30:11,  2.78s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 444/2388 [20:36<1:30:07,  2.78s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 445/2388 [20:39<1:30:04,  2.78s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 446/2388 [20:41<1:30:01,  2.78s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 447/2388 [20:44<1:29:58,  2.78s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 448/2388 [20:47<1:29:55,  2.78s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 449/2388 [20:50<1:29:52,  2.78s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 450/2388 [20:53<1:29:50,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4324, 'learning_rate': 0.00016231155778894472, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 450/2388 [20:53<1:29:50,  2.78s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 451/2388 [20:55<1:29:47,  2.78s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 452/2388 [20:58<1:29:44,  2.78s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 453/2388 [21:01<1:29:43,  2.78s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 454/2388 [21:04<1:29:40,  2.78s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 455/2388 [21:06<1:29:37,  2.78s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 456/2388 [21:09<1:29:34,  2.78s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 457/2388 [21:12<1:29:32,  2.78s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 458/2388 [21:15<1:29:29,  2.78s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 459/2388 [21:18<1:29:26,  2.78s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 460/2388 [21:20<1:29:23,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4003, 'learning_rate': 0.0001614740368509213, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 460/2388 [21:20<1:29:23,  2.78s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 461/2388 [21:23<1:29:20,  2.78s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 462/2388 [21:26<1:29:17,  2.78s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 463/2388 [21:29<1:29:14,  2.78s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 464/2388 [21:32<1:29:11,  2.78s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 465/2388 [21:34<1:29:08,  2.78s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 466/2388 [21:37<1:29:05,  2.78s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 467/2388 [21:40<1:29:02,  2.78s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 468/2388 [21:43<1:29:00,  2.78s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 469/2388 [21:45<1:28:57,  2.78s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 470/2388 [21:48<1:28:54,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4202, 'learning_rate': 0.00016063651591289782, 'epoch': 0.59}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 470/2388 [21:48<1:28:54,  2.78s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 471/2388 [21:51<1:28:52,  2.78s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 472/2388 [21:54<1:28:49,  2.78s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 473/2388 [21:57<1:28:46,  2.78s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 474/2388 [21:59<1:28:43,  2.78s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 475/2388 [22:02<1:28:42,  2.78s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 476/2388 [22:05<1:28:39,  2.78s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 477/2388 [22:08<1:28:35,  2.78s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 478/2388 [22:10<1:28:33,  2.78s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 479/2388 [22:13<1:28:31,  2.78s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 480/2388 [22:16<1:28:28,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3423, 'learning_rate': 0.00015979899497487439, 'epoch': 0.6}\u001b[0m\n",
      "\u001b[34m20%|██        | 480/2388 [22:16<1:28:28,  2.78s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 481/2388 [22:19<1:28:25,  2.78s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 482/2388 [22:22<1:28:22,  2.78s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 483/2388 [22:24<1:28:18,  2.78s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 484/2388 [22:27<1:28:15,  2.78s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 485/2388 [22:30<1:28:12,  2.78s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 486/2388 [22:33<1:28:09,  2.78s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 487/2388 [22:35<1:28:07,  2.78s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 488/2388 [22:38<1:28:04,  2.78s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 489/2388 [22:41<1:28:01,  2.78s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 490/2388 [22:44<1:27:58,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3315, 'learning_rate': 0.00015896147403685092, 'epoch': 0.62}\u001b[0m\n",
      "\u001b[34m21%|██        | 490/2388 [22:44<1:27:58,  2.78s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 491/2388 [22:47<1:27:56,  2.78s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 492/2388 [22:49<1:27:53,  2.78s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 493/2388 [22:52<1:27:50,  2.78s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 494/2388 [22:55<1:27:47,  2.78s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 495/2388 [22:58<1:27:44,  2.78s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 496/2388 [23:01<1:27:42,  2.78s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 497/2388 [23:03<1:27:39,  2.78s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 498/2388 [23:06<1:27:36,  2.78s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 499/2388 [23:09<1:27:34,  2.78s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 500/2388 [23:12<1:27:32,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3263, 'learning_rate': 0.00015812395309882749, 'epoch': 0.63}\u001b[0m\n",
      "\u001b[34m21%|██        | 500/2388 [23:12<1:27:32,  2.78s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 501/2388 [23:14<1:27:31,  2.78s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 502/2388 [23:17<1:27:28,  2.78s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 503/2388 [23:20<1:27:25,  2.78s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 504/2388 [23:23<1:27:22,  2.78s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 505/2388 [23:26<1:27:20,  2.78s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 506/2388 [23:28<1:27:17,  2.78s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 507/2388 [23:31<1:27:14,  2.78s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 508/2388 [23:34<1:27:11,  2.78s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 509/2388 [23:37<1:27:08,  2.78s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 510/2388 [23:39<1:27:05,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4005, 'learning_rate': 0.00015728643216080402, 'epoch': 0.64}\u001b[0m\n",
      "\u001b[34m21%|██▏       | 510/2388 [23:39<1:27:05,  2.78s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 511/2388 [23:42<1:27:03,  2.78s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 512/2388 [23:45<1:27:00,  2.78s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 513/2388 [23:48<1:26:57,  2.78s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 514/2388 [23:51<1:26:54,  2.78s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 515/2388 [23:53<1:26:51,  2.78s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 516/2388 [23:56<1:26:49,  2.78s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 517/2388 [23:59<1:26:46,  2.78s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 518/2388 [24:02<1:26:44,  2.78s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 519/2388 [24:05<1:26:41,  2.78s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 520/2388 [24:07<1:26:38,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3546, 'learning_rate': 0.00015644891122278058, 'epoch': 0.65}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 520/2388 [24:07<1:26:38,  2.78s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 521/2388 [24:10<1:26:35,  2.78s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 522/2388 [24:13<1:26:33,  2.78s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 523/2388 [24:16<1:26:30,  2.78s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 524/2388 [24:18<1:26:27,  2.78s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 525/2388 [24:21<1:26:24,  2.78s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 526/2388 [24:24<1:26:21,  2.78s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 527/2388 [24:27<1:26:18,  2.78s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 528/2388 [24:30<1:26:15,  2.78s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 529/2388 [24:32<1:26:11,  2.78s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 530/2388 [24:35<1:26:08,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3771, 'learning_rate': 0.00015561139028475712, 'epoch': 0.67}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 530/2388 [24:35<1:26:08,  2.78s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 531/2388 [24:38<1:26:05,  2.78s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 532/2388 [24:41<1:26:02,  2.78s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 533/2388 [24:43<1:25:59,  2.78s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 534/2388 [24:46<1:25:57,  2.78s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 535/2388 [24:49<1:25:54,  2.78s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 536/2388 [24:52<1:25:52,  2.78s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 537/2388 [24:55<1:25:49,  2.78s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 538/2388 [24:57<1:25:47,  2.78s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 539/2388 [25:00<1:25:44,  2.78s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 540/2388 [25:03<1:25:41,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.365, 'learning_rate': 0.00015477386934673368, 'epoch': 0.68}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 540/2388 [25:03<1:25:41,  2.78s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 541/2388 [25:06<1:25:39,  2.78s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 542/2388 [25:09<1:25:36,  2.78s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 543/2388 [25:11<1:25:34,  2.78s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 544/2388 [25:14<1:25:31,  2.78s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 545/2388 [25:17<1:25:28,  2.78s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 546/2388 [25:20<1:25:25,  2.78s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 547/2388 [25:22<1:25:22,  2.78s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 548/2388 [25:25<1:25:20,  2.78s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 549/2388 [25:28<1:25:17,  2.78s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 550/2388 [25:31<1:25:14,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4417, 'learning_rate': 0.00015393634840871022, 'epoch': 0.69}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 550/2388 [25:31<1:25:14,  2.78s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 551/2388 [25:34<1:25:11,  2.78s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 552/2388 [25:36<1:25:09,  2.78s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 553/2388 [25:39<1:25:06,  2.78s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 554/2388 [25:42<1:25:03,  2.78s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 555/2388 [25:45<1:25:00,  2.78s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 556/2388 [25:47<1:24:57,  2.78s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 557/2388 [25:50<1:24:54,  2.78s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 558/2388 [25:53<1:24:51,  2.78s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 559/2388 [25:56<1:24:49,  2.78s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 560/2388 [25:59<1:24:46,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3681, 'learning_rate': 0.00015309882747068678, 'epoch': 0.7}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 560/2388 [25:59<1:24:46,  2.78s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 561/2388 [26:01<1:24:44,  2.78s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 562/2388 [26:04<1:24:41,  2.78s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 563/2388 [26:07<1:24:38,  2.78s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 564/2388 [26:10<1:24:35,  2.78s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 565/2388 [26:13<1:24:33,  2.78s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 566/2388 [26:15<1:24:30,  2.78s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 567/2388 [26:18<1:24:27,  2.78s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 568/2388 [26:21<1:24:24,  2.78s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 569/2388 [26:24<1:24:21,  2.78s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 570/2388 [26:26<1:24:18,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3547, 'learning_rate': 0.00015226130653266332, 'epoch': 0.72}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 570/2388 [26:26<1:24:18,  2.78s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 571/2388 [26:29<1:24:16,  2.78s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 572/2388 [26:32<1:24:12,  2.78s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 573/2388 [26:35<1:24:09,  2.78s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 574/2388 [26:38<1:24:07,  2.78s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 575/2388 [26:40<1:24:04,  2.78s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 576/2388 [26:43<1:24:01,  2.78s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 577/2388 [26:46<1:23:58,  2.78s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 578/2388 [26:49<1:23:55,  2.78s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 579/2388 [26:51<1:23:53,  2.78s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 580/2388 [26:54<1:23:50,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.355, 'learning_rate': 0.00015142378559463988, 'epoch': 0.73}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 580/2388 [26:54<1:23:50,  2.78s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 581/2388 [26:57<1:23:47,  2.78s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 582/2388 [27:00<1:23:45,  2.78s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 583/2388 [27:03<1:23:43,  2.78s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 584/2388 [27:05<1:23:40,  2.78s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 585/2388 [27:08<1:23:36,  2.78s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 586/2388 [27:11<1:23:33,  2.78s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 587/2388 [27:14<1:23:31,  2.78s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 588/2388 [27:17<1:23:28,  2.78s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 589/2388 [27:19<1:23:25,  2.78s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 590/2388 [27:22<1:23:22,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3766, 'learning_rate': 0.00015058626465661642, 'epoch': 0.74}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 590/2388 [27:22<1:23:22,  2.78s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 591/2388 [27:25<1:23:20,  2.78s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 592/2388 [27:28<1:23:17,  2.78s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 593/2388 [27:30<1:23:14,  2.78s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 594/2388 [27:33<1:23:11,  2.78s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 595/2388 [27:36<1:23:08,  2.78s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 596/2388 [27:39<1:23:04,  2.78s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 597/2388 [27:42<1:23:02,  2.78s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 598/2388 [27:44<1:22:58,  2.78s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 599/2388 [27:47<1:22:55,  2.78s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 600/2388 [27:50<1:22:52,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4231, 'learning_rate': 0.00014974874371859298, 'epoch': 0.75}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 600/2388 [27:50<1:22:52,  2.78s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 601/2388 [27:53<1:22:50,  2.78s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 602/2388 [27:55<1:22:47,  2.78s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 603/2388 [27:58<1:22:44,  2.78s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 604/2388 [28:01<1:22:40,  2.78s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 605/2388 [28:04<1:22:38,  2.78s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 606/2388 [28:07<1:22:35,  2.78s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 607/2388 [28:09<1:22:32,  2.78s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 608/2388 [28:12<1:22:31,  2.78s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 609/2388 [28:15<1:22:28,  2.78s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 610/2388 [28:18<1:22:25,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.287, 'learning_rate': 0.00014891122278056952, 'epoch': 0.77}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 610/2388 [28:18<1:22:25,  2.78s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 611/2388 [28:20<1:22:22,  2.78s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 612/2388 [28:23<1:22:19,  2.78s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 613/2388 [28:26<1:22:16,  2.78s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 614/2388 [28:29<1:22:13,  2.78s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 615/2388 [28:32<1:22:10,  2.78s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 616/2388 [28:34<1:22:07,  2.78s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 617/2388 [28:37<1:22:04,  2.78s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 618/2388 [28:40<1:22:01,  2.78s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 619/2388 [28:43<1:21:58,  2.78s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 620/2388 [28:46<1:21:55,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.392, 'learning_rate': 0.00014807370184254608, 'epoch': 0.78}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 620/2388 [28:46<1:21:55,  2.78s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 621/2388 [28:48<1:21:53,  2.78s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 622/2388 [28:51<1:21:50,  2.78s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 623/2388 [28:54<1:21:47,  2.78s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 624/2388 [28:57<1:21:45,  2.78s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 625/2388 [28:59<1:21:42,  2.78s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 626/2388 [29:02<1:21:39,  2.78s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 627/2388 [29:05<1:21:36,  2.78s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 628/2388 [29:08<1:21:34,  2.78s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 629/2388 [29:11<1:21:32,  2.78s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 630/2388 [29:13<1:21:30,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3236, 'learning_rate': 0.00014723618090452262, 'epoch': 0.79}\u001b[0m\n",
      "\u001b[34m26%|██▋       | 630/2388 [29:13<1:21:30,  2.78s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 631/2388 [29:16<1:21:28,  2.78s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 632/2388 [29:19<1:21:25,  2.78s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 633/2388 [29:22<1:21:23,  2.78s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 634/2388 [29:24<1:21:20,  2.78s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 635/2388 [29:27<1:21:17,  2.78s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 636/2388 [29:30<1:21:14,  2.78s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 637/2388 [29:33<1:21:11,  2.78s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 638/2388 [29:36<1:21:09,  2.78s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 639/2388 [29:38<1:21:07,  2.78s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 640/2388 [29:41<1:21:04,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4171, 'learning_rate': 0.00014639865996649918, 'epoch': 0.8}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 640/2388 [29:41<1:21:04,  2.78s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 641/2388 [29:44<1:21:01,  2.78s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 642/2388 [29:47<1:20:59,  2.78s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 643/2388 [29:49<1:20:56,  2.78s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 644/2388 [29:52<1:20:53,  2.78s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 645/2388 [29:55<1:20:50,  2.78s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 646/2388 [29:58<1:20:47,  2.78s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 647/2388 [30:01<1:20:44,  2.78s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 648/2388 [30:03<1:20:41,  2.78s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 649/2388 [30:06<1:20:37,  2.78s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 650/2388 [30:09<1:20:34,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.454, 'learning_rate': 0.00014556113902847571, 'epoch': 0.82}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 650/2388 [30:09<1:20:34,  2.78s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 651/2388 [30:12<1:20:34,  2.78s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 652/2388 [30:15<1:20:31,  2.78s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 653/2388 [30:17<1:20:27,  2.78s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 654/2388 [30:20<1:20:24,  2.78s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 655/2388 [30:23<1:20:20,  2.78s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 656/2388 [30:26<1:20:17,  2.78s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 657/2388 [30:28<1:20:14,  2.78s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 658/2388 [30:31<1:20:11,  2.78s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 659/2388 [30:34<1:20:08,  2.78s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 660/2388 [30:37<1:20:05,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3748, 'learning_rate': 0.00014472361809045228, 'epoch': 0.83}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 660/2388 [30:37<1:20:05,  2.78s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 661/2388 [30:40<1:20:03,  2.78s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 662/2388 [30:42<1:20:00,  2.78s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 663/2388 [30:45<1:19:57,  2.78s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 664/2388 [30:48<1:19:54,  2.78s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 665/2388 [30:51<1:19:51,  2.78s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 666/2388 [30:53<1:19:48,  2.78s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 667/2388 [30:56<1:19:46,  2.78s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 668/2388 [30:59<1:19:43,  2.78s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 669/2388 [31:02<1:19:41,  2.78s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 670/2388 [31:05<1:19:38,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2812, 'learning_rate': 0.0001438860971524288, 'epoch': 0.84}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 670/2388 [31:05<1:19:38,  2.78s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 671/2388 [31:07<1:19:36,  2.78s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 672/2388 [31:10<1:19:33,  2.78s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 673/2388 [31:13<1:19:31,  2.78s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 674/2388 [31:16<1:19:28,  2.78s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 675/2388 [31:19<1:19:26,  2.78s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 676/2388 [31:21<1:19:23,  2.78s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 677/2388 [31:24<1:19:20,  2.78s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 678/2388 [31:27<1:19:18,  2.78s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 679/2388 [31:30<1:19:15,  2.78s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 680/2388 [31:32<1:19:12,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3881, 'learning_rate': 0.00014304857621440538, 'epoch': 0.85}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 680/2388 [31:32<1:19:12,  2.78s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 681/2388 [31:35<1:19:10,  2.78s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 682/2388 [31:38<1:19:07,  2.78s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 683/2388 [31:41<1:19:04,  2.78s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 684/2388 [31:44<1:19:01,  2.78s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 685/2388 [31:46<1:18:58,  2.78s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 686/2388 [31:49<1:18:56,  2.78s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 687/2388 [31:52<1:18:53,  2.78s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 688/2388 [31:55<1:18:50,  2.78s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 689/2388 [31:57<1:18:47,  2.78s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 690/2388 [32:00<1:18:44,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3157, 'learning_rate': 0.0001422110552763819, 'epoch': 0.87}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 690/2388 [32:00<1:18:44,  2.78s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 691/2388 [32:03<1:18:42,  2.78s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 692/2388 [32:06<1:18:39,  2.78s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 693/2388 [32:09<1:18:36,  2.78s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 694/2388 [32:11<1:18:33,  2.78s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 695/2388 [32:14<1:18:31,  2.78s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 696/2388 [32:17<1:18:28,  2.78s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 697/2388 [32:20<1:18:25,  2.78s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 698/2388 [32:23<1:18:22,  2.78s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 699/2388 [32:25<1:18:19,  2.78s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 700/2388 [32:28<1:18:16,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3985, 'learning_rate': 0.00014137353433835848, 'epoch': 0.88}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 700/2388 [32:28<1:18:16,  2.78s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 701/2388 [32:31<1:18:14,  2.78s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 702/2388 [32:34<1:18:11,  2.78s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 703/2388 [32:36<1:18:08,  2.78s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 704/2388 [32:39<1:18:05,  2.78s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 705/2388 [32:42<1:18:02,  2.78s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 706/2388 [32:45<1:17:59,  2.78s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 707/2388 [32:48<1:17:56,  2.78s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 708/2388 [32:50<1:17:53,  2.78s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 709/2388 [32:53<1:17:50,  2.78s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 710/2388 [32:56<1:17:47,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4325, 'learning_rate': 0.000140536013400335, 'epoch': 0.89}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 710/2388 [32:56<1:17:47,  2.78s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 711/2388 [32:59<1:17:44,  2.78s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 712/2388 [33:01<1:17:42,  2.78s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 713/2388 [33:04<1:17:39,  2.78s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 714/2388 [33:07<1:17:36,  2.78s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 715/2388 [33:10<1:17:33,  2.78s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 716/2388 [33:13<1:17:32,  2.78s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 717/2388 [33:15<1:17:29,  2.78s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 718/2388 [33:18<1:17:25,  2.78s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 719/2388 [33:21<1:17:22,  2.78s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 720/2388 [33:24<1:17:19,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4259, 'learning_rate': 0.00013969849246231157, 'epoch': 0.9}\u001b[0m\n",
      "\u001b[34m30%|███       | 720/2388 [33:24<1:17:19,  2.78s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 721/2388 [33:27<1:17:17,  2.78s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 722/2388 [33:29<1:17:14,  2.78s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 723/2388 [33:32<1:17:11,  2.78s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 724/2388 [33:35<1:17:08,  2.78s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 725/2388 [33:38<1:17:05,  2.78s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 726/2388 [33:40<1:17:02,  2.78s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 727/2388 [33:43<1:17:00,  2.78s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 728/2388 [33:46<1:16:58,  2.78s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 729/2388 [33:49<1:16:55,  2.78s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 730/2388 [33:52<1:16:53,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3728, 'learning_rate': 0.0001388609715242881, 'epoch': 0.92}\u001b[0m\n",
      "\u001b[34m31%|███       | 730/2388 [33:52<1:16:53,  2.78s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 731/2388 [33:54<1:16:50,  2.78s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 732/2388 [33:57<1:16:48,  2.78s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 733/2388 [34:00<1:16:45,  2.78s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 734/2388 [34:03<1:16:42,  2.78s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 735/2388 [34:05<1:16:40,  2.78s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 736/2388 [34:08<1:16:37,  2.78s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 737/2388 [34:11<1:16:34,  2.78s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 738/2388 [34:14<1:16:32,  2.78s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 739/2388 [34:17<1:16:29,  2.78s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 740/2388 [34:19<1:16:26,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2971, 'learning_rate': 0.00013802345058626467, 'epoch': 0.93}\u001b[0m\n",
      "\u001b[34m31%|███       | 740/2388 [34:19<1:16:26,  2.78s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 741/2388 [34:22<1:16:23,  2.78s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 742/2388 [34:25<1:16:20,  2.78s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 743/2388 [34:28<1:16:17,  2.78s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 744/2388 [34:30<1:16:14,  2.78s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 745/2388 [34:33<1:16:11,  2.78s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 746/2388 [34:36<1:16:08,  2.78s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 747/2388 [34:39<1:16:04,  2.78s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 748/2388 [34:42<1:16:02,  2.78s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 749/2388 [34:44<1:15:59,  2.78s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 750/2388 [34:47<1:15:57,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3866, 'learning_rate': 0.0001371859296482412, 'epoch': 0.94}\u001b[0m\n",
      "\u001b[34m31%|███▏      | 750/2388 [34:47<1:15:57,  2.78s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 751/2388 [34:50<1:15:54,  2.78s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 752/2388 [34:53<1:15:52,  2.78s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 753/2388 [34:56<1:15:49,  2.78s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 754/2388 [34:58<1:15:46,  2.78s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 755/2388 [35:01<1:15:44,  2.78s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 756/2388 [35:04<1:15:41,  2.78s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 757/2388 [35:07<1:15:38,  2.78s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 758/2388 [35:09<1:15:35,  2.78s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 759/2388 [35:12<1:15:33,  2.78s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 760/2388 [35:15<1:15:29,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3746, 'learning_rate': 0.00013634840871021777, 'epoch': 0.95}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 760/2388 [35:15<1:15:29,  2.78s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 761/2388 [35:18<1:15:27,  2.78s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 762/2388 [35:21<1:15:23,  2.78s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 763/2388 [35:23<1:15:20,  2.78s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 764/2388 [35:26<1:15:17,  2.78s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 765/2388 [35:29<1:15:14,  2.78s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 766/2388 [35:32<1:15:11,  2.78s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 767/2388 [35:34<1:15:08,  2.78s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 768/2388 [35:37<1:15:05,  2.78s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 769/2388 [35:40<1:15:02,  2.78s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 770/2388 [35:43<1:15:00,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3524, 'learning_rate': 0.0001355108877721943, 'epoch': 0.97}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 770/2388 [35:43<1:15:00,  2.78s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 771/2388 [35:46<1:14:57,  2.78s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 772/2388 [35:48<1:14:54,  2.78s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 773/2388 [35:51<1:14:51,  2.78s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 774/2388 [35:54<1:14:48,  2.78s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 775/2388 [35:57<1:14:46,  2.78s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 776/2388 [36:00<1:14:43,  2.78s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 777/2388 [36:02<1:14:41,  2.78s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 778/2388 [36:05<1:14:38,  2.78s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 779/2388 [36:08<1:14:35,  2.78s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 780/2388 [36:11<1:14:33,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3387, 'learning_rate': 0.00013467336683417087, 'epoch': 0.98}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 780/2388 [36:11<1:14:33,  2.78s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 781/2388 [36:13<1:14:31,  2.78s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 782/2388 [36:16<1:14:28,  2.78s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 783/2388 [36:19<1:15:21,  2.82s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 784/2388 [36:22<1:15:01,  2.81s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 785/2388 [36:25<1:14:46,  2.80s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 786/2388 [36:27<1:14:35,  2.79s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 787/2388 [36:30<1:14:26,  2.79s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 788/2388 [36:33<1:14:19,  2.79s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 789/2388 [36:36<1:14:13,  2.79s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 790/2388 [36:39<1:14:08,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4874, 'learning_rate': 0.0001338358458961474, 'epoch': 0.99}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 790/2388 [36:39<1:14:08,  2.78s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 791/2388 [36:41<1:14:05,  2.78s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 792/2388 [36:44<1:14:01,  2.78s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 793/2388 [36:47<1:13:57,  2.78s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 794/2388 [36:50<1:13:54,  2.78s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 795/2388 [36:52<1:13:49,  2.78s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 796/2388 [36:54<1:04:08,  2.42s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 797/2388 [36:57<1:07:03,  2.53s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 798/2388 [37:00<1:09:01,  2.60s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 799/2388 [37:02<1:10:23,  2.66s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 800/2388 [37:05<1:11:19,  2.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.333, 'learning_rate': 0.00013299832495812397, 'epoch': 1.01}\u001b[0m\n",
      "\u001b[34m34%|███▎      | 800/2388 [37:05<1:11:19,  2.70s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 801/2388 [37:08<1:11:58,  2.72s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 802/2388 [37:11<1:12:24,  2.74s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 803/2388 [37:14<1:12:42,  2.75s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 804/2388 [37:16<1:12:54,  2.76s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 805/2388 [37:19<1:13:00,  2.77s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 806/2388 [37:22<1:13:04,  2.77s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 807/2388 [37:25<1:13:05,  2.77s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 808/2388 [37:27<1:13:07,  2.78s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 809/2388 [37:30<1:13:07,  2.78s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 810/2388 [37:33<1:13:06,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2347, 'learning_rate': 0.0001321608040201005, 'epoch': 1.02}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 810/2388 [37:33<1:13:06,  2.78s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 811/2388 [37:36<1:13:04,  2.78s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 812/2388 [37:39<1:13:02,  2.78s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 813/2388 [37:41<1:13:00,  2.78s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 814/2388 [37:44<1:12:57,  2.78s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 815/2388 [37:47<1:12:54,  2.78s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 816/2388 [37:50<1:12:51,  2.78s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 817/2388 [37:52<1:12:49,  2.78s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 818/2388 [37:55<1:12:46,  2.78s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 819/2388 [37:58<1:12:43,  2.78s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 820/2388 [38:01<1:12:40,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3138, 'learning_rate': 0.00013132328308207707, 'epoch': 1.03}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 820/2388 [38:01<1:12:40,  2.78s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 821/2388 [38:04<1:12:38,  2.78s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 822/2388 [38:06<1:12:35,  2.78s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 823/2388 [38:09<1:12:32,  2.78s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 824/2388 [38:12<1:12:31,  2.78s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 825/2388 [38:15<1:12:28,  2.78s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 826/2388 [38:18<1:12:25,  2.78s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 827/2388 [38:20<1:12:22,  2.78s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 828/2388 [38:23<1:12:19,  2.78s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 829/2388 [38:26<1:12:16,  2.78s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 830/2388 [38:29<1:12:14,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3049, 'learning_rate': 0.0001304857621440536, 'epoch': 1.04}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 830/2388 [38:29<1:12:14,  2.78s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 831/2388 [38:31<1:12:12,  2.78s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 832/2388 [38:34<1:12:09,  2.78s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 833/2388 [38:37<1:12:06,  2.78s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 834/2388 [38:40<1:12:03,  2.78s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 835/2388 [38:43<1:12:00,  2.78s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 836/2388 [38:45<1:11:57,  2.78s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 837/2388 [38:48<1:11:55,  2.78s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 838/2388 [38:51<1:11:52,  2.78s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 839/2388 [38:54<1:11:49,  2.78s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 840/2388 [38:56<1:11:46,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2435, 'learning_rate': 0.00012964824120603017, 'epoch': 1.06}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 840/2388 [38:56<1:11:46,  2.78s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 841/2388 [38:59<1:11:43,  2.78s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 842/2388 [39:02<1:11:41,  2.78s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 843/2388 [39:05<1:11:38,  2.78s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 844/2388 [39:08<1:11:35,  2.78s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 845/2388 [39:10<1:11:32,  2.78s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 846/2388 [39:13<1:11:30,  2.78s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 847/2388 [39:16<1:11:27,  2.78s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 848/2388 [39:19<1:11:23,  2.78s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 849/2388 [39:22<1:11:21,  2.78s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 850/2388 [39:24<1:11:17,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2181, 'learning_rate': 0.0001288107202680067, 'epoch': 1.07}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 850/2388 [39:24<1:11:17,  2.78s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 851/2388 [39:27<1:11:16,  2.78s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 852/2388 [39:30<1:11:13,  2.78s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 853/2388 [39:33<1:11:11,  2.78s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 854/2388 [39:35<1:11:08,  2.78s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 855/2388 [39:38<1:11:05,  2.78s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 856/2388 [39:41<1:11:02,  2.78s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 857/2388 [39:44<1:10:59,  2.78s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 858/2388 [39:47<1:10:55,  2.78s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 859/2388 [39:49<1:10:52,  2.78s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 860/2388 [39:52<1:10:49,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2679, 'learning_rate': 0.00012797319932998327, 'epoch': 1.08}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 860/2388 [39:52<1:10:49,  2.78s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 861/2388 [39:55<1:10:47,  2.78s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 862/2388 [39:58<1:10:44,  2.78s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 863/2388 [40:00<1:10:41,  2.78s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 864/2388 [40:03<1:10:39,  2.78s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 865/2388 [40:06<1:10:37,  2.78s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 866/2388 [40:09<1:10:34,  2.78s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 867/2388 [40:12<1:10:31,  2.78s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 868/2388 [40:14<1:10:29,  2.78s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 869/2388 [40:17<1:10:26,  2.78s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 870/2388 [40:20<1:10:22,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3015, 'learning_rate': 0.0001271356783919598, 'epoch': 1.09}\u001b[0m\n",
      "\u001b[34m36%|███▋      | 870/2388 [40:20<1:10:22,  2.78s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 871/2388 [40:23<1:10:20,  2.78s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 872/2388 [40:25<1:10:17,  2.78s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 873/2388 [40:28<1:10:14,  2.78s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 874/2388 [40:31<1:10:11,  2.78s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 875/2388 [40:34<1:10:07,  2.78s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 876/2388 [40:37<1:10:05,  2.78s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 877/2388 [40:39<1:10:02,  2.78s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 878/2388 [40:42<1:09:59,  2.78s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 879/2388 [40:45<1:09:56,  2.78s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 880/2388 [40:48<1:09:54,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2776, 'learning_rate': 0.00012629815745393637, 'epoch': 1.11}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 880/2388 [40:48<1:09:54,  2.78s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 881/2388 [40:51<1:09:51,  2.78s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 882/2388 [40:53<1:09:48,  2.78s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 883/2388 [40:56<1:09:45,  2.78s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 884/2388 [40:59<1:09:42,  2.78s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 885/2388 [41:02<1:09:40,  2.78s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 886/2388 [41:04<1:09:38,  2.78s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 887/2388 [41:07<1:09:35,  2.78s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 888/2388 [41:10<1:09:32,  2.78s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 889/2388 [41:13<1:09:30,  2.78s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 890/2388 [41:16<1:09:28,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2147, 'learning_rate': 0.0001254606365159129, 'epoch': 1.12}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 890/2388 [41:16<1:09:28,  2.78s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 891/2388 [41:18<1:09:25,  2.78s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 892/2388 [41:21<1:09:22,  2.78s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 893/2388 [41:24<1:09:18,  2.78s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 894/2388 [41:27<1:09:15,  2.78s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 895/2388 [41:29<1:09:12,  2.78s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 896/2388 [41:32<1:09:10,  2.78s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 897/2388 [41:35<1:09:07,  2.78s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 898/2388 [41:38<1:09:04,  2.78s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 899/2388 [41:41<1:09:01,  2.78s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 900/2388 [41:43<1:08:58,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2413, 'learning_rate': 0.00012462311557788947, 'epoch': 1.13}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 900/2388 [41:43<1:08:58,  2.78s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 901/2388 [41:46<1:08:56,  2.78s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 902/2388 [41:49<1:08:53,  2.78s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 903/2388 [41:52<1:08:50,  2.78s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 904/2388 [41:54<1:08:47,  2.78s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 905/2388 [41:57<1:08:44,  2.78s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 906/2388 [42:00<1:08:41,  2.78s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 907/2388 [42:03<1:08:39,  2.78s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 908/2388 [42:06<1:08:37,  2.78s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 909/2388 [42:08<1:08:34,  2.78s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 910/2388 [42:11<1:08:31,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3389, 'learning_rate': 0.000123785594639866, 'epoch': 1.14}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 910/2388 [42:11<1:08:31,  2.78s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 911/2388 [42:14<1:08:29,  2.78s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 912/2388 [42:17<1:08:26,  2.78s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 913/2388 [42:20<1:08:22,  2.78s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 914/2388 [42:22<1:08:19,  2.78s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 915/2388 [42:25<1:08:16,  2.78s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 916/2388 [42:28<1:08:13,  2.78s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 917/2388 [42:31<1:08:11,  2.78s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 918/2388 [42:33<1:08:08,  2.78s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 919/2388 [42:36<1:08:05,  2.78s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 920/2388 [42:39<1:08:02,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2698, 'learning_rate': 0.00012294807370184256, 'epoch': 1.16}\u001b[0m\n",
      "\u001b[34m39%|███▊      | 920/2388 [42:39<1:08:02,  2.78s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 921/2388 [42:42<1:08:00,  2.78s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 922/2388 [42:45<1:07:57,  2.78s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 923/2388 [42:47<1:07:54,  2.78s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 924/2388 [42:50<1:07:51,  2.78s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 925/2388 [42:53<1:07:49,  2.78s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 926/2388 [42:56<1:07:46,  2.78s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 927/2388 [42:58<1:07:43,  2.78s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 928/2388 [43:01<1:07:41,  2.78s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 929/2388 [43:04<1:07:38,  2.78s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 930/2388 [43:07<1:07:35,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2794, 'learning_rate': 0.0001221105527638191, 'epoch': 1.17}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 930/2388 [43:07<1:07:35,  2.78s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 931/2388 [43:10<1:07:33,  2.78s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 932/2388 [43:12<1:07:31,  2.78s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 933/2388 [43:15<1:07:28,  2.78s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 934/2388 [43:18<1:07:25,  2.78s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 935/2388 [43:21<1:07:23,  2.78s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 936/2388 [43:24<1:07:20,  2.78s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 937/2388 [43:26<1:07:17,  2.78s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 938/2388 [43:29<1:07:14,  2.78s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 939/2388 [43:32<1:07:12,  2.78s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 940/2388 [43:35<1:07:09,  2.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.296, 'learning_rate': 0.00012127303182579566, 'epoch': 1.18}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 940/2388 [43:35<1:07:09,  2.78s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 941/2388 [43:37<1:07:06,  2.78s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 942/2388 [43:40<1:07:04,  2.78s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 943/2388 [43:43<1:07:01,  2.78s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 944/2388 [43:46<1:06:58,  2.78s/it]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example for LLaMA 13B, the SageMaker training job took `31728 seconds`, which is about `8.8 hours`. The ml.g5.4xlarge instance we used costs `$2.03 per hour` for on-demand usage. As a result, the total cost for training our fine-tuned LLaMa 2 model was only ~`$18`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps \n",
    "\n",
    "You can deploy your fine-tuned LLaMA model to a SageMaker endpoint and use it for inference. Check out the [Deploy Falcon 7B & 40B on Amazon SageMaker](https://www.philschmid.de/sagemaker-falcon-llm) and [Securely deploy LLMs inside VPCs with Hugging Face and Amazon SageMaker](https://www.philschmid.de/sagemaker-llm-vpc) for more details."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
