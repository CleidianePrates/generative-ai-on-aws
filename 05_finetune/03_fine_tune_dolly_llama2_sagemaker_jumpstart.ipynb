{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af2ee7fb-e888-4e38-a349-c7c40dfd2963",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Fine-tune LLaMA 2 models on SageMaker JumpStart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85addd9d-ec89-44a7-9fb5-9bc24fe9993b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: sagemaker==2.202.1 in /opt/conda/lib/python3.10/site-packages (2.202.1)\n",
      "Requirement already satisfied: datasets==2.15.0 in /opt/conda/lib/python3.10/site-packages (2.15.0)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker==2.202.1) (23.1.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.33.3 in /opt/conda/lib/python3.10/site-packages (from sagemaker==2.202.1) (1.34.7)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker==2.202.1) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.10/site-packages (from sagemaker==2.202.1) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker==2.202.1) (1.26.2)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.10/site-packages (from sagemaker==2.202.1) (4.25.1)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker==2.202.1) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker==2.202.1) (6.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker==2.202.1) (23.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from sagemaker==2.202.1) (2.1.4)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.10/site-packages (from sagemaker==2.202.1) (0.3.1)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.10/site-packages (from sagemaker==2.202.1) (0.7.5)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker==2.202.1) (6.0.1)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from sagemaker==2.202.1) (4.20.0)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from sagemaker==2.202.1) (4.1.0)\n",
      "Requirement already satisfied: tblib<3,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker==2.202.1) (2.0.0)\n",
      "Requirement already satisfied: urllib3<1.27 in /opt/conda/lib/python3.10/site-packages (from sagemaker==2.202.1) (1.26.18)\n",
      "Requirement already satisfied: uvicorn==0.22.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker==2.202.1) (0.22.0)\n",
      "Requirement already satisfied: fastapi==0.95.2 in /opt/conda/lib/python3.10/site-packages (from sagemaker==2.202.1) (0.95.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from sagemaker==2.202.1) (2.31.0)\n",
      "Requirement already satisfied: docker in /opt/conda/lib/python3.10/site-packages (from sagemaker==2.202.1) (7.0.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sagemaker==2.202.1) (4.66.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from sagemaker==2.202.1) (5.9.7)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (14.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (0.3.7)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.15.0) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.18.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (0.20.1)\n",
      "Requirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from fastapi==0.95.2->sagemaker==2.202.1) (1.10.13)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from fastapi==0.95.2->sagemaker==2.202.1) (0.27.0)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn==0.22.0->sagemaker==2.202.1) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn==0.22.0->sagemaker==2.202.1) (0.14.0)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.7 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker==2.202.1) (1.34.7)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker==2.202.1) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker==2.202.1) (0.10.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (4.0.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.18.0->datasets==2.15.0) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.18.0->datasets==2.15.0) (4.9.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker==2.202.1) (3.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker==2.202.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker==2.202.1) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker==2.202.1) (2023.11.17)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from google-pasta->sagemaker==2.202.1) (1.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker==2.202.1) (2023.11.2)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker==2.202.1) (0.32.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker==2.202.1) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker==2.202.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker==2.202.1) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker==2.202.1) (2023.3)\n",
      "Requirement already satisfied: ppft>=1.7.6.7 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker==2.202.1) (1.7.6.7)\n",
      "Requirement already satisfied: pox>=0.3.3 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker==2.202.1) (0.3.3)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.10/site-packages (from schema->sagemaker==2.202.1) (21.6.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /opt/conda/lib/python3.10/site-packages (from starlette<0.28.0,>=0.27.0->fastapi==0.95.2->sagemaker==2.202.1) (4.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi==0.95.2->sagemaker==2.202.1) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi==0.95.2->sagemaker==2.202.1) (1.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U sagemaker==2.202.1 datasets==2.15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13274b9b-87bd-4090-a6aa-294570c31e0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Deploy Pre-trained Model\n",
    "\n",
    "---\n",
    "\n",
    "First we will deploy the Llama-2 model as a SageMaker endpoint.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97e1d98f",
   "metadata": {
    "jumpStartAlterations": [
     "modelIdVersion"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = \"meta-textgeneration-llama-2-7b\", \"2.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1722b230-b7bc-487f-b4ee-98ca42848423",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "pretrained_model = JumpStartModel(model_id=model_id, model_version=model_version)\n",
    "pretrained_predictor = pretrained_model.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8017c4ef-eb89-4da6-8e28-c800adbfc4b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Invoke the endpoint\n",
    "\n",
    "---\n",
    "Next, we invoke the endpoint with some sample queries. Later, in this notebook, we will fine-tune this model with a custom dataset and carry out inference using the fine-tuned model. We will also show comparison between results obtained via the pre-trained and the fine-tuned models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b795a085-048f-42b2-945f-0cd339c1cf91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_response(payload, response):\n",
    "    print(payload[\"inputs\"])\n",
    "    print(f\"> {response[0]['generation']}\")\n",
    "    print(\"\\n==================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dd833f8-1ddc-4805-80b2-19e7db629880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"inputs\": \"I believe the meaning of life is\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 64,\n",
    "        # \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6,\n",
    "        \"return_full_text\": False,\n",
    "    },\n",
    "}\n",
    "try:\n",
    "    response = pretrained_predictor.predict(payload, custom_attributes=\"accept_eula=true\")\n",
    "    print_response(payload, response)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6773e6-7cf2-4cea-bce6-905d5995d857",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "To learn about additional use cases of pre-trained model, please checkout the notebook [Text completion: Run Llama 2 models in SageMaker JumpStart](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart-foundation-models/llama-2-text-completion.ipynb).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e19e16f-d459-40c6-9d6b-0272938b3878",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset preparation for fine-tuning\n",
    "\n",
    "---\n",
    "\n",
    "You can fine-tune on the dataset with domain adaptation format or instruction tuning format. Please find more details in the section [Dataset instruction](#Dataset-instruction). In this demo, we will use a subset of [Dolly dataset](https://huggingface.co/datasets/databricks/databricks-dolly-15k) in an instruction tuning format. Dolly dataset contains roughly 15,000 instruction following records for various categories such as question answering, summarization, information extraction etc. It is available under Apache 2.0 license. We will select the summarization examples for fine-tuning.\n",
    "\n",
    "\n",
    "Training data is formatted in JSON lines (.jsonl) format, where each line is a dictionary representing a single data sample. All training data must be in a single folder, however it can be saved in multiple jsonl files. The training folder can also contain a template.json file describing the input and output formats.\n",
    "\n",
    "To train your model on a collection of unstructured dataset (text files), please see the section [Example fine-tuning with Domain-Adaptation dataset format](#Example-fine-tuning-with-Domain-Adaptation-dataset-format) in the Appendix.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dd20a0d-15a5-49b0-a330-a75755d046ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Creating json from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 48.77ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2082194"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dolly_dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "# To train for question answering/information extraction, you can replace the assertion in next line to example[\"category\"] == \"closed_qa\"/\"information_extraction\".\n",
    "summarization_dataset = dolly_dataset.filter(lambda example: example[\"category\"] == \"summarization\")\n",
    "summarization_dataset = summarization_dataset.remove_columns(\"category\")\n",
    "\n",
    "# We split the dataset into two where test data is used to evaluate at the end.\n",
    "train_and_test_dataset = summarization_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# Dumping the training data to a local file to be used for training.\n",
    "train_and_test_dataset[\"train\"].to_json(\"continued_pretraining.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9fbf002-3ee3-4cc8-8fce-871939f1bd19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Can you give me a summary of the story of the Vidhivilasa movie?',\n",
       " 'context': 'Vidhivilasa is a 1962 Kannada language swashbuckler film directed by S. V. Mahesh. The film stars Rajkumar, Leelavathi and K. S. Ashwath. The film is based on the story of a King who confronts destiny (fate), which appears to him in the form of a young lady, by challenging it on how he would die. The King tries in vain all means at his disposal to prevent events that destiny foretold would happen. Finally, destiny wins, employing a twisted turn of events. The film has musical score by T. Padman. The story, screenplay, dialogues and lyrics were written by H. L. Narayana Rao who happens to be the father of actor Vishnuvardhan.',\n",
       " 'response': 'A king is confronted by destiny in person of a young lady and tries by any means possible to avoid his fate.  However the king is not able to prevent his destiny.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_test_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e5489-33dc-4623-92da-f6fc97bd25ab",
   "metadata": {},
   "source": [
    "---\n",
    "Next, we create a prompt template for using the data in an instruction / input format for the training job (since we are instruction fine-tuning the model in this example), and also for inferencing the deployed endpoint.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90451114-7cf5-445c-88e3-02ccaa5d3a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "template = {\n",
    "    \"prompt\": \"What is the best way to deploy a generative model on AWS?\",\n",
    "    \"completion\": \" {response}\",\n",
    "}\n",
    "with open(\"template.json\", \"w\") as f:\n",
    "    json.dump(template, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22171b1-1cec-4cec-9ce4-db62761633d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Upload dataset to S3\n",
    "---\n",
    "\n",
    "We will upload the prepared dataset to S3 which will be used for fine-tuning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e1ee29a-8439-4788-8088-35a433fe2110",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: s3://sagemaker-us-east-1-079002598131/dolly_dataset\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "import sagemaker\n",
    "import random\n",
    "\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "local_data_file = \"continued_pretraining.jsonl\"\n",
    "train_data_location = f\"s3://{bucket}/dolly_dataset\"\n",
    "S3Uploader.upload(local_data_file, train_data_location)\n",
    "S3Uploader.upload(\"template.json\", train_data_location)\n",
    "print(f\"Training data: {train_data_location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011b28f9-b752-4ab9-a8a6-73b7c7ddb486",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 ls --recursive s3://sagemaker-us-east-1-079002598131/dolly_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e61340-bc81-477d-aaf1-f37e8c554863",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train the model\n",
    "---\n",
    "Next, we fine-tune the LLaMA v2 7B model on the summarization dataset from Dolly. Finetuning scripts are based on scripts provided by [this repo](https://github.com/facebookresearch/llama-recipes/tree/main). To learn more about the fine-tuning scripts, please checkout section [5. Few notes about the fine-tuning method](#5.-Few-notes-about-the-fine-tuning-method). For a list of supported hyper-parameters and their default values, please see section [3. Supported Hyper-parameters for fine-tuning](#3.-Supported-Hyper-parameters-for-fine-tuning).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a71087e-9c9e-42d7-999e-5f3fac07bc4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: meta-textgeneration-llama-2-7b-2023-12-23-15-26-00-310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-23 15:26:00 Starting - Starting the training job...\n",
      "2023-12-23 15:26:27 Starting - Preparing the instances for training.......................................\n",
      "2023-12-23 15:32:52 Downloading - Downloading input data.....................\n",
      "2023-12-23 15:36:28 Downloading - Downloading the training image............\n",
      "2023-12-23 15:38:09 Training - Training image download completed. Training in progress....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-12-23 15:38:54,656 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-12-23 15:38:54,710 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-12-23 15:38:54,719 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-12-23 15:38:54,720 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2023-12-23 15:38:58,656 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2023-12-23 15:38:58,709 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-12-23 15:38:58,718 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2023-12-23 15:38:58,720 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-12-23 15:39:02,656 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/accelerate/accelerate-0.21.0-py3-none-any.whl (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/bitsandbytes/bitsandbytes-0.39.1-py3-none-any.whl (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/black/black-23.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/brotli/Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/datasets/datasets-2.14.1-py3-none-any.whl (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/fire/fire-0.5.0.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/inflate64/inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/loralib/loralib-0.1.1-py3-none-any.whl (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/multivolumefile/multivolumefile-0.2.3-py3-none-any.whl (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/mypy-extensions/mypy_extensions-1.0.0-py3-none-any.whl (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pathspec/pathspec-0.11.1-py3-none-any.whl (from -r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/peft/peft-0.4.0-py3-none-any.whl (from -r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/py7zr/py7zr-0.20.5-py3-none-any.whl (from -r requirements.txt (line 13))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pybcj/pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pycryptodomex/pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyppmd/pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pytorch-triton/pytorch_triton-2.1.0+6e4932cda8-cp310-cp310-linux_x86_64.whl (from -r requirements.txt (line 17))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyzstd/pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 18))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/safetensors/safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 19))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/scipy/scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 20))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/termcolor/termcolor-2.3.0-py3-none-any.whl (from -r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/texttable/texttable-1.6.7-py2.py3-none-any.whl (from -r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenize-rt/tokenize_rt-5.1.0-py2.py3-none-any.whl (from -r requirements.txt (line 23))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenizers/tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 24))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/torch/torch-2.2.0.dev20231104+cu118-cp310-cp310-linux_x86_64.whl (from -r requirements.txt (line 25))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/transformers/transformers-4.31.0-py3-none-any.whl (from -r requirements.txt (line 26))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_script_utilities/sagemaker_jumpstart_script_utilities-1.1.9-py2.py3-none-any.whl (from -r requirements.txt (line 27))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_huggingface_script_utilities/sagemaker_jumpstart_huggingface_script_utilities-1.1.3-py2.py3-none-any.whl (from -r requirements.txt (line 28))\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (8.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: platformdirs>=2 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (12.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2023.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.8.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.14.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 6)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from pytorch-triton==2.1.0+6e4932cda8->-r requirements.txt (line 17)) (3.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 26)) (2023.5.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.9.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (1.3.0)\u001b[0m\n",
      "\u001b[34mtokenizers is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: fire\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): started\u001b[0m\n",
      "\u001b[35m2023-12-23 15:39:06,626 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35mProcessing ./lib/accelerate/accelerate-0.21.0-py3-none-any.whl (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/bitsandbytes/bitsandbytes-0.39.1-py3-none-any.whl (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/black/black-23.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/brotli/Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/datasets/datasets-2.14.1-py3-none-any.whl (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/fire/fire-0.5.0.tar.gz\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mProcessing ./lib/inflate64/inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/loralib/loralib-0.1.1-py3-none-any.whl (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/multivolumefile/multivolumefile-0.2.3-py3-none-any.whl (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/mypy-extensions/mypy_extensions-1.0.0-py3-none-any.whl (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/pathspec/pathspec-0.11.1-py3-none-any.whl (from -r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/peft/peft-0.4.0-py3-none-any.whl (from -r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/py7zr/py7zr-0.20.5-py3-none-any.whl (from -r requirements.txt (line 13))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/pybcj/pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/pycryptodomex/pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/pyppmd/pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/pytorch-triton/pytorch_triton-2.1.0+6e4932cda8-cp310-cp310-linux_x86_64.whl (from -r requirements.txt (line 17))\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=68b2ae44f66a45c90e83f2b9952309fe9686caee746e4db1cb6ff2f4c868eb92\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/db/3d/41/7e69dca5f61e37d109a4457082ffc5c6edb55ab633bafded38\u001b[0m\n",
      "\u001b[34mSuccessfully built fire\u001b[0m\n",
      "\u001b[35mProcessing ./lib/pyzstd/pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 18))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/safetensors/safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 19))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/scipy/scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 20))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/termcolor/termcolor-2.3.0-py3-none-any.whl (from -r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/texttable/texttable-1.6.7-py2.py3-none-any.whl (from -r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/tokenize-rt/tokenize_rt-5.1.0-py2.py3-none-any.whl (from -r requirements.txt (line 23))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/tokenizers/tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 24))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/torch/torch-2.2.0.dev20231104+cu118-cp310-cp310-linux_x86_64.whl (from -r requirements.txt (line 25))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/transformers/transformers-4.31.0-py3-none-any.whl (from -r requirements.txt (line 26))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/sagemaker_jumpstart_script_utilities/sagemaker_jumpstart_script_utilities-1.1.9-py2.py3-none-any.whl (from -r requirements.txt (line 27))\u001b[0m\n",
      "\u001b[35mProcessing ./lib/sagemaker_jumpstart_huggingface_script_utilities/sagemaker_jumpstart_huggingface_script_utilities-1.1.3-py2.py3-none-any.whl (from -r requirements.txt (line 28))\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (5.9.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (8.1.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: platformdirs>=2 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (3.5.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (12.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.3.6)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.28.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (4.65.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.70.14)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2023.5.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.8.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.14.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 6)) (1.16.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from pytorch-triton==2.1.0+6e4932cda8->-r requirements.txt (line 17)) (3.12.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (4.5.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (1.11.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (3.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (3.1.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 26)) (2023.5.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (22.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (3.1.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (6.0.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (4.0.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.9.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.3.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.3.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (3.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (1.26.15)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: texttable, safetensors, Brotli, bitsandbytes, tokenize-rt, termcolor, scipy, sagemaker-jumpstart-script-utilities, sagemaker-jumpstart-huggingface-script-utilities, pyzstd, pytorch-triton, pyppmd, pycryptodomex, pybcj, pathspec, mypy-extensions, multivolumefile, loralib, inflate64, torch, py7zr, fire, black, transformers, accelerate, peft, datasets\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (2.1.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2.8.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (1.3.0)\u001b[0m\n",
      "\u001b[35mtokenizers is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: fire\u001b[0m\n",
      "\u001b[35mBuilding wheel for fire (setup.py): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for fire (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=e6df2bcb81511021d4cc9197dd3fc46e0df6ace5c572b1764913542038715e9d\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/db/3d/41/7e69dca5f61e37d109a4457082ffc5c6edb55ab633bafded38\u001b[0m\n",
      "\u001b[35mSuccessfully built fire\u001b[0m\n",
      "\u001b[34mAttempting uninstall: scipy\u001b[0m\n",
      "\u001b[34mFound existing installation: scipy 1.10.1\u001b[0m\n",
      "\u001b[34mUninstalling scipy-1.10.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled scipy-1.10.1\u001b[0m\n",
      "\u001b[35mInstalling collected packages: texttable, safetensors, Brotli, bitsandbytes, tokenize-rt, termcolor, scipy, sagemaker-jumpstart-script-utilities, sagemaker-jumpstart-huggingface-script-utilities, pyzstd, pytorch-triton, pyppmd, pycryptodomex, pybcj, pathspec, mypy-extensions, multivolumefile, loralib, inflate64, torch, py7zr, fire, black, transformers, accelerate, peft, datasets\u001b[0m\n",
      "\u001b[35mAttempting uninstall: scipy\u001b[0m\n",
      "\u001b[35mFound existing installation: scipy 1.10.1\u001b[0m\n",
      "\u001b[35mUninstalling scipy-1.10.1:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled scipy-1.10.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: torch\u001b[0m\n",
      "\u001b[34mFound existing installation: torch 2.0.0\u001b[0m\n",
      "\u001b[34mUninstalling torch-2.0.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled torch-2.0.0\u001b[0m\n",
      "\u001b[35mAttempting uninstall: torch\u001b[0m\n",
      "\u001b[35mFound existing installation: torch 2.0.0\u001b[0m\n",
      "\u001b[35mUninstalling torch-2.0.0:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled torch-2.0.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[35mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[35mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[35mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.12.0\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.12.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.12.0\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mfastai 2.7.12 requires torch<2.1,>=1.7, but you have torch 2.2.0.dev20231104+cu118 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed Brotli-1.0.9 accelerate-0.21.0 bitsandbytes-0.39.1 black-23.7.0 datasets-2.14.1 fire-0.5.0 inflate64-0.3.1 loralib-0.1.1 multivolumefile-0.2.3 mypy-extensions-1.0.0 pathspec-0.11.1 peft-0.4.0 py7zr-0.20.5 pybcj-1.0.1 pycryptodomex-3.18.0 pyppmd-1.0.0 pytorch-triton-2.1.0+6e4932cda8 pyzstd-0.15.9 safetensors-0.3.1 sagemaker-jumpstart-huggingface-script-utilities-1.1.3 sagemaker-jumpstart-script-utilities-1.1.9 scipy-1.11.1 termcolor-2.3.0 texttable-1.6.7 tokenize-rt-5.1.0 torch-2.2.0.dev20231104+cu118 transformers-4.31.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2023-12-23 15:39:57,558 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-12-23 15:39:57,558 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-12-23 15:39:57,638 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-12-23 15:39:57,701 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-12-23 15:39:57,764 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-12-23 15:39:57,774 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"code\": \"/opt/ml/input/data/code\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-2\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"add_input_output_demarcation_key\": \"True\",\n",
      "        \"chat_dataset\": \"False\",\n",
      "        \"enable_fsdp\": \"True\",\n",
      "        \"epoch\": \"5\",\n",
      "        \"instruction_tuned\": \"True\",\n",
      "        \"int8_quantization\": \"False\",\n",
      "        \"learning_rate\": \"0.0001\",\n",
      "        \"lora_alpha\": \"32\",\n",
      "        \"lora_dropout\": \"0.05\",\n",
      "        \"lora_r\": \"8\",\n",
      "        \"max_input_length\": \"1024\",\n",
      "        \"max_train_samples\": \"-1\",\n",
      "        \"max_val_samples\": \"-1\",\n",
      "        \"per_device_eval_batch_size\": \"1\",\n",
      "        \"per_device_train_batch_size\": \"4\",\n",
      "        \"preprocessing_num_workers\": \"None\",\n",
      "        \"seed\": \"10\",\n",
      "        \"train_data_split_seed\": \"0\",\n",
      "        \"validation_split_ratio\": \"0.2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"code\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-2\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"meta-textgeneration-llama-2-7b-2023-12-23-15-26-00-310\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/input/data/code/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-2\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"add_input_output_demarcation_key\":\"True\",\"chat_dataset\":\"False\",\"enable_fsdp\":\"True\",\"epoch\":\"5\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"code\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-2\",\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/input/data/code/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"code\":\"/opt/ml/input/data/code\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-2\",\"algo-1\"],\"current_instance_type\":\"ml.g5.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"add_input_output_demarcation_key\":\"True\",\"chat_dataset\":\"False\",\"enable_fsdp\":\"True\",\"epoch\":\"5\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"meta-textgeneration-llama-2-7b-2023-12-23-15-26-00-310\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/input/data/code/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--add_input_output_demarcation_key\",\"True\",\"--chat_dataset\",\"False\",\"--enable_fsdp\",\"True\",\"--epoch\",\"5\",\"--instruction_tuned\",\"True\",\"--int8_quantization\",\"False\",\"--learning_rate\",\"0.0001\",\"--lora_alpha\",\"32\",\"--lora_dropout\",\"0.05\",\"--lora_r\",\"8\",\"--max_input_length\",\"1024\",\"--max_train_samples\",\"-1\",\"--max_val_samples\",\"-1\",\"--per_device_eval_batch_size\",\"1\",\"--per_device_train_batch_size\",\"4\",\"--preprocessing_num_workers\",\"None\",\"--seed\",\"10\",\"--train_data_split_seed\",\"0\",\"--validation_split_ratio\",\"0.2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CODE=/opt/ml/input/data/code\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_ADD_INPUT_OUTPUT_DEMARCATION_KEY=True\u001b[0m\n",
      "\u001b[34mSM_HP_CHAT_DATASET=False\u001b[0m\n",
      "\u001b[34mSM_HP_ENABLE_FSDP=True\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCH=5\u001b[0m\n",
      "\u001b[34mSM_HP_INSTRUCTION_TUNED=True\u001b[0m\n",
      "\u001b[34mSM_HP_INT8_QUANTIZATION=False\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0001\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_ALPHA=32\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_DROPOUT=0.05\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_R=8\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_INPUT_LENGTH=1024\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_TRAIN_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_VAL_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_PREPROCESSING_NUM_WORKERS=None\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=10\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_DATA_SPLIT_SEED=0\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_SPLIT_RATIO=0.2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 transfer_learning.py --add_input_output_demarcation_key True --chat_dataset False --enable_fsdp True --epoch 5 --instruction_tuned True --int8_quantization False --learning_rate 0.0001 --lora_alpha 32 --lora_dropout 0.05 --lora_r 8 --max_input_length 1024 --max_train_samples -1 --max_val_samples -1 --per_device_eval_batch_size 1 --per_device_train_batch_size 4 --preprocessing_num_workers None --seed 10 --train_data_split_seed 0 --validation_split_ratio 0.2\u001b[0m\n",
      "\u001b[34m2023-12-23 15:39:57,801 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[35mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[35mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[35mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[35mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[35mFound existing installation: datasets 2.12.0\u001b[0m\n",
      "\u001b[35mUninstalling datasets-2.12.0:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled datasets-2.12.0\u001b[0m\n",
      "\u001b[35mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[35mfastai 2.7.12 requires torch<2.1,>=1.7, but you have torch 2.2.0.dev20231104+cu118 which is incompatible.\u001b[0m\n",
      "\u001b[35mSuccessfully installed Brotli-1.0.9 accelerate-0.21.0 bitsandbytes-0.39.1 black-23.7.0 datasets-2.14.1 fire-0.5.0 inflate64-0.3.1 loralib-0.1.1 multivolumefile-0.2.3 mypy-extensions-1.0.0 pathspec-0.11.1 peft-0.4.0 py7zr-0.20.5 pybcj-1.0.1 pycryptodomex-3.18.0 pyppmd-1.0.0 pytorch-triton-2.1.0+6e4932cda8 pyzstd-0.15.9 safetensors-0.3.1 sagemaker-jumpstart-huggingface-script-utilities-1.1.3 sagemaker-jumpstart-script-utilities-1.1.9 scipy-1.11.1 termcolor-2.3.0 texttable-1.6.7 tokenize-rt-5.1.0 torch-2.2.0.dev20231104+cu118 transformers-4.31.0\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m2023-12-23 15:40:01,354 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2023-12-23 15:40:01,354 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2023-12-23 15:40:01,409 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-12-23 15:40:01,472 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-12-23 15:40:01,537 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-12-23 15:40:01,546 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"code\": \"/opt/ml/input/data/code\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-2\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"add_input_output_demarcation_key\": \"True\",\n",
      "        \"chat_dataset\": \"False\",\n",
      "        \"enable_fsdp\": \"True\",\n",
      "        \"epoch\": \"5\",\n",
      "        \"instruction_tuned\": \"True\",\n",
      "        \"int8_quantization\": \"False\",\n",
      "        \"learning_rate\": \"0.0001\",\n",
      "        \"lora_alpha\": \"32\",\n",
      "        \"lora_dropout\": \"0.05\",\n",
      "        \"lora_r\": \"8\",\n",
      "        \"max_input_length\": \"1024\",\n",
      "        \"max_train_samples\": \"-1\",\n",
      "        \"max_val_samples\": \"-1\",\n",
      "        \"per_device_eval_batch_size\": \"1\",\n",
      "        \"per_device_train_batch_size\": \"4\",\n",
      "        \"preprocessing_num_workers\": \"None\",\n",
      "        \"seed\": \"10\",\n",
      "        \"train_data_split_seed\": \"0\",\n",
      "        \"validation_split_ratio\": \"0.2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"code\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-2\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"meta-textgeneration-llama-2-7b-2023-12-23-15-26-00-310\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/input/data/code/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-2\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"add_input_output_demarcation_key\":\"True\",\"chat_dataset\":\"False\",\"enable_fsdp\":\"True\",\"epoch\":\"5\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"code\",\"training\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-2\",\"algo-1\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}}\u001b[0m\n",
      "\u001b[35mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[35mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[35mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=/opt/ml/input/data/code/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"code\":\"/opt/ml/input/data/code\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-2\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-2\",\"algo-1\"],\"current_instance_type\":\"ml.g5.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"add_input_output_demarcation_key\":\"True\",\"chat_dataset\":\"False\",\"enable_fsdp\":\"True\",\"epoch\":\"5\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"meta-textgeneration-llama-2-7b-2023-12-23-15-26-00-310\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/input/data/code/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--add_input_output_demarcation_key\",\"True\",\"--chat_dataset\",\"False\",\"--enable_fsdp\",\"True\",\"--epoch\",\"5\",\"--instruction_tuned\",\"True\",\"--int8_quantization\",\"False\",\"--learning_rate\",\"0.0001\",\"--lora_alpha\",\"32\",\"--lora_dropout\",\"0.05\",\"--lora_r\",\"8\",\"--max_input_length\",\"1024\",\"--max_train_samples\",\"-1\",\"--max_val_samples\",\"-1\",\"--per_device_eval_batch_size\",\"1\",\"--per_device_train_batch_size\",\"4\",\"--preprocessing_num_workers\",\"None\",\"--seed\",\"10\",\"--train_data_split_seed\",\"0\",\"--validation_split_ratio\",\"0.2\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_CODE=/opt/ml/input/data/code\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[35mSM_HP_ADD_INPUT_OUTPUT_DEMARCATION_KEY=True\u001b[0m\n",
      "\u001b[35mSM_HP_CHAT_DATASET=False\u001b[0m\n",
      "\u001b[35mSM_HP_ENABLE_FSDP=True\u001b[0m\n",
      "\u001b[35mSM_HP_EPOCH=5\u001b[0m\n",
      "\u001b[35mSM_HP_INSTRUCTION_TUNED=True\u001b[0m\n",
      "\u001b[35mSM_HP_INT8_QUANTIZATION=False\u001b[0m\n",
      "\u001b[35mSM_HP_LEARNING_RATE=0.0001\u001b[0m\n",
      "\u001b[35mSM_HP_LORA_ALPHA=32\u001b[0m\n",
      "\u001b[35mSM_HP_LORA_DROPOUT=0.05\u001b[0m\n",
      "\u001b[35mSM_HP_LORA_R=8\u001b[0m\n",
      "\u001b[35mSM_HP_MAX_INPUT_LENGTH=1024\u001b[0m\n",
      "\u001b[35mSM_HP_MAX_TRAIN_SAMPLES=-1\u001b[0m\n",
      "\u001b[35mSM_HP_MAX_VAL_SAMPLES=-1\u001b[0m\n",
      "\u001b[35mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[35mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[35mSM_HP_PREPROCESSING_NUM_WORKERS=None\u001b[0m\n",
      "\u001b[35mSM_HP_SEED=10\u001b[0m\n",
      "\u001b[35mSM_HP_TRAIN_DATA_SPLIT_SEED=0\u001b[0m\n",
      "\u001b[35mSM_HP_VALIDATION_SPLIT_RATIO=0.2\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.10 transfer_learning.py --add_input_output_demarcation_key True --chat_dataset False --enable_fsdp True --epoch 5 --instruction_tuned True --int8_quantization False --learning_rate 0.0001 --lora_alpha 32 --lora_dropout 0.05 --lora_r 8 --max_input_length 1024 --max_train_samples -1 --max_val_samples -1 --per_device_eval_batch_size 1 --per_device_train_batch_size 4 --preprocessing_num_workers None --seed 10 --train_data_split_seed 0 --validation_split_ratio 0.2\u001b[0m\n",
      "\u001b[35m2023-12-23 15:40:01,573 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mINFO:root:Using pre-trained artifacts in SAGEMAKER_ADDITIONAL_S3_DATA_PATH=/opt/ml/additonals3data\u001b[0m\n",
      "\u001b[34mINFO:root:Identify file serving.properties in the un-tar directory /opt/ml/additonals3data. Copying it over to /opt/ml/model for model deployment after training is finished.\u001b[0m\n",
      "\u001b[34mINFO:root:Invoking the training command ['torchrun', '--nnodes', '1', '--nproc_per_node', '4', 'llama_finetuning.py', '--model_name', '/opt/ml/additonals3data', '--num_gpus', '4', '--pure_bf16', '--dist_checkpoint_root_folder', 'model_checkpoints', '--dist_checkpoint_folder', 'fine-tuned', '--batch_size_training', '4', '--micro_batch_size', '4', '--train_file', '/opt/ml/input/data/training', '--lr', '0.0001', '--do_train', '--output_dir', 'saved_peft_model', '--num_epochs', '5', '--use_peft', '--peft_method', 'lora', '--max_train_samples', '-1', '--max_val_samples', '-1', '--seed', '10', '--per_device_eval_batch_size', '1', '--max_input_length', '1024', '--preprocessing_num_workers', '--None', '--validation_split_ratio', '0.2', '--train_data_split_seed', '0', '--num_workers_dataloader', '0', '--weight_decay', '0.1', '--lora_r', '8', '--lora_alpha', '32', '--lora_dropout', '0.05', '--enable_fsdp', '--add_input_output_demarcation_key', '--instruction_tuned'].\u001b[0m\n",
      "\u001b[35m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[35mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[35mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[35m================================================================================\u001b[0m\n",
      "\u001b[35mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[35mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[35mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[35mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[35mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m[2023-12-23 15:40:03,065] torch.distributed.run: [WARNING] \u001b[0m\n",
      "\u001b[34m[2023-12-23 15:40:03,065] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m[2023-12-23 15:40:03,065] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m[2023-12-23 15:40:03,065] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[35mINFO:root:Using pre-trained artifacts in SAGEMAKER_ADDITIONAL_S3_DATA_PATH=/opt/ml/additonals3data\u001b[0m\n",
      "\u001b[35mINFO:root:Identify file serving.properties in the un-tar directory /opt/ml/additonals3data. Copying it over to /opt/ml/model for model deployment after training is finished.\u001b[0m\n",
      "\u001b[35mINFO:root:Invoking the training command ['torchrun', '--nnodes', '1', '--nproc_per_node', '4', 'llama_finetuning.py', '--model_name', '/opt/ml/additonals3data', '--num_gpus', '4', '--pure_bf16', '--dist_checkpoint_root_folder', 'model_checkpoints', '--dist_checkpoint_folder', 'fine-tuned', '--batch_size_training', '4', '--micro_batch_size', '4', '--train_file', '/opt/ml/input/data/training', '--lr', '0.0001', '--do_train', '--output_dir', 'saved_peft_model', '--num_epochs', '5', '--use_peft', '--peft_method', 'lora', '--max_train_samples', '-1', '--max_val_samples', '-1', '--seed', '10', '--per_device_eval_batch_size', '1', '--max_input_length', '1024', '--preprocessing_num_workers', '--None', '--validation_split_ratio', '0.2', '--train_data_split_seed', '0', '--num_workers_dataloader', '0', '--weight_decay', '0.1', '--lora_r', '8', '--lora_alpha', '32', '--lora_dropout', '0.05', '--enable_fsdp', '--add_input_output_demarcation_key', '--instruction_tuned'].\u001b[0m\n",
      "\u001b[35m[2023-12-23 15:40:06,881] torch.distributed.run: [WARNING] \u001b[0m\n",
      "\u001b[35m[2023-12-23 15:40:06,881] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[35m[2023-12-23 15:40:06,881] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[35m[2023-12-23 15:40:06,881] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 1. Rank is 1\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 1\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 0. Rank is 0\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 0\u001b[0m\n",
      "\u001b[34m--> Running with torch dist debug set to detail\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 2. Rank is 2\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 2\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 3. Rank is 3\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 3\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 1/1 [00:00<00:00, 12336.19it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|██████████| 1/1 [00:00<00:00, 1303.39it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mGenerating train split: 2138 examples [00:00, 64817.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2138 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2138 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2138 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2138 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  89%|████████▉ | 1912/2138 [00:00<00:00, 19008.13 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2138/2138 [00:00<00:00, 18453.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2138 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  84%|████████▍ | 1805/2138 [00:00<00:00, 17939.45 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 2000/2138 [00:00<00:00, 18777.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2138/2138 [00:00<00:00, 18630.74 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  88%|████████▊ | 1871/2138 [00:00<00:00, 18594.53 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2138 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2138/2138 [00:00<00:00, 17420.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2138 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2138/2138 [00:00<00:00, 18078.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2138 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[35mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[35mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[35m================================================================================\u001b[0m\n",
      "\u001b[35m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[35mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[35mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[35m================================================================================\u001b[0m\n",
      "\u001b[35mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[35mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[35mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[35mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[35mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[35m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[35mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[35mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[35m================================================================================\u001b[0m\n",
      "\u001b[35mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[35mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[35mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[35mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[35mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[35mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[35mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[35mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[35mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[35mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[35m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[35mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[35mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[35m================================================================================\u001b[0m\n",
      "\u001b[35mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[35mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[35mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[35mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[35mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mMap:  47%|████▋     | 1000/2138 [00:02<00:02, 390.56 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  47%|████▋     | 1000/2138 [00:02<00:02, 391.73 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  47%|████▋     | 1000/2138 [00:02<00:02, 389.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  47%|████▋     | 1000/2138 [00:02<00:02, 385.65 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 2000/2138 [00:05<00:00, 395.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 2000/2138 [00:05<00:00, 394.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 2000/2138 [00:05<00:00, 394.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 2000/2138 [00:05<00:00, 392.20 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2138/2138 [00:05<00:00, 399.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2138/2138 [00:05<00:00, 397.13 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2138 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2138/2138 [00:05<00:00, 397.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2138/2138 [00:05<00:00, 396.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2138 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2138/2138 [00:05<00:00, 398.04 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2138/2138 [00:05<00:00, 395.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2138 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2138/2138 [00:05<00:00, 396.02 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2138/2138 [00:05<00:00, 393.49 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/2138 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  47%|████▋     | 1000/2138 [00:00<00:00, 1766.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  47%|████▋     | 1000/2138 [00:00<00:00, 1853.47 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  47%|████▋     | 1000/2138 [00:00<00:00, 1744.82 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  47%|████▋     | 1000/2138 [00:00<00:00, 1714.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 2000/2138 [00:01<00:00, 1812.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 2000/2138 [00:01<00:00, 1851.36 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2138/2138 [00:01<00:00, 1808.56 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 2000/2138 [00:01<00:00, 1749.36 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2138/2138 [00:01<00:00, 1856.15 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 2000/2138 [00:01<00:00, 1735.98 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2138/2138 [00:01<00:00, 1751.72 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[35mINFO:root:Local rank is 2. Rank is 2\u001b[0m\n",
      "\u001b[35mINFO:root:Setting torch device = 2\u001b[0m\n",
      "\u001b[35mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[35mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[35mINFO:root:Local rank is 1. Rank is 1\u001b[0m\n",
      "\u001b[35mINFO:root:Setting torch device = 1\u001b[0m\n",
      "\u001b[35mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[35mINFO:root:Local rank is 3. Rank is 3\u001b[0m\n",
      "\u001b[35mINFO:root:Setting torch device = 3\u001b[0m\n",
      "\u001b[35mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[35mDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mDownloading data files: 100%|██████████| 1/1 [00:00<00:00, 11397.57it/s]\u001b[0m\n",
      "\u001b[35mExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mExtracting data files: 100%|██████████| 1/1 [00:00<00:00, 1236.89it/s]\u001b[0m\n",
      "\u001b[35mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[35mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[35mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[35mINFO:root:Local rank is 0. Rank is 0\u001b[0m\n",
      "\u001b[35mINFO:root:Setting torch device = 0\u001b[0m\n",
      "\u001b[35m--> Running with torch dist debug set to detail\u001b[0m\n",
      "\u001b[35mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[35mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[35mGenerating train split: 2138 examples [00:00, 64228.27 examples/s]\u001b[0m\n",
      "\u001b[35mMap:   0%|          | 0/2138 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mMap:   0%|          | 0/2138 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mMap:   0%|          | 0/2138 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mMap:   0%|          | 0/2138 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mMap:  88%|████████▊ | 1878/2138 [00:00<00:00, 18676.80 examples/s]\u001b[0m\n",
      "\u001b[35mMap:  88%|████████▊ | 1887/2138 [00:00<00:00, 18743.15 examples/s]\u001b[0m\n",
      "\u001b[35mMap: 100%|██████████| 2138/2138 [00:00<00:00, 18170.66 examples/s]\u001b[0m\n",
      "\u001b[35mMap:   0%|          | 0/2138 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mMap: 100%|██████████| 2138/2138 [00:00<00:00, 18108.83 examples/s]\u001b[0m\n",
      "\u001b[35mMap:   0%|          | 0/2138 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mMap:  86%|████████▋ | 1845/2138 [00:00<00:00, 18342.96 examples/s]\u001b[0m\n",
      "\u001b[35mMap: 100%|██████████| 2138/2138 [00:00<00:00, 17744.62 examples/s]\u001b[0m\n",
      "\u001b[35mMap:   0%|          | 0/2138 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mMap:  88%|████████▊ | 1891/2138 [00:00<00:00, 18777.15 examples/s]\u001b[0m\n",
      "\u001b[35mMap: 100%|██████████| 2138/2138 [00:00<00:00, 18242.41 examples/s]\u001b[0m\n",
      "\u001b[35mMap:   0%|          | 0/2138 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mMap:  47%|████▋     | 1000/2138 [00:02<00:02, 389.51 examples/s]\u001b[0m\n",
      "\u001b[35mMap:  47%|████▋     | 1000/2138 [00:02<00:02, 390.62 examples/s]\u001b[0m\n",
      "\u001b[35mMap:  47%|████▋     | 1000/2138 [00:02<00:02, 388.98 examples/s]\u001b[0m\n",
      "\u001b[35mMap:  47%|████▋     | 1000/2138 [00:02<00:02, 388.36 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 2138/2138 [00:01<00:00, 1736.46 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[35mMap:  94%|█████████▎| 2000/2138 [00:05<00:00, 395.34 examples/s]\u001b[0m\n",
      "\u001b[35mMap:  94%|█████████▎| 2000/2138 [00:05<00:00, 394.95 examples/s]\u001b[0m\n",
      "\u001b[35mMap:  94%|█████████▎| 2000/2138 [00:05<00:00, 393.04 examples/s]\u001b[0m\n",
      "\u001b[35mMap:  94%|█████████▎| 2000/2138 [00:05<00:00, 393.63 examples/s]\u001b[0m\n",
      "\u001b[35mMap: 100%|██████████| 2138/2138 [00:05<00:00, 398.97 examples/s]\u001b[0m\n",
      "\u001b[35mMap: 100%|██████████| 2138/2138 [00:05<00:00, 396.63 examples/s]\u001b[0m\n",
      "\u001b[35mMap:   0%|          | 0/2138 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mMap: 100%|██████████| 2138/2138 [00:05<00:00, 398.65 examples/s]\u001b[0m\n",
      "\u001b[35mMap: 100%|██████████| 2138/2138 [00:05<00:00, 396.50 examples/s]\u001b[0m\n",
      "\u001b[35mMap:   0%|          | 0/2138 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mMap: 100%|██████████| 2138/2138 [00:05<00:00, 396.67 examples/s]\u001b[0m\n",
      "\u001b[35mMap: 100%|██████████| 2138/2138 [00:05<00:00, 394.58 examples/s]\u001b[0m\n",
      "\u001b[35mMap:   0%|          | 0/2138 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mMap: 100%|██████████| 2138/2138 [00:05<00:00, 397.23 examples/s]\u001b[0m\n",
      "\u001b[35mMap: 100%|██████████| 2138/2138 [00:05<00:00, 394.97 examples/s]\u001b[0m\n",
      "\u001b[35mMap:   0%|          | 0/2138 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mMap:  47%|████▋     | 1000/2138 [00:00<00:00, 1847.68 examples/s]\u001b[0m\n",
      "\u001b[35mMap:  47%|████▋     | 1000/2138 [00:00<00:00, 1736.05 examples/s]\u001b[0m\n",
      "\u001b[35mMap:  47%|████▋     | 1000/2138 [00:00<00:00, 1749.12 examples/s]\u001b[0m\n",
      "\u001b[35mMap:  47%|████▋     | 1000/2138 [00:00<00:00, 1855.73 examples/s]\u001b[0m\n",
      "\u001b[35mMap:  94%|█████████▎| 2000/2138 [00:01<00:00, 1846.16 examples/s]\u001b[0m\n",
      "\u001b[35mMap: 100%|██████████| 2138/2138 [00:01<00:00, 1848.09 examples/s]\u001b[0m\n",
      "\u001b[35mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[35mMap:  94%|█████████▎| 2000/2138 [00:01<00:00, 1749.36 examples/s]\u001b[0m\n",
      "\u001b[35mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[35mMap:  94%|█████████▎| 2000/2138 [00:01<00:00, 1857.05 examples/s]\u001b[0m\n",
      "\u001b[35mMap:  94%|█████████▎| 2000/2138 [00:01<00:00, 1764.79 examples/s]\u001b[0m\n",
      "\u001b[35mMap: 100%|██████████| 2138/2138 [00:01<00:00, 1752.41 examples/s]\u001b[0m\n",
      "\u001b[35mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[35mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[35mMap: 100%|██████████| 2138/2138 [00:01<00:00, 1860.57 examples/s]\u001b[0m\n",
      "\u001b[35mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[35mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[35mMap: 100%|██████████| 2138/2138 [00:01<00:00, 1766.41 examples/s]\u001b[0m\n",
      "\u001b[35mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[35mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.88s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.53s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.42s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.35s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.18s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.83s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.27s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.88s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.45s/it]#015Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.30s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.97s/it]\u001b[0m\n",
      "\u001b[34m#015Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.92s/it]\u001b[0m\n",
      "\u001b[34m--> Model /opt/ml/additonals3data\u001b[0m\n",
      "\u001b[34m--> /opt/ml/additonals3data has 6738.415616 Million params\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.54s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.26s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.18s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.19s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.14s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.65s/it]\u001b[0m\n",
      "\u001b[35m--> Model /opt/ml/additonals3data\u001b[0m\n",
      "\u001b[35m--> /opt/ml/additonals3data has 6738.415616 Million params\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.21s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.82s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.18s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.78s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.19s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.79s/it]\u001b[0m\n",
      "\u001b[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[34mbFloat16 enabled for mixed precision - using bfSixteen policy\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34mINFO:root:--> Training Set Length = 888\u001b[0m\n",
      "\u001b[34mINFO:root:--> Validation Set Length = 223\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/55 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/55 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/55 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mNCCL version 2.19.3+cuda11.8\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/55 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[35mbFloat16 enabled for mixed precision - using bfSixteen policy\u001b[0m\n",
      "\u001b[35mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[35mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[35mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[35m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[35m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[35mINFO:root:--> Training Set Length = 888\u001b[0m\n",
      "\u001b[35mINFO:root:--> Validation Set Length = 223\u001b[0m\n",
      "\u001b[35m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35mTraining Epoch0:   0%|#033[34m          #033[0m| 0/55 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35mTraining Epoch0:   0%|#033[34m          #033[0m| 0/55 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35mTraining Epoch0:   0%|#033[34m          #033[0m| 0/55 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mNCCL version 2.19.3+cuda11.8\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35mTraining Epoch0:   0%|#033[34m          #033[0m| 0/55 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 1/55 [00:10<09:26, 10.49s/it]#015Training Epoch0:   2%|#033[34m▏         #033[0m| 1/55 [00:10<09:36, 10.67s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 1/55 [00:10<09:41, 10.77s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.5590364933013916\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 1/55 [00:10<09:35, 10.66s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:   2%|#033[34m▏         #033[0m| 1/55 [00:10<09:36, 10.67s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:   2%|#033[34m▏         #033[0m| 1/55 [00:10<09:27, 10.51s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:   2%|#033[34m▏         #033[0m| 1/55 [00:10<09:38, 10.72s/it]\u001b[0m\n",
      "\u001b[35mstep 0 is completed and loss is 1.5590364933013916\u001b[0m\n",
      "\u001b[35mTraining Epoch0:   2%|#033[34m▏         #033[0m| 1/55 [00:10<09:40, 10.74s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 1.610149621963501\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 2/55 [00:20<08:59, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 2/55 [00:20<08:56, 10.12s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 2/55 [00:20<09:02, 10.23s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 2/55 [00:20<09:00, 10.19s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:   4%|#033[34m▎         #033[0m| 2/55 [00:20<08:57, 10.14s/it]\u001b[0m\n",
      "\u001b[35mstep 1 is completed and loss is 1.610149621963501\u001b[0m\n",
      "\u001b[35mTraining Epoch0:   4%|#033[34m▎         #033[0m| 2/55 [00:20<09:02, 10.23s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:   4%|#033[34m▎         #033[0m| 2/55 [00:20<09:00, 10.20s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:   4%|#033[34m▎         #033[0m| 2/55 [00:20<09:01, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 3/55 [00:30<08:42, 10.06s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 3/55 [00:30<08:41, 10.03s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 3/55 [00:30<08:39, 10.00s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 1.741499662399292\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 3/55 [00:30<08:41, 10.03s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:   5%|#033[34m▌         #033[0m| 3/55 [00:30<08:42, 10.06s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:   5%|#033[34m▌         #033[0m| 3/55 [00:30<08:43, 10.07s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:   5%|#033[34m▌         #033[0m| 3/55 [00:30<08:41, 10.02s/it]\u001b[0m\n",
      "\u001b[35mstep 2 is completed and loss is 1.741499662399292\u001b[0m\n",
      "\u001b[35mTraining Epoch0:   5%|#033[34m▌         #033[0m| 3/55 [00:30<08:43, 10.07s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 1.4991017580032349\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 4/55 [00:40<08:27,  9.96s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 4/55 [00:40<08:28,  9.96s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 4/55 [00:40<08:28,  9.97s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 4/55 [00:40<08:26,  9.94s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:   7%|#033[34m▋         #033[0m| 4/55 [00:40<08:29,  9.99s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:   7%|#033[34m▋         #033[0m| 4/55 [00:40<08:29,  9.99s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:   7%|#033[34m▋         #033[0m| 4/55 [00:40<08:28,  9.97s/it]\u001b[0m\n",
      "\u001b[35mstep 3 is completed and loss is 1.4991017580032349\u001b[0m\n",
      "\u001b[35mTraining Epoch0:   7%|#033[34m▋         #033[0m| 4/55 [00:40<08:29, 10.00s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 1.5530177354812622\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 5/55 [00:50<08:15,  9.91s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 5/55 [00:50<08:15,  9.91s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 5/55 [00:49<08:14,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 5/55 [00:50<08:16,  9.92s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:   9%|#033[34m▉         #033[0m| 5/55 [00:50<08:17,  9.95s/it]\u001b[0m\n",
      "\u001b[35mstep 4 is completed and loss is 1.5530177354812622\u001b[0m\n",
      "\u001b[35mTraining Epoch0:   9%|#033[34m▉         #033[0m| 5/55 [00:50<08:17,  9.95s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:   9%|#033[34m▉         #033[0m| 5/55 [00:50<08:16,  9.93s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:   9%|#033[34m▉         #033[0m| 5/55 [00:50<08:17,  9.95s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 6/55 [00:59<08:04,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 6/55 [01:00<08:05,  9.90s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.5788195133209229\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 6/55 [00:59<08:04,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 6/55 [00:59<08:04,  9.89s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  11%|#033[34m█         #033[0m| 6/55 [00:59<08:05,  9.92s/it]\u001b[0m\n",
      "\u001b[35mstep 5 is completed and loss is 1.5788195133209229\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  11%|#033[34m█         #033[0m| 6/55 [01:00<08:06,  9.93s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  11%|#033[34m█         #033[0m| 6/55 [01:00<08:06,  9.93s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  11%|#033[34m█         #033[0m| 6/55 [01:00<08:06,  9.93s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 7/55 [01:09<07:53,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 7/55 [01:09<07:53,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 7/55 [01:09<07:54,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.2296397686004639\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 7/55 [01:09<07:53,  9.87s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 7/55 [01:09<07:55,  9.90s/it]\u001b[0m\n",
      "\u001b[35mstep 6 is completed and loss is 1.2296397686004639\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 7/55 [01:10<07:55,  9.91s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 7/55 [01:09<07:55,  9.91s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 7/55 [01:09<07:55,  9.91s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 8/55 [01:19<07:43,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 1.295676827430725\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 8/55 [01:19<07:43,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 8/55 [01:19<07:43,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 8/55 [01:19<07:43,  9.87s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 8/55 [01:19<07:45,  9.90s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 8/55 [01:19<07:44,  9.89s/it]\u001b[0m\n",
      "\u001b[35mstep 7 is completed and loss is 1.295676827430725\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 8/55 [01:19<07:45,  9.90s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 8/55 [01:19<07:45,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▋        #033[0m| 9/55 [01:29<07:33,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 1.5176221132278442\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▋        #033[0m| 9/55 [01:29<07:33,  9.86s/it]#015Training Epoch0:  16%|#033[34m█▋        #033[0m| 9/55 [01:29<07:33,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▋        #033[0m| 9/55 [01:29<07:33,  9.86s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  16%|#033[34m█▋        #033[0m| 9/55 [01:29<07:35,  9.89s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  16%|#033[34m█▋        #033[0m| 9/55 [01:29<07:34,  9.89s/it]\u001b[0m\n",
      "\u001b[35mstep 8 is completed and loss is 1.5176221132278442\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  16%|#033[34m█▋        #033[0m| 9/55 [01:29<07:35,  9.89s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  16%|#033[34m█▋        #033[0m| 9/55 [01:29<07:34,  9.89s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 1.5607212781906128\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 10/55 [01:39<07:23,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 10/55 [01:39<07:23,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 10/55 [01:39<07:23,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 10/55 [01:39<07:23,  9.86s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 10/55 [01:39<07:24,  9.89s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 10/55 [01:39<07:24,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 10/55 [01:39<07:24,  9.89s/it]\u001b[0m\n",
      "\u001b[35mstep 9 is completed and loss is 1.5607212781906128\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 10/55 [01:39<07:24,  9.89s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 1.4567896127700806\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 11/55 [01:49<07:13,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 11/55 [01:49<07:13,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 11/55 [01:48<07:13,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 11/55 [01:49<07:13,  9.85s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  20%|#033[34m██        #033[0m| 11/55 [01:49<07:15,  9.89s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  20%|#033[34m██        #033[0m| 11/55 [01:49<07:15,  9.89s/it]\u001b[0m\n",
      "\u001b[35mstep 10 is completed and loss is 1.4567896127700806\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  20%|#033[34m██        #033[0m| 11/55 [01:49<07:15,  9.89s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  20%|#033[34m██        #033[0m| 11/55 [01:49<07:15,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 12/55 [01:59<07:03,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 1.3054516315460205\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 12/55 [01:58<07:03,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 12/55 [01:58<07:03,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 12/55 [01:58<07:03,  9.85s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 12/55 [01:59<07:05,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 12/55 [01:59<07:05,  9.88s/it]\u001b[0m\n",
      "\u001b[35mstep 11 is completed and loss is 1.3054516315460205\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 12/55 [01:59<07:05,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 12/55 [01:59<07:04,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▎       #033[0m| 13/55 [02:09<06:58,  9.96s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▎       #033[0m| 13/55 [02:09<06:58,  9.96s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▎       #033[0m| 13/55 [02:09<06:58,  9.96s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 1.595920443534851\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▎       #033[0m| 13/55 [02:09<06:58,  9.96s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  24%|#033[34m██▎       #033[0m| 13/55 [02:09<06:58,  9.96s/it]\u001b[0m\n",
      "\u001b[35mstep 12 is completed and loss is 1.595920443534851\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  24%|#033[34m██▎       #033[0m| 13/55 [02:09<06:58,  9.96s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  24%|#033[34m██▎       #033[0m| 13/55 [02:09<06:58,  9.96s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  24%|#033[34m██▎       #033[0m| 13/55 [02:09<06:58,  9.96s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 14/55 [02:18<06:47,  9.93s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 14/55 [02:19<06:47,  9.93s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 14/55 [02:19<06:47,  9.93s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 1.4640095233917236\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 14/55 [02:19<06:47,  9.93s/it]\u001b[0m\n",
      "\u001b[35mstep 13 is completed and loss is 1.4640095233917236\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 14/55 [02:19<06:47,  9.93s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 14/55 [02:19<06:47,  9.93s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 14/55 [02:19<06:47,  9.93s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 14/55 [02:19<06:47,  9.93s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 15/55 [02:28<06:36,  9.90s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 1.3221749067306519\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 15/55 [02:28<06:36,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 15/55 [02:28<06:36,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 15/55 [02:28<06:36,  9.90s/it]\u001b[0m\n",
      "\u001b[35mstep 14 is completed and loss is 1.3221749067306519\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 15/55 [02:29<06:36,  9.92s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 15/55 [02:29<06:36,  9.92s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 15/55 [02:29<06:36,  9.92s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 15/55 [02:29<06:36,  9.92s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 1.493040919303894\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 16/55 [02:38<06:25,  9.89s/it]#015Training Epoch0:  29%|#033[34m██▉       #033[0m| 16/55 [02:38<06:25,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 16/55 [02:38<06:25,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 16/55 [02:38<06:25,  9.89s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 16/55 [02:39<06:26,  9.91s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 16/55 [02:39<06:26,  9.91s/it]#015Training Epoch0:  29%|#033[34m██▉       #033[0m| 16/55 [02:38<06:26,  9.91s/it]\u001b[0m\n",
      "\u001b[35mstep 15 is completed and loss is 1.493040919303894\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 16/55 [02:39<06:26,  9.91s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 17/55 [02:48<06:15,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 1.4939712285995483\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 17/55 [02:48<06:15,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 17/55 [02:48<06:15,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 17/55 [02:48<06:15,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  31%|#033[34m███       #033[0m| 17/55 [02:48<06:15,  9.89s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  31%|#033[34m███       #033[0m| 17/55 [02:49<06:15,  9.89s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  31%|#033[34m███       #033[0m| 17/55 [02:48<06:15,  9.89s/it]\u001b[0m\n",
      "\u001b[35mstep 16 is completed and loss is 1.4939712285995483\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  31%|#033[34m███       #033[0m| 17/55 [02:49<06:15,  9.89s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 1.2990541458129883\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 18/55 [02:58<06:05,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 18/55 [02:58<06:05,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 18/55 [02:58<06:05,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 18/55 [02:58<06:05,  9.87s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 18/55 [02:58<06:05,  9.88s/it]\u001b[0m\n",
      "\u001b[35mstep 17 is completed and loss is 1.2990541458129883\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 18/55 [02:58<06:05,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 18/55 [02:58<06:05,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 18/55 [02:58<06:05,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 1.5093275308609009\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 19/55 [03:08<05:55,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 19/55 [03:08<05:55,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 19/55 [03:08<05:55,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 19/55 [03:08<05:55,  9.86s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 19/55 [03:08<05:55,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 19/55 [03:08<05:55,  9.88s/it]\u001b[0m\n",
      "\u001b[35mstep 18 is completed and loss is 1.5093275308609009\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 19/55 [03:08<05:55,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 19/55 [03:08<05:55,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 1.4044508934020996\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▋      #033[0m| 20/55 [03:18<05:45,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▋      #033[0m| 20/55 [03:17<05:45,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▋      #033[0m| 20/55 [03:18<05:45,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▋      #033[0m| 20/55 [03:18<05:45,  9.86s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  36%|#033[34m███▋      #033[0m| 20/55 [03:18<05:45,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  36%|#033[34m███▋      #033[0m| 20/55 [03:18<05:45,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  36%|#033[34m███▋      #033[0m| 20/55 [03:18<05:45,  9.88s/it]\u001b[0m\n",
      "\u001b[35mstep 19 is completed and loss is 1.4044508934020996\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  36%|#033[34m███▋      #033[0m| 20/55 [03:18<05:45,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 1.344421148300171\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 21/55 [03:27<05:35,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 21/55 [03:27<05:35,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 21/55 [03:28<05:35,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 21/55 [03:28<05:35,  9.86s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 21/55 [03:28<05:35,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 21/55 [03:28<05:35,  9.88s/it]\u001b[0m\n",
      "\u001b[35mstep 20 is completed and loss is 1.344421148300171\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 21/55 [03:28<05:35,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 21/55 [03:28<05:35,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 22/55 [03:37<05:25,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 1.332436203956604\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 22/55 [03:37<05:25,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 22/55 [03:37<05:25,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 22/55 [03:37<05:25,  9.86s/it]\u001b[0m\n",
      "\u001b[35mstep 21 is completed and loss is 1.332436203956604\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  40%|#033[34m████      #033[0m| 22/55 [03:38<05:26,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  40%|#033[34m████      #033[0m| 22/55 [03:38<05:26,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  40%|#033[34m████      #033[0m| 22/55 [03:38<05:26,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  40%|#033[34m████      #033[0m| 22/55 [03:38<05:26,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 23/55 [03:47<05:15,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 23/55 [03:47<05:15,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 1.0240384340286255\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 23/55 [03:47<05:15,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 23/55 [03:47<05:15,  9.85s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 23/55 [03:48<05:16,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 23/55 [03:48<05:16,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 23/55 [03:48<05:16,  9.88s/it]\u001b[0m\n",
      "\u001b[35mstep 22 is completed and loss is 1.0240384340286255\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 23/55 [03:48<05:16,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▎     #033[0m| 24/55 [03:57<05:05,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 1.2538594007492065\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▎     #033[0m| 24/55 [03:57<05:05,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▎     #033[0m| 24/55 [03:57<05:05,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▎     #033[0m| 24/55 [03:57<05:05,  9.85s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  44%|#033[34m████▎     #033[0m| 24/55 [03:58<05:06,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  44%|#033[34m████▎     #033[0m| 24/55 [03:58<05:06,  9.88s/it]\u001b[0m\n",
      "\u001b[35mstep 23 is completed and loss is 1.2538594007492065\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  44%|#033[34m████▎     #033[0m| 24/55 [03:57<05:06,  9.88s/it]#015Training Epoch0:  44%|#033[34m████▎     #033[0m| 24/55 [03:58<05:06,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 1.292189121246338\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 25/55 [04:07<04:55,  9.85s/it]#015Training Epoch0:  45%|#033[34m████▌     #033[0m| 25/55 [04:07<04:55,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 25/55 [04:07<04:55,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 25/55 [04:07<04:55,  9.85s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 25/55 [04:07<04:56,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 25/55 [04:07<04:56,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 25/55 [04:08<04:56,  9.88s/it]\u001b[0m\n",
      "\u001b[35mstep 24 is completed and loss is 1.292189121246338\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 25/55 [04:08<04:56,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 26/55 [04:17<04:47,  9.93s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 1.1695202589035034\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 26/55 [04:17<04:47,  9.93s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 26/55 [04:17<04:47,  9.93s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 26/55 [04:17<04:47,  9.93s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 26/55 [04:17<04:47,  9.91s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 26/55 [04:17<04:47,  9.91s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 26/55 [04:18<04:47,  9.91s/it]\u001b[0m\n",
      "\u001b[35mstep 25 is completed and loss is 1.1695202589035034\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 26/55 [04:18<04:47,  9.91s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 1.3061431646347046\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  49%|#033[34m████▉     #033[0m| 27/55 [04:27<04:37,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  49%|#033[34m████▉     #033[0m| 27/55 [04:27<04:37,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  49%|#033[34m████▉     #033[0m| 27/55 [04:27<04:37,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  49%|#033[34m████▉     #033[0m| 27/55 [04:27<04:37,  9.90s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  49%|#033[34m████▉     #033[0m| 27/55 [04:27<04:37,  9.91s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  49%|#033[34m████▉     #033[0m| 27/55 [04:27<04:37,  9.91s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  49%|#033[34m████▉     #033[0m| 27/55 [04:27<04:37,  9.91s/it]\u001b[0m\n",
      "\u001b[35mstep 26 is completed and loss is 1.3061431646347046\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  49%|#033[34m████▉     #033[0m| 27/55 [04:27<04:37,  9.91s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  51%|#033[34m█████     #033[0m| 28/55 [04:37<04:26,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  51%|#033[34m█████     #033[0m| 28/55 [04:37<04:26,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 1.6245174407958984\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  51%|#033[34m█████     #033[0m| 28/55 [04:37<04:26,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  51%|#033[34m█████     #033[0m| 28/55 [04:37<04:26,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  51%|#033[34m█████     #033[0m| 28/55 [04:37<04:27,  9.90s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  51%|#033[34m█████     #033[0m| 28/55 [04:37<04:27,  9.90s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  51%|#033[34m█████     #033[0m| 28/55 [04:37<04:27,  9.90s/it]\u001b[0m\n",
      "\u001b[35mstep 27 is completed and loss is 1.6245174407958984\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  51%|#033[34m█████     #033[0m| 28/55 [04:37<04:27,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  53%|#033[34m█████▎    #033[0m| 29/55 [04:46<04:16,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  53%|#033[34m█████▎    #033[0m| 29/55 [04:47<04:16,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 28 is completed and loss is 1.4284430742263794\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  53%|#033[34m█████▎    #033[0m| 29/55 [04:47<04:16,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  53%|#033[34m█████▎    #033[0m| 29/55 [04:47<04:16,  9.87s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  53%|#033[34m█████▎    #033[0m| 29/55 [04:47<04:17,  9.89s/it]#015Training Epoch0:  53%|#033[34m█████▎    #033[0m| 29/55 [04:47<04:17,  9.89s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  53%|#033[34m█████▎    #033[0m| 29/55 [04:47<04:17,  9.89s/it]\u001b[0m\n",
      "\u001b[35mstep 28 is completed and loss is 1.4284430742263794\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  53%|#033[34m█████▎    #033[0m| 29/55 [04:47<04:17,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  55%|#033[34m█████▍    #033[0m| 30/55 [04:56<04:06,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  55%|#033[34m█████▍    #033[0m| 30/55 [04:56<04:06,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  55%|#033[34m█████▍    #033[0m| 30/55 [04:56<04:06,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 29 is completed and loss is 1.3426792621612549\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  55%|#033[34m█████▍    #033[0m| 30/55 [04:56<04:06,  9.87s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  55%|#033[34m█████▍    #033[0m| 30/55 [04:57<04:07,  9.89s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  55%|#033[34m█████▍    #033[0m| 30/55 [04:57<04:07,  9.89s/it]#015Training Epoch0:  55%|#033[34m█████▍    #033[0m| 30/55 [04:57<04:07,  9.89s/it]\u001b[0m\n",
      "\u001b[35mstep 29 is completed and loss is 1.3426792621612549\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  55%|#033[34m█████▍    #033[0m| 30/55 [04:57<04:07,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  56%|#033[34m█████▋    #033[0m| 31/55 [05:06<03:56,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 30 is completed and loss is 1.4486159086227417\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  56%|#033[34m█████▋    #033[0m| 31/55 [05:06<03:56,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  56%|#033[34m█████▋    #033[0m| 31/55 [05:06<03:56,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  56%|#033[34m█████▋    #033[0m| 31/55 [05:06<03:56,  9.86s/it]\u001b[0m\n",
      "\u001b[35mstep 30 is completed and loss is 1.4486159086227417\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  56%|#033[34m█████▋    #033[0m| 31/55 [05:07<03:57,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  56%|#033[34m█████▋    #033[0m| 31/55 [05:07<03:57,  9.88s/it]#015Training Epoch0:  56%|#033[34m█████▋    #033[0m| 31/55 [05:07<03:57,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  56%|#033[34m█████▋    #033[0m| 31/55 [05:07<03:57,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  58%|#033[34m█████▊    #033[0m| 32/55 [05:16<03:46,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 31 is completed and loss is 1.152133822441101\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  58%|#033[34m█████▊    #033[0m| 32/55 [05:16<03:46,  9.86s/it]#015Training Epoch0:  58%|#033[34m█████▊    #033[0m| 32/55 [05:16<03:46,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  58%|#033[34m█████▊    #033[0m| 32/55 [05:16<03:46,  9.86s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  58%|#033[34m█████▊    #033[0m| 32/55 [05:17<03:47,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  58%|#033[34m█████▊    #033[0m| 32/55 [05:17<03:47,  9.88s/it]\u001b[0m\n",
      "\u001b[35mstep 31 is completed and loss is 1.152133822441101\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  58%|#033[34m█████▊    #033[0m| 32/55 [05:17<03:47,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  58%|#033[34m█████▊    #033[0m| 32/55 [05:17<03:47,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 32 is completed and loss is 1.165588617324829\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  60%|#033[34m██████    #033[0m| 33/55 [05:26<03:36,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  60%|#033[34m██████    #033[0m| 33/55 [05:26<03:36,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  60%|#033[34m██████    #033[0m| 33/55 [05:26<03:36,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  60%|#033[34m██████    #033[0m| 33/55 [05:26<03:36,  9.86s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  60%|#033[34m██████    #033[0m| 33/55 [05:27<03:37,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  60%|#033[34m██████    #033[0m| 33/55 [05:27<03:37,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  60%|#033[34m██████    #033[0m| 33/55 [05:26<03:37,  9.88s/it]\u001b[0m\n",
      "\u001b[35mstep 32 is completed and loss is 1.165588617324829\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  60%|#033[34m██████    #033[0m| 33/55 [05:27<03:37,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  62%|#033[34m██████▏   #033[0m| 34/55 [05:36<03:26,  9.85s/it]#015Training Epoch0:  62%|#033[34m██████▏   #033[0m| 34/55 [05:36<03:26,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 33 is completed and loss is 1.1385705471038818\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  62%|#033[34m██████▏   #033[0m| 34/55 [05:36<03:26,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  62%|#033[34m██████▏   #033[0m| 34/55 [05:36<03:26,  9.85s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  62%|#033[34m██████▏   #033[0m| 34/55 [05:37<03:27,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  62%|#033[34m██████▏   #033[0m| 34/55 [05:36<03:27,  9.88s/it]\u001b[0m\n",
      "\u001b[35mstep 33 is completed and loss is 1.1385705471038818\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  62%|#033[34m██████▏   #033[0m| 34/55 [05:37<03:27,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  62%|#033[34m██████▏   #033[0m| 34/55 [05:36<03:27,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 34 is completed and loss is 1.2839083671569824\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  64%|#033[34m██████▎   #033[0m| 35/55 [05:46<03:17,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  64%|#033[34m██████▎   #033[0m| 35/55 [05:46<03:17,  9.85s/it]#015Training Epoch0:  64%|#033[34m██████▎   #033[0m| 35/55 [05:46<03:17,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  64%|#033[34m██████▎   #033[0m| 35/55 [05:45<03:17,  9.85s/it]\u001b[0m\n",
      "\u001b[35mstep 34 is completed and loss is 1.2839083671569824\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  64%|#033[34m██████▎   #033[0m| 35/55 [05:46<03:17,  9.88s/it]#015Training Epoch0:  64%|#033[34m██████▎   #033[0m| 35/55 [05:46<03:17,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  64%|#033[34m██████▎   #033[0m| 35/55 [05:46<03:17,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  64%|#033[34m██████▎   #033[0m| 35/55 [05:46<03:17,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  65%|#033[34m██████▌   #033[0m| 36/55 [05:55<03:07,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 35 is completed and loss is 1.243728756904602\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  65%|#033[34m██████▌   #033[0m| 36/55 [05:55<03:07,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  65%|#033[34m██████▌   #033[0m| 36/55 [05:55<03:07,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  65%|#033[34m██████▌   #033[0m| 36/55 [05:56<03:07,  9.85s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  65%|#033[34m██████▌   #033[0m| 36/55 [05:56<03:07,  9.88s/it]\u001b[0m\n",
      "\u001b[35mstep 35 is completed and loss is 1.243728756904602\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  65%|#033[34m██████▌   #033[0m| 36/55 [05:56<03:07,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  65%|#033[34m██████▌   #033[0m| 36/55 [05:56<03:07,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  65%|#033[34m██████▌   #033[0m| 36/55 [05:56<03:07,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 37/55 [06:05<02:57,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 37/55 [06:05<02:57,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 37/55 [06:05<02:57,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 36 is completed and loss is 1.253936767578125\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 37/55 [06:05<02:57,  9.86s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 37/55 [06:06<02:57,  9.87s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 37/55 [06:06<02:57,  9.87s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 37/55 [06:06<02:57,  9.87s/it]\u001b[0m\n",
      "\u001b[35mstep 36 is completed and loss is 1.253936767578125\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 37/55 [06:06<02:57,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 37 is completed and loss is 1.1983599662780762\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  69%|#033[34m██████▉   #033[0m| 38/55 [06:15<02:47,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  69%|#033[34m██████▉   #033[0m| 38/55 [06:15<02:47,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  69%|#033[34m██████▉   #033[0m| 38/55 [06:15<02:47,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  69%|#033[34m██████▉   #033[0m| 38/55 [06:15<02:47,  9.85s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  69%|#033[34m██████▉   #033[0m| 38/55 [06:16<02:48,  9.94s/it]\u001b[0m\n",
      "\u001b[35mstep 37 is completed and loss is 1.1983599662780762\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  69%|#033[34m██████▉   #033[0m| 38/55 [06:16<02:48,  9.94s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  69%|#033[34m██████▉   #033[0m| 38/55 [06:16<02:48,  9.94s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  69%|#033[34m██████▉   #033[0m| 38/55 [06:16<02:48,  9.94s/it]\u001b[0m\n",
      "\u001b[34mstep 38 is completed and loss is 1.2923616170883179\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████   #033[0m| 39/55 [06:25<02:38,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████   #033[0m| 39/55 [06:25<02:38,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████   #033[0m| 39/55 [06:25<02:38,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████   #033[0m| 39/55 [06:25<02:38,  9.90s/it]\u001b[0m\n",
      "\u001b[35mstep 38 is completed and loss is 1.2923616170883179\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  71%|#033[34m███████   #033[0m| 39/55 [06:26<02:39,  9.96s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  71%|#033[34m███████   #033[0m| 39/55 [06:26<02:39,  9.96s/it]#015Training Epoch0:  71%|#033[34m███████   #033[0m| 39/55 [06:26<02:39,  9.96s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  71%|#033[34m███████   #033[0m| 39/55 [06:26<02:39,  9.96s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 40/55 [06:35<02:28,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 40/55 [06:35<02:28,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 39 is completed and loss is 1.1815190315246582\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 40/55 [06:35<02:28,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 40/55 [06:35<02:28,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 40/55 [06:36<02:29,  9.97s/it]\u001b[0m\n",
      "\u001b[35mstep 39 is completed and loss is 1.1815190315246582\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 40/55 [06:36<02:29,  9.97s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 40/55 [06:36<02:29,  9.97s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 40/55 [06:36<02:29,  9.97s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▍  #033[0m| 41/55 [06:45<02:18,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 40 is completed and loss is 1.2803232669830322\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▍  #033[0m| 41/55 [06:45<02:18,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▍  #033[0m| 41/55 [06:45<02:18,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▍  #033[0m| 41/55 [06:45<02:18,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  75%|#033[34m███████▍  #033[0m| 41/55 [06:46<02:19,  9.94s/it]#015Training Epoch0:  75%|#033[34m███████▍  #033[0m| 41/55 [06:46<02:19,  9.94s/it]\u001b[0m\n",
      "\u001b[35mstep 40 is completed and loss is 1.2803232669830322\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  75%|#033[34m███████▍  #033[0m| 41/55 [06:46<02:19,  9.94s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  75%|#033[34m███████▍  #033[0m| 41/55 [06:46<02:19,  9.94s/it]\u001b[0m\n",
      "\u001b[34mstep 41 is completed and loss is 1.4710755348205566\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  76%|#033[34m███████▋  #033[0m| 42/55 [06:55<02:08,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  76%|#033[34m███████▋  #033[0m| 42/55 [06:55<02:08,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  76%|#033[34m███████▋  #033[0m| 42/55 [06:55<02:08,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  76%|#033[34m███████▋  #033[0m| 42/55 [06:55<02:08,  9.87s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  76%|#033[34m███████▋  #033[0m| 42/55 [06:56<02:08,  9.92s/it]\u001b[0m\n",
      "\u001b[35mstep 41 is completed and loss is 1.4710755348205566\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  76%|#033[34m███████▋  #033[0m| 42/55 [06:56<02:08,  9.92s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  76%|#033[34m███████▋  #033[0m| 42/55 [06:56<02:08,  9.92s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  76%|#033[34m███████▋  #033[0m| 42/55 [06:56<02:08,  9.92s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 43/55 [07:04<01:58,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 43/55 [07:05<01:58,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 42 is completed and loss is 1.1654587984085083\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 43/55 [07:05<01:58,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 43/55 [07:05<01:58,  9.86s/it]\u001b[0m\n",
      "\u001b[35mstep 42 is completed and loss is 1.1654587984085083\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 43/55 [07:06<01:58,  9.91s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 43/55 [07:06<01:58,  9.91s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 43/55 [07:06<01:58,  9.91s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 43/55 [07:06<01:58,  9.91s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  80%|#033[34m████████  #033[0m| 44/55 [07:14<01:48,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  80%|#033[34m████████  #033[0m| 44/55 [07:15<01:48,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  80%|#033[34m████████  #033[0m| 44/55 [07:14<01:48,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 43 is completed and loss is 1.1053638458251953\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  80%|#033[34m████████  #033[0m| 44/55 [07:14<01:48,  9.86s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  80%|#033[34m████████  #033[0m| 44/55 [07:16<01:48,  9.90s/it]#015Training Epoch0:  80%|#033[34m████████  #033[0m| 44/55 [07:16<01:48,  9.90s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  80%|#033[34m████████  #033[0m| 44/55 [07:16<01:48,  9.90s/it]\u001b[0m\n",
      "\u001b[35mstep 43 is completed and loss is 1.1053638458251953\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  80%|#033[34m████████  #033[0m| 44/55 [07:16<01:48,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  82%|#033[34m████████▏ #033[0m| 45/55 [07:24<01:38,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 44 is completed and loss is 1.2170343399047852\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  82%|#033[34m████████▏ #033[0m| 45/55 [07:24<01:38,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  82%|#033[34m████████▏ #033[0m| 45/55 [07:24<01:38,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  82%|#033[34m████████▏ #033[0m| 45/55 [07:24<01:38,  9.86s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  82%|#033[34m████████▏ #033[0m| 45/55 [07:25<01:38,  9.89s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  82%|#033[34m████████▏ #033[0m| 45/55 [07:26<01:38,  9.90s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  82%|#033[34m████████▏ #033[0m| 45/55 [07:26<01:38,  9.89s/it]\u001b[0m\n",
      "\u001b[35mstep 44 is completed and loss is 1.2170343399047852\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  82%|#033[34m████████▏ #033[0m| 45/55 [07:26<01:38,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  84%|#033[34m████████▎ #033[0m| 46/55 [07:34<01:28,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 45 is completed and loss is 1.2920353412628174\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  84%|#033[34m████████▎ #033[0m| 46/55 [07:34<01:28,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  84%|#033[34m████████▎ #033[0m| 46/55 [07:34<01:28,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  84%|#033[34m████████▎ #033[0m| 46/55 [07:34<01:28,  9.86s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  84%|#033[34m████████▎ #033[0m| 46/55 [07:35<01:29,  9.89s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  84%|#033[34m████████▎ #033[0m| 46/55 [07:36<01:29,  9.89s/it]\u001b[0m\n",
      "\u001b[35mstep 45 is completed and loss is 1.2920353412628174\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  84%|#033[34m████████▎ #033[0m| 46/55 [07:36<01:29,  9.89s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  84%|#033[34m████████▎ #033[0m| 46/55 [07:35<01:29,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  85%|#033[34m████████▌ #033[0m| 47/55 [07:44<01:18,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  85%|#033[34m████████▌ #033[0m| 47/55 [07:44<01:18,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  85%|#033[34m████████▌ #033[0m| 47/55 [07:44<01:18,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 46 is completed and loss is 1.4523733854293823\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  85%|#033[34m████████▌ #033[0m| 47/55 [07:44<01:18,  9.86s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  85%|#033[34m████████▌ #033[0m| 47/55 [07:45<01:19,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  85%|#033[34m████████▌ #033[0m| 47/55 [07:45<01:19,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  85%|#033[34m████████▌ #033[0m| 47/55 [07:45<01:19,  9.88s/it]\u001b[0m\n",
      "\u001b[35mstep 46 is completed and loss is 1.4523733854293823\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  85%|#033[34m████████▌ #033[0m| 47/55 [07:45<01:19,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 47 is completed and loss is 1.4977163076400757\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  87%|#033[34m████████▋ #033[0m| 48/55 [07:54<01:08,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  87%|#033[34m████████▋ #033[0m| 48/55 [07:54<01:08,  9.85s/it]#015Training Epoch0:  87%|#033[34m████████▋ #033[0m| 48/55 [07:54<01:08,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  87%|#033[34m████████▋ #033[0m| 48/55 [07:54<01:08,  9.86s/it]\u001b[0m\n",
      "\u001b[35mstep 47 is completed and loss is 1.4977163076400757\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  87%|#033[34m████████▋ #033[0m| 48/55 [07:55<01:09,  9.89s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  87%|#033[34m████████▋ #033[0m| 48/55 [07:55<01:09,  9.89s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  87%|#033[34m████████▋ #033[0m| 48/55 [07:55<01:09,  9.89s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  87%|#033[34m████████▋ #033[0m| 48/55 [07:55<01:09,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 49/55 [08:04<00:59,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 49/55 [08:04<00:59,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 48 is completed and loss is 1.4309356212615967\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 49/55 [08:04<00:59,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 49/55 [08:04<00:59,  9.85s/it]\u001b[0m\n",
      "\u001b[35mstep 48 is completed and loss is 1.4309356212615967\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 49/55 [08:05<00:59,  9.89s/it]#015Training Epoch0:  89%|#033[34m████████▉ #033[0m| 49/55 [08:05<00:59,  9.89s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 49/55 [08:05<00:59,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 49/55 [08:05<00:59,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  91%|#033[34m█████████ #033[0m| 50/55 [08:13<00:49,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 49 is completed and loss is 1.2178034782409668\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  91%|#033[34m█████████ #033[0m| 50/55 [08:14<00:49,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  91%|#033[34m█████████ #033[0m| 50/55 [08:14<00:49,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  91%|#033[34m█████████ #033[0m| 50/55 [08:14<00:49,  9.85s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  91%|#033[34m█████████ #033[0m| 50/55 [08:15<00:49,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  91%|#033[34m█████████ #033[0m| 50/55 [08:15<00:49,  9.88s/it]\u001b[0m\n",
      "\u001b[35mstep 49 is completed and loss is 1.2178034782409668\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  91%|#033[34m█████████ #033[0m| 50/55 [08:15<00:49,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  91%|#033[34m█████████ #033[0m| 50/55 [08:15<00:49,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 50 is completed and loss is 1.433895468711853\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 51/55 [08:23<00:39,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 51/55 [08:23<00:39,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 51/55 [08:24<00:39,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 51/55 [08:23<00:39,  9.85s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 51/55 [08:25<00:39,  9.88s/it]\u001b[0m\n",
      "\u001b[35mstep 50 is completed and loss is 1.433895468711853\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 51/55 [08:25<00:39,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 51/55 [08:25<00:39,  9.88s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 51/55 [08:25<00:39,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 51 is completed and loss is 1.3791404962539673\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  95%|#033[34m█████████▍#033[0m| 52/55 [08:33<00:29,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  95%|#033[34m█████████▍#033[0m| 52/55 [08:33<00:29,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  95%|#033[34m█████████▍#033[0m| 52/55 [08:33<00:29,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  95%|#033[34m█████████▍#033[0m| 52/55 [08:33<00:29,  9.86s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  95%|#033[34m█████████▍#033[0m| 52/55 [08:35<00:29,  9.93s/it]#015Training Epoch0:  95%|#033[34m█████████▍#033[0m| 52/55 [08:35<00:29,  9.93s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  95%|#033[34m█████████▍#033[0m| 52/55 [08:35<00:29,  9.93s/it]\u001b[0m\n",
      "\u001b[35mstep 51 is completed and loss is 1.3791404962539673\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  95%|#033[34m█████████▍#033[0m| 52/55 [08:35<00:29,  9.93s/it]\u001b[0m\n",
      "\u001b[34mstep 52 is completed and loss is 1.0078175067901611\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 53/55 [08:43<00:19,  9.93s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 53/55 [08:43<00:19,  9.93s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 53/55 [08:43<00:19,  9.93s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 53/55 [08:43<00:19,  9.93s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 53/55 [08:45<00:19,  9.92s/it]#015Training Epoch0:  96%|#033[34m█████████▋#033[0m| 53/55 [08:45<00:19,  9.92s/it]\u001b[0m\n",
      "\u001b[35mstep 52 is completed and loss is 1.0078175067901611\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 53/55 [08:45<00:19,  9.92s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 53/55 [08:45<00:19,  9.92s/it]\u001b[0m\n",
      "\u001b[34mstep 53 is completed and loss is 1.4158527851104736\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  98%|#033[34m█████████▊#033[0m| 54/55 [08:53<00:09,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  98%|#033[34m█████████▊#033[0m| 54/55 [08:53<00:09,  9.90s/it]#015Training Epoch0:  98%|#033[34m█████████▊#033[0m| 54/55 [08:53<00:09,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  98%|#033[34m█████████▊#033[0m| 54/55 [08:53<00:09,  9.90s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  98%|#033[34m█████████▊#033[0m| 54/55 [08:55<00:09,  9.91s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  98%|#033[34m█████████▊#033[0m| 54/55 [08:55<00:09,  9.91s/it]\u001b[0m\n",
      "\u001b[35mstep 53 is completed and loss is 1.4158527851104736\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  98%|#033[34m█████████▊#033[0m| 54/55 [08:55<00:09,  9.91s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0:  98%|#033[34m█████████▊#033[0m| 54/55 [08:55<00:09,  9.91s/it]\u001b[0m\n",
      "\u001b[34mstep 54 is completed and loss is 1.2874191999435425\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 55/55 [09:03<00:00,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 55/55 [09:03<00:00,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 55/55 [09:03<00:00,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 55/55 [09:03<00:00,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 55/55 [09:03<00:00,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 55/55 [09:03<00:00,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 55/55 [09:03<00:00,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 55/55 [09:03<00:00,  9.88s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 1 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/56 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/56 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/56 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/56 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   2%|#033[32m▏         #033[0m| 1/56 [00:03<03:18,  3.62s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   2%|#033[32m▏         #033[0m| 1/56 [00:03<03:18,  3.62s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   2%|#033[32m▏         #033[0m| 1/56 [00:03<03:19,  3.62s/it]#015evaluating Epoch:   2%|#033[32m▏         #033[0m| 1/56 [00:03<03:19,  3.62s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0: 100%|#033[34m██████████#033[0m| 55/55 [09:04<00:00,  9.90s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0: 100%|#033[34m██████████#033[0m| 55/55 [09:05<00:00,  9.90s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0: 100%|#033[34m██████████#033[0m| 55/55 [09:04<00:00,  9.91s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0: 100%|#033[34m██████████#033[0m| 55/55 [09:05<00:00,  9.91s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0: 100%|#033[34m██████████#033[0m| 55/55 [09:05<00:00,  9.90s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0: 100%|#033[34m██████████#033[0m| 55/55 [09:05<00:00,  9.91s/it]\u001b[0m\n",
      "\u001b[35mstep 54 is completed and loss is 1.2874191999435425\u001b[0m\n",
      "\u001b[35mTraining Epoch0: 100%|#033[34m██████████#033[0m| 55/55 [09:05<00:00,  9.90s/it]\u001b[0m\n",
      "\u001b[35mTraining Epoch0: 100%|#033[34m██████████#033[0m| 55/55 [09:05<00:00,  9.91s/it]\u001b[0m\n",
      "\u001b[35mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[35mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[35mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[35mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[35mCPU Total Peak Memory consumed during the train (max): 1 GB\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/56 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/56 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/56 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/56 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 2/56 [00:07<03:11,  3.54s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 2/56 [00:07<03:11,  3.54s/it]#015evaluating Epoch:   4%|#033[32m▎         #033[0m| 2/56 [00:07<03:11,  3.55s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 2/56 [00:07<03:11,  3.55s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   2%|#033[32m▏         #033[0m| 1/56 [00:03<03:19,  3.63s/it]#015evaluating Epoch:   2%|#033[32m▏         #033[0m| 1/56 [00:03<03:19,  3.63s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   2%|#033[32m▏         #033[0m| 1/56 [00:03<03:19,  3.63s/it]#015evaluating Epoch:   2%|#033[32m▏         #033[0m| 1/56 [00:03<03:19,  3.63s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   5%|#033[32m▌         #033[0m| 3/56 [00:10<03:06,  3.52s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   5%|#033[32m▌         #033[0m| 3/56 [00:10<03:06,  3.52s/it]#015evaluating Epoch:   5%|#033[32m▌         #033[0m| 3/56 [00:10<03:06,  3.52s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   5%|#033[32m▌         #033[0m| 3/56 [00:10<03:06,  3.52s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 2/56 [00:07<03:12,  3.56s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 2/56 [00:07<03:11,  3.55s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 2/56 [00:07<03:12,  3.56s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 2/56 [00:07<03:12,  3.56s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 4/56 [00:14<03:02,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 4/56 [00:14<03:02,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 4/56 [00:14<03:02,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 4/56 [00:14<03:02,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   5%|#033[32m▌         #033[0m| 3/56 [00:10<03:07,  3.53s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   5%|#033[32m▌         #033[0m| 3/56 [00:10<03:07,  3.53s/it]#015evaluating Epoch:   5%|#033[32m▌         #033[0m| 3/56 [00:10<03:07,  3.53s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   5%|#033[32m▌         #033[0m| 3/56 [00:10<03:07,  3.53s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▉         #033[0m| 5/56 [00:17<02:58,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▉         #033[0m| 5/56 [00:17<02:58,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▉         #033[0m| 5/56 [00:17<02:58,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▉         #033[0m| 5/56 [00:17<02:58,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 4/56 [00:14<03:03,  3.52s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 4/56 [00:14<03:03,  3.52s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 4/56 [00:14<03:03,  3.52s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 4/56 [00:14<03:03,  3.52s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 6/56 [00:21<02:54,  3.50s/it]#015evaluating Epoch:  11%|#033[32m█         #033[0m| 6/56 [00:21<02:54,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 6/56 [00:21<02:54,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 6/56 [00:21<02:54,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   9%|#033[32m▉         #033[0m| 5/56 [00:17<02:59,  3.52s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   9%|#033[32m▉         #033[0m| 5/56 [00:17<02:59,  3.52s/it]#015evaluating Epoch:   9%|#033[32m▉         #033[0m| 5/56 [00:17<02:59,  3.52s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:   9%|#033[32m▉         #033[0m| 5/56 [00:17<02:59,  3.52s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  12%|#033[32m█▎        #033[0m| 7/56 [00:24<02:51,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  12%|#033[32m█▎        #033[0m| 7/56 [00:24<02:51,  3.50s/it]#015evaluating Epoch:  12%|#033[32m█▎        #033[0m| 7/56 [00:24<02:51,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  12%|#033[32m█▎        #033[0m| 7/56 [00:24<02:51,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  11%|#033[32m█         #033[0m| 6/56 [00:21<02:55,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  11%|#033[32m█         #033[0m| 6/56 [00:21<02:55,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  11%|#033[32m█         #033[0m| 6/56 [00:21<02:55,  3.51s/it]#015evaluating Epoch:  11%|#033[32m█         #033[0m| 6/56 [00:21<02:55,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 8/56 [00:28<02:47,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 8/56 [00:28<02:47,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 8/56 [00:28<02:47,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 8/56 [00:28<02:47,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  12%|#033[32m█▎        #033[0m| 7/56 [00:24<02:52,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  12%|#033[32m█▎        #033[0m| 7/56 [00:24<02:51,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  12%|#033[32m█▎        #033[0m| 7/56 [00:24<02:51,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  12%|#033[32m█▎        #033[0m| 7/56 [00:24<02:51,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  16%|#033[32m█▌        #033[0m| 9/56 [00:31<02:44,  3.49s/it]#015evaluating Epoch:  16%|#033[32m█▌        #033[0m| 9/56 [00:31<02:44,  3.49s/it]#015evaluating Epoch:  16%|#033[32m█▌        #033[0m| 9/56 [00:31<02:44,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  16%|#033[32m█▌        #033[0m| 9/56 [00:31<02:44,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 8/56 [00:28<02:48,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 8/56 [00:28<02:48,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 8/56 [00:28<02:48,  3.51s/it]#015evaluating Epoch:  14%|#033[32m█▍        #033[0m| 8/56 [00:28<02:48,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  16%|#033[32m█▌        #033[0m| 9/56 [00:31<02:44,  3.51s/it]#015evaluating Epoch:  16%|#033[32m█▌        #033[0m| 9/56 [00:31<02:44,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  16%|#033[32m█▌        #033[0m| 9/56 [00:31<02:44,  3.51s/it]#015evaluating Epoch:  16%|#033[32m█▌        #033[0m| 9/56 [00:31<02:44,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 10/56 [00:35<02:40,  3.49s/it]#015evaluating Epoch:  18%|#033[32m█▊        #033[0m| 10/56 [00:35<02:40,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 10/56 [00:35<02:40,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 10/56 [00:35<02:40,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  20%|#033[32m█▉        #033[0m| 11/56 [00:38<02:37,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  20%|#033[32m█▉        #033[0m| 11/56 [00:38<02:37,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  20%|#033[32m█▉        #033[0m| 11/56 [00:38<02:37,  3.49s/it]#015evaluating Epoch:  20%|#033[32m█▉        #033[0m| 11/56 [00:38<02:37,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 10/56 [00:35<02:41,  3.51s/it]#015evaluating Epoch:  18%|#033[32m█▊        #033[0m| 10/56 [00:35<02:41,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 10/56 [00:35<02:41,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 10/56 [00:35<02:41,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 12/56 [00:42<02:33,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 12/56 [00:42<02:33,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 12/56 [00:42<02:33,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 12/56 [00:42<02:33,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  20%|#033[32m█▉        #033[0m| 11/56 [00:38<02:37,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  20%|#033[32m█▉        #033[0m| 11/56 [00:38<02:37,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  20%|#033[32m█▉        #033[0m| 11/56 [00:38<02:37,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  20%|#033[32m█▉        #033[0m| 11/56 [00:38<02:37,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  23%|#033[32m██▎       #033[0m| 13/56 [00:45<02:30,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  23%|#033[32m██▎       #033[0m| 13/56 [00:45<02:30,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  23%|#033[32m██▎       #033[0m| 13/56 [00:45<02:30,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  23%|#033[32m██▎       #033[0m| 13/56 [00:45<02:30,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 12/56 [00:42<02:34,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 12/56 [00:42<02:34,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 12/56 [00:42<02:34,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 12/56 [00:42<02:34,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 14/56 [00:48<02:26,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 14/56 [00:48<02:26,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 14/56 [00:48<02:26,  3.49s/it]#015evaluating Epoch:  25%|#033[32m██▌       #033[0m| 14/56 [00:48<02:26,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  23%|#033[32m██▎       #033[0m| 13/56 [00:45<02:30,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  23%|#033[32m██▎       #033[0m| 13/56 [00:45<02:30,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  23%|#033[32m██▎       #033[0m| 13/56 [00:45<02:30,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  23%|#033[32m██▎       #033[0m| 13/56 [00:45<02:30,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  27%|#033[32m██▋       #033[0m| 15/56 [00:52<02:23,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  27%|#033[32m██▋       #033[0m| 15/56 [00:52<02:23,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  27%|#033[32m██▋       #033[0m| 15/56 [00:52<02:23,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  27%|#033[32m██▋       #033[0m| 15/56 [00:52<02:23,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 14/56 [00:49<02:27,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 14/56 [00:49<02:27,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 14/56 [00:49<02:27,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 14/56 [00:49<02:27,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 16/56 [00:55<02:19,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 16/56 [00:55<02:19,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 16/56 [00:55<02:19,  3.49s/it]#015evaluating Epoch:  29%|#033[32m██▊       #033[0m| 16/56 [00:55<02:19,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  27%|#033[32m██▋       #033[0m| 15/56 [00:52<02:23,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  27%|#033[32m██▋       #033[0m| 15/56 [00:52<02:23,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  27%|#033[32m██▋       #033[0m| 15/56 [00:52<02:23,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  27%|#033[32m██▋       #033[0m| 15/56 [00:52<02:23,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m███       #033[0m| 17/56 [00:59<02:16,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m███       #033[0m| 17/56 [00:59<02:16,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m███       #033[0m| 17/56 [00:59<02:16,  3.49s/it]#015evaluating Epoch:  30%|#033[32m███       #033[0m| 17/56 [00:59<02:16,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 16/56 [00:56<02:20,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 16/56 [00:56<02:20,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 16/56 [00:56<02:20,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 16/56 [00:56<02:20,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 18/56 [01:02<02:12,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 18/56 [01:02<02:12,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 18/56 [01:02<02:12,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 18/56 [01:02<02:12,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  30%|#033[32m███       #033[0m| 17/56 [00:59<02:16,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  30%|#033[32m███       #033[0m| 17/56 [00:59<02:16,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  30%|#033[32m███       #033[0m| 17/56 [00:59<02:16,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  30%|#033[32m███       #033[0m| 17/56 [00:59<02:16,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 19/56 [01:06<02:09,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 19/56 [01:06<02:09,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 19/56 [01:06<02:09,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 19/56 [01:06<02:09,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 18/56 [01:03<02:13,  3.50s/it]#015evaluating Epoch:  32%|#033[32m███▏      #033[0m| 18/56 [01:03<02:13,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 18/56 [01:03<02:13,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 18/56 [01:03<02:13,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 20/56 [01:09<02:05,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 20/56 [01:09<02:05,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 20/56 [01:09<02:05,  3.49s/it]#015evaluating Epoch:  36%|#033[32m███▌      #033[0m| 20/56 [01:09<02:05,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 19/56 [01:06<02:09,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 19/56 [01:06<02:09,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 19/56 [01:06<02:09,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  34%|#033[32m███▍      #033[0m| 19/56 [01:06<02:09,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 21/56 [01:13<02:01,  3.48s/it]#015evaluating Epoch:  38%|#033[32m███▊      #033[0m| 21/56 [01:13<02:01,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 21/56 [01:13<02:01,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 21/56 [01:13<02:01,  3.48s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 20/56 [01:10<02:06,  3.50s/it]#015evaluating Epoch:  36%|#033[32m███▌      #033[0m| 20/56 [01:10<02:06,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 20/56 [01:10<02:06,  3.50s/it]#015evaluating Epoch:  36%|#033[32m███▌      #033[0m| 20/56 [01:10<02:06,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 21/56 [01:13<02:02,  3.51s/it]#015evaluating Epoch:  38%|#033[32m███▊      #033[0m| 21/56 [01:13<02:02,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 21/56 [01:13<02:02,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 21/56 [01:13<02:02,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 22/56 [01:16<01:58,  3.49s/it]#015evaluating Epoch:  39%|#033[32m███▉      #033[0m| 22/56 [01:16<01:58,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 22/56 [01:16<01:58,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 22/56 [01:16<01:58,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████      #033[0m| 23/56 [01:20<01:55,  3.49s/it]#015evaluating Epoch:  41%|#033[32m████      #033[0m| 23/56 [01:20<01:55,  3.49s/it]#015evaluating Epoch:  41%|#033[32m████      #033[0m| 23/56 [01:20<01:55,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████      #033[0m| 23/56 [01:20<01:55,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 22/56 [01:17<01:59,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 22/56 [01:17<01:59,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 22/56 [01:17<01:59,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 22/56 [01:17<01:59,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 24/56 [01:23<01:51,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 24/56 [01:23<01:51,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 24/56 [01:23<01:51,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 24/56 [01:23<01:51,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  41%|#033[32m████      #033[0m| 23/56 [01:20<01:55,  3.50s/it]#015evaluating Epoch:  41%|#033[32m████      #033[0m| 23/56 [01:20<01:55,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  41%|#033[32m████      #033[0m| 23/56 [01:20<01:55,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  41%|#033[32m████      #033[0m| 23/56 [01:20<01:55,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 25/56 [01:27<01:48,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 25/56 [01:27<01:48,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 25/56 [01:27<01:48,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 25/56 [01:27<01:48,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 24/56 [01:24<01:52,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 24/56 [01:24<01:52,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 24/56 [01:24<01:52,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 24/56 [01:24<01:52,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 26/56 [01:30<01:44,  3.49s/it]#015evaluating Epoch:  46%|#033[32m████▋     #033[0m| 26/56 [01:30<01:44,  3.49s/it]#015evaluating Epoch:  46%|#033[32m████▋     #033[0m| 26/56 [01:30<01:44,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 26/56 [01:30<01:44,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 25/56 [01:27<01:48,  3.50s/it]#015evaluating Epoch:  45%|#033[32m████▍     #033[0m| 25/56 [01:27<01:48,  3.50s/it]#015evaluating Epoch:  45%|#033[32m████▍     #033[0m| 25/56 [01:27<01:48,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 25/56 [01:27<01:48,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 27/56 [01:34<01:41,  3.49s/it]#015evaluating Epoch:  48%|#033[32m████▊     #033[0m| 27/56 [01:34<01:41,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 27/56 [01:34<01:41,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 27/56 [01:34<01:41,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 26/56 [01:31<01:44,  3.50s/it]#015evaluating Epoch:  46%|#033[32m████▋     #033[0m| 26/56 [01:31<01:44,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 26/56 [01:31<01:44,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 26/56 [01:31<01:44,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 27/56 [01:34<01:41,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 27/56 [01:34<01:41,  3.50s/it]#015evaluating Epoch:  48%|#033[32m████▊     #033[0m| 27/56 [01:34<01:41,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 27/56 [01:34<01:41,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 28/56 [01:37<01:37,  3.49s/it]#015evaluating Epoch:  50%|#033[32m█████     #033[0m| 28/56 [01:37<01:37,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 28/56 [01:37<01:37,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 28/56 [01:37<01:37,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 29/56 [01:41<01:34,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 29/56 [01:41<01:34,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 29/56 [01:41<01:34,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 29/56 [01:41<01:34,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  50%|#033[32m█████     #033[0m| 28/56 [01:38<01:38,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  50%|#033[32m█████     #033[0m| 28/56 [01:38<01:38,  3.50s/it]#015evaluating Epoch:  50%|#033[32m█████     #033[0m| 28/56 [01:38<01:38,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  50%|#033[32m█████     #033[0m| 28/56 [01:38<01:38,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 30/56 [01:44<01:30,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 30/56 [01:44<01:30,  3.49s/it]#015evaluating Epoch:  54%|#033[32m█████▎    #033[0m| 30/56 [01:44<01:30,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 30/56 [01:44<01:30,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 29/56 [01:41<01:34,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 29/56 [01:41<01:34,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 29/56 [01:41<01:34,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 29/56 [01:41<01:34,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 31/56 [01:48<01:27,  3.49s/it]#015evaluating Epoch:  55%|#033[32m█████▌    #033[0m| 31/56 [01:48<01:27,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 31/56 [01:48<01:27,  3.49s/it]#015evaluating Epoch:  55%|#033[32m█████▌    #033[0m| 31/56 [01:48<01:27,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 30/56 [01:45<01:31,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 30/56 [01:45<01:31,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 30/56 [01:45<01:31,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 30/56 [01:45<01:31,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 32/56 [01:51<01:23,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 32/56 [01:51<01:23,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 32/56 [01:51<01:23,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 32/56 [01:51<01:23,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 31/56 [01:48<01:27,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 31/56 [01:48<01:27,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 31/56 [01:48<01:27,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 31/56 [01:48<01:27,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▉    #033[0m| 33/56 [01:55<01:20,  3.49s/it]#015evaluating Epoch:  59%|#033[32m█████▉    #033[0m| 33/56 [01:55<01:20,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▉    #033[0m| 33/56 [01:55<01:20,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▉    #033[0m| 33/56 [01:55<01:20,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 32/56 [01:52<01:23,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 32/56 [01:52<01:24,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 32/56 [01:52<01:24,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 32/56 [01:52<01:24,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 34/56 [01:58<01:16,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 34/56 [01:58<01:16,  3.49s/it]#015evaluating Epoch:  61%|#033[32m██████    #033[0m| 34/56 [01:58<01:16,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 34/56 [01:58<01:16,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  59%|#033[32m█████▉    #033[0m| 33/56 [01:55<01:20,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  59%|#033[32m█████▉    #033[0m| 33/56 [01:55<01:20,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  59%|#033[32m█████▉    #033[0m| 33/56 [01:55<01:20,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  59%|#033[32m█████▉    #033[0m| 33/56 [01:55<01:20,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▎   #033[0m| 35/56 [02:02<01:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▎   #033[0m| 35/56 [02:02<01:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▎   #033[0m| 35/56 [02:02<01:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▎   #033[0m| 35/56 [02:02<01:13,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 34/56 [01:59<01:16,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 34/56 [01:59<01:16,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 34/56 [01:59<01:16,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 34/56 [01:59<01:16,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 36/56 [02:05<01:09,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 36/56 [02:05<01:09,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 36/56 [02:05<01:09,  3.49s/it]#015evaluating Epoch:  64%|#033[32m██████▍   #033[0m| 36/56 [02:05<01:09,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  62%|#033[32m██████▎   #033[0m| 35/56 [02:02<01:13,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  62%|#033[32m██████▎   #033[0m| 35/56 [02:02<01:13,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  62%|#033[32m██████▎   #033[0m| 35/56 [02:02<01:13,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  62%|#033[32m██████▎   #033[0m| 35/56 [02:02<01:13,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 37/56 [02:09<01:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 37/56 [02:09<01:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 37/56 [02:09<01:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 37/56 [02:09<01:06,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 36/56 [02:06<01:09,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 36/56 [02:06<01:09,  3.50s/it]#015evaluating Epoch:  64%|#033[32m██████▍   #033[0m| 36/56 [02:06<01:09,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 36/56 [02:06<01:09,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 38/56 [02:12<01:02,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 38/56 [02:12<01:02,  3.49s/it]#015evaluating Epoch:  68%|#033[32m██████▊   #033[0m| 38/56 [02:12<01:02,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 38/56 [02:12<01:02,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 37/56 [02:09<01:06,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 37/56 [02:09<01:06,  3.49s/it]#015evaluating Epoch:  66%|#033[32m██████▌   #033[0m| 37/56 [02:09<01:06,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  66%|#033[32m██████▌   #033[0m| 37/56 [02:09<01:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m██████▉   #033[0m| 39/56 [02:16<00:59,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m██████▉   #033[0m| 39/56 [02:16<00:59,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m██████▉   #033[0m| 39/56 [02:16<00:59,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m██████▉   #033[0m| 39/56 [02:16<00:59,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 38/56 [02:13<01:02,  3.50s/it]#015evaluating Epoch:  68%|#033[32m██████▊   #033[0m| 38/56 [02:13<01:02,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 38/56 [02:13<01:02,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 38/56 [02:13<01:02,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 40/56 [02:19<00:55,  3.49s/it]#015evaluating Epoch:  71%|#033[32m███████▏  #033[0m| 40/56 [02:19<00:55,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 40/56 [02:19<00:55,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 40/56 [02:19<00:55,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  70%|#033[32m██████▉   #033[0m| 39/56 [02:16<00:59,  3.50s/it]#015evaluating Epoch:  70%|#033[32m██████▉   #033[0m| 39/56 [02:16<00:59,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  70%|#033[32m██████▉   #033[0m| 39/56 [02:16<00:59,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  70%|#033[32m██████▉   #033[0m| 39/56 [02:16<00:59,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  73%|#033[32m███████▎  #033[0m| 41/56 [02:23<00:52,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  73%|#033[32m███████▎  #033[0m| 41/56 [02:23<00:52,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  73%|#033[32m███████▎  #033[0m| 41/56 [02:23<00:52,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  73%|#033[32m███████▎  #033[0m| 41/56 [02:23<00:52,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 40/56 [02:20<00:56,  3.50s/it]#015evaluating Epoch:  71%|#033[32m███████▏  #033[0m| 40/56 [02:20<00:56,  3.50s/it]#015evaluating Epoch:  71%|#033[32m███████▏  #033[0m| 40/56 [02:20<00:56,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 40/56 [02:20<00:56,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 42/56 [02:26<00:48,  3.49s/it]#015evaluating Epoch:  75%|#033[32m███████▌  #033[0m| 42/56 [02:26<00:48,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 42/56 [02:26<00:48,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 42/56 [02:26<00:48,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  73%|#033[32m███████▎  #033[0m| 41/56 [02:23<00:52,  3.50s/it]#015evaluating Epoch:  73%|#033[32m███████▎  #033[0m| 41/56 [02:23<00:52,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  73%|#033[32m███████▎  #033[0m| 41/56 [02:23<00:52,  3.50s/it]#015evaluating Epoch:  73%|#033[32m███████▎  #033[0m| 41/56 [02:23<00:52,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  77%|#033[32m███████▋  #033[0m| 43/56 [02:30<00:45,  3.49s/it]#015evaluating Epoch:  77%|#033[32m███████▋  #033[0m| 43/56 [02:30<00:45,  3.49s/it]#015evaluating Epoch:  77%|#033[32m███████▋  #033[0m| 43/56 [02:30<00:45,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  77%|#033[32m███████▋  #033[0m| 43/56 [02:30<00:45,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 42/56 [02:27<00:49,  3.50s/it]#015evaluating Epoch:  75%|#033[32m███████▌  #033[0m| 42/56 [02:27<00:49,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 42/56 [02:27<00:49,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 42/56 [02:27<00:49,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 44/56 [02:33<00:41,  3.49s/it]#015evaluating Epoch:  79%|#033[32m███████▊  #033[0m| 44/56 [02:33<00:41,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 44/56 [02:33<00:41,  3.49s/it]#015evaluating Epoch:  79%|#033[32m███████▊  #033[0m| 44/56 [02:33<00:41,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  77%|#033[32m███████▋  #033[0m| 43/56 [02:30<00:45,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  77%|#033[32m███████▋  #033[0m| 43/56 [02:30<00:45,  3.50s/it]#015evaluating Epoch:  77%|#033[32m███████▋  #033[0m| 43/56 [02:30<00:45,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  77%|#033[32m███████▋  #033[0m| 43/56 [02:30<00:45,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  80%|#033[32m████████  #033[0m| 45/56 [02:37<00:38,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  80%|#033[32m████████  #033[0m| 45/56 [02:37<00:38,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  80%|#033[32m████████  #033[0m| 45/56 [02:37<00:38,  3.49s/it]#015evaluating Epoch:  80%|#033[32m████████  #033[0m| 45/56 [02:37<00:38,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 44/56 [02:34<00:42,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 44/56 [02:34<00:42,  3.50s/it]#015evaluating Epoch:  79%|#033[32m███████▊  #033[0m| 44/56 [02:34<00:42,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 44/56 [02:34<00:42,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 46/56 [02:40<00:34,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 46/56 [02:40<00:34,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 46/56 [02:40<00:34,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 46/56 [02:40<00:34,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  80%|#033[32m████████  #033[0m| 45/56 [02:37<00:38,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  80%|#033[32m████████  #033[0m| 45/56 [02:37<00:38,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  80%|#033[32m████████  #033[0m| 45/56 [02:37<00:38,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  80%|#033[32m████████  #033[0m| 45/56 [02:37<00:38,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  84%|#033[32m████████▍ #033[0m| 47/56 [02:44<00:31,  3.49s/it]#015evaluating Epoch:  84%|#033[32m████████▍ #033[0m| 47/56 [02:44<00:31,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  84%|#033[32m████████▍ #033[0m| 47/56 [02:44<00:31,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  84%|#033[32m████████▍ #033[0m| 47/56 [02:44<00:31,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 46/56 [02:41<00:35,  3.50s/it]#015evaluating Epoch:  82%|#033[32m████████▏ #033[0m| 46/56 [02:41<00:35,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 46/56 [02:41<00:35,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 46/56 [02:41<00:35,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 48/56 [02:47<00:27,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 48/56 [02:47<00:27,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 48/56 [02:47<00:27,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 48/56 [02:47<00:27,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  84%|#033[32m████████▍ #033[0m| 47/56 [02:44<00:31,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  84%|#033[32m████████▍ #033[0m| 47/56 [02:44<00:31,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  84%|#033[32m████████▍ #033[0m| 47/56 [02:44<00:31,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  84%|#033[32m████████▍ #033[0m| 47/56 [02:44<00:31,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  88%|#033[32m████████▊ #033[0m| 49/56 [02:51<00:24,  3.49s/it]#015evaluating Epoch:  88%|#033[32m████████▊ #033[0m| 49/56 [02:51<00:24,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  88%|#033[32m████████▊ #033[0m| 49/56 [02:51<00:24,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  88%|#033[32m████████▊ #033[0m| 49/56 [02:51<00:24,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 48/56 [02:48<00:28,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 48/56 [02:48<00:28,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 48/56 [02:48<00:28,  3.50s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 48/56 [02:48<00:28,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 50/56 [02:54<00:20,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 50/56 [02:54<00:20,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 50/56 [02:54<00:20,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 50/56 [02:54<00:20,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  88%|#033[32m████████▊ #033[0m| 49/56 [02:51<00:24,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  88%|#033[32m████████▊ #033[0m| 49/56 [02:51<00:24,  3.51s/it]#015evaluating Epoch:  88%|#033[32m████████▊ #033[0m| 49/56 [02:51<00:24,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  88%|#033[32m████████▊ #033[0m| 49/56 [02:51<00:24,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████ #033[0m| 51/56 [02:58<00:17,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████ #033[0m| 51/56 [02:58<00:17,  3.49s/it]#015evaluating Epoch:  91%|#033[32m█████████ #033[0m| 51/56 [02:58<00:17,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████ #033[0m| 51/56 [02:58<00:17,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 50/56 [02:55<00:21,  3.51s/it]#015evaluating Epoch:  89%|#033[32m████████▉ #033[0m| 50/56 [02:55<00:21,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 50/56 [02:55<00:21,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 50/56 [02:55<00:21,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 52/56 [03:01<00:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 52/56 [03:01<00:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 52/56 [03:01<00:13,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 52/56 [03:01<00:13,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  91%|#033[32m█████████ #033[0m| 51/56 [02:58<00:17,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  91%|#033[32m█████████ #033[0m| 51/56 [02:58<00:17,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  91%|#033[32m█████████ #033[0m| 51/56 [02:58<00:17,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  91%|#033[32m█████████ #033[0m| 51/56 [02:58<00:17,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  95%|#033[32m█████████▍#033[0m| 53/56 [03:05<00:10,  3.49s/it]#015evaluating Epoch:  95%|#033[32m█████████▍#033[0m| 53/56 [03:05<00:10,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  95%|#033[32m█████████▍#033[0m| 53/56 [03:05<00:10,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  95%|#033[32m█████████▍#033[0m| 53/56 [03:05<00:10,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 52/56 [03:02<00:14,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 52/56 [03:02<00:14,  3.51s/it]#015evaluating Epoch:  93%|#033[32m█████████▎#033[0m| 52/56 [03:02<00:14,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 52/56 [03:02<00:14,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 54/56 [03:08<00:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 54/56 [03:08<00:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 54/56 [03:08<00:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 54/56 [03:08<00:06,  3.49s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  95%|#033[32m█████████▍#033[0m| 53/56 [03:05<00:10,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  95%|#033[32m█████████▍#033[0m| 53/56 [03:05<00:10,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  95%|#033[32m█████████▍#033[0m| 53/56 [03:05<00:10,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  95%|#033[32m█████████▍#033[0m| 53/56 [03:05<00:10,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  98%|#033[32m█████████▊#033[0m| 55/56 [03:12<00:03,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  98%|#033[32m█████████▊#033[0m| 55/56 [03:12<00:03,  3.48s/it]#015evaluating Epoch:  98%|#033[32m█████████▊#033[0m| 55/56 [03:12<00:03,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  98%|#033[32m█████████▊#033[0m| 55/56 [03:12<00:03,  3.48s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 54/56 [03:09<00:07,  3.51s/it]#015evaluating Epoch:  96%|#033[32m█████████▋#033[0m| 54/56 [03:09<00:07,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 54/56 [03:09<00:07,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 54/56 [03:09<00:07,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 56/56 [03:15<00:00,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 56/56 [03:15<00:00,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 56/56 [03:15<00:00,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 56/56 [03:15<00:00,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 56/56 [03:15<00:00,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 56/56 [03:15<00:00,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 56/56 [03:15<00:00,  3.49s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 56/56 [03:15<00:00,  3.49s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(3.6596, device='cuda:0') eval_epoch_loss=tensor(1.2974, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  98%|#033[32m█████████▊#033[0m| 55/56 [03:12<00:03,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  98%|#033[32m█████████▊#033[0m| 55/56 [03:12<00:03,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  98%|#033[32m█████████▊#033[0m| 55/56 [03:12<00:03,  3.51s/it]\u001b[0m\n",
      "\u001b[35mevaluating Epoch:  98%|#033[32m█████████▊#033[0m| 55/56 [03:12<00:03,  3.51s/it]\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 0 is 1.2973655462265015\u001b[0m\n",
      "\u001b[34mEpoch 1: train_perplexity=3.8823, train_epoch_loss=1.3564, epcoh time 543.811855374s\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/55 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/55 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/55 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/55 [00:00<?, ?it/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=\"ml.g5.12xlarge\",\n",
    "    instance_count=2,\n",
    "    environment={\"accept_eula\": \"true\"}\n",
    ")\n",
    "\n",
    "# By default, instruction tuning is set to false. Thus, to use instruction tuning dataset you use\n",
    "estimator.set_hyperparameters(instruction_tuned=\"True\", \n",
    "                              epoch=\"5\", \n",
    "                              max_input_length=\"1024\")\n",
    "estimator.fit({\"training\": train_data_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9decbf-08c6-4cb4-8644-4a96afb5bebf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy the fine-tuned model\n",
    "---\n",
    "Next, we deploy fine-tuned model. We will compare the performance of fine-tuned and pre-trained model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016e591b-63f8-4e0f-941c-4b4e0b9dc6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_predictor = estimator.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb57904a-9631-45fe-bc3f-ae2fbb992960",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluate the pre-trained and fine-tuned model\n",
    "---\n",
    "Next, we use the test data to evaluate the performance of the fine-tuned model and compare it with the pre-trained model. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87085bf6-dc7e-46f3-8563-d2e4aafd0820",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "test_dataset = train_and_test_dataset[\"test\"]\n",
    "\n",
    "inputs, ground_truth_responses, responses_before_finetuning, responses_after_finetuning = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "\n",
    "\n",
    "def predict_and_print(datapoint):\n",
    "    # For instruction fine-tuning, we insert a special key between input and output\n",
    "    input_output_demarkation_key = \"\\n\\n### Response:\\n\"\n",
    "\n",
    "    prompt = \"What is the best way to deploy a generative model on AWS?\"\n",
    "    \n",
    "    payload = {\n",
    "        #\"inputs\": template[\"prompt\"].format(\n",
    "        \"inputs\": prompt.format(\n",
    "            instruction=datapoint[\"instruction\"], context=datapoint[\"context\"]\n",
    "        )\n",
    "        + input_output_demarkation_key,\n",
    "        \"parameters\": {\"max_new_tokens\": 100},\n",
    "    }\n",
    "    inputs.append(payload[\"inputs\"])\n",
    "    ground_truth_responses.append(datapoint[\"response\"])\n",
    "    # Please change the following line to \"accept_eula=True\"\n",
    "    pretrained_response = pretrained_predictor.predict(\n",
    "        payload, custom_attributes=\"accept_eula=true\"\n",
    "    )\n",
    "    responses_before_finetuning.append(pretrained_response[0][\"generation\"])\n",
    "    # Please change the following line to \"accept_eula=True\"\n",
    "    finetuned_response = finetuned_predictor.predict(payload, custom_attributes=\"accept_eula=true\")\n",
    "    responses_after_finetuning.append(finetuned_response[0][\"generation\"])\n",
    "\n",
    "\n",
    "try:\n",
    "    for i, datapoint in enumerate(test_dataset.select(range(5))):\n",
    "        predict_and_print(datapoint)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Inputs\": inputs,\n",
    "            \"Ground Truth\": ground_truth_responses,\n",
    "            \"Response from non-finetuned model\": responses_before_finetuning,\n",
    "            \"Response from fine-tuned model\": responses_after_finetuning,\n",
    "        }\n",
    "    )\n",
    "    display(HTML(df.to_html()))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b0a0f5-ef34-40db-8ab7-c24a5d14b525",
   "metadata": {},
   "source": [
    "### Clean up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73ab2da-d00f-46db-90eb-81812898653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Delete resources\n",
    "# pretrained_predictor.delete_model()\n",
    "# pretrained_predictor.delete_endpoint()\n",
    "# finetuned_predictor.delete_model()\n",
    "# finetuned_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759ce98f-a35a-4c64-9fae-50894b5e9f37",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1c8c86-bfe2-4828-a7aa-dbd7a5ee075f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Supported Inference Parameters\n",
    "\n",
    "---\n",
    "This model supports the following inference payload parameters:\n",
    "\n",
    "* **max_new_tokens:** Model generates text until the output length (excluding the input context length) reaches max_new_tokens. If specified, it must be a positive integer.\n",
    "* **temperature:** Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If `temperature` -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "* **top_p:** In each step of text generation, sample from the smallest possible set of words with cumulative probability `top_p`. If specified, it must be a float between 0 and 1.\n",
    "* **return_full_text:** If True, input text will be part of the output generated text. If specified, it must be boolean. The default value for it is False.\n",
    "\n",
    "You may specify any subset of the parameters mentioned above while invoking an endpoint. \n",
    "\n",
    "\n",
    "### Notes\n",
    "- If `max_new_tokens` is not defined, the model may generate up to the maximum total tokens allowed, which is 4K for these models. This may result in endpoint query timeout errors, so it is recommended to set `max_new_tokens` when possible. For 7B, 13B, and 70B models, we recommend to set `max_new_tokens` no greater than 1500, 1000, and 500 respectively, while keeping the total number of tokens less than 4K.\n",
    "- In order to support a 4k context length, this model has restricted query payloads to only utilize a batch size of 1. Payloads with larger batch sizes will receive an endpoint error prior to inference.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f7e4f8-970f-4a1d-b6ee-86bc77b8b9a9",
   "metadata": {},
   "source": [
    "### Supported Hyper-parameters for fine-tuning\n",
    "---\n",
    "- epoch: The number of passes that the fine-tuning algorithm takes through the training dataset. Must be an integer greater than 1. Default: 5\n",
    "- learning_rate: The rate at which the model weights are updated after working through each batch of training examples. Must be a positive float greater than 0. Default: 1e-4.\n",
    "- instruction_tuned: Whether to instruction-train the model or not. Must be 'True' or 'False'. Default: 'False'\n",
    "- per_device_train_batch_size: The batch size per GPU core/CPU for training. Must be a positive integer. Default: 4.\n",
    "- per_device_eval_batch_size: The batch size per GPU core/CPU for evaluation. Must be a positive integer. Default: 1\n",
    "- max_train_samples: For debugging purposes or quicker training, truncate the number of training examples to this value. Value -1 means using all of training samples. Must be a positive integer or -1. Default: -1. \n",
    "- max_val_samples: For debugging purposes or quicker training, truncate the number of validation examples to this value. Value -1 means using all of validation samples. Must be a positive integer or -1. Default: -1. \n",
    "- max_input_length: Maximum total input sequence length after tokenization. Sequences longer than this will be truncated. If -1, max_input_length is set to the minimum of 1024 and the maximum model length defined by the tokenizer. If set to a positive value, max_input_length is set to the minimum of the provided value and the model_max_length defined by the tokenizer. Must be a positive integer or -1. Default: -1. \n",
    "- validation_split_ratio: If validation channel is none, ratio of train-validation split from the train data. Must be between 0 and 1. Default: 0.2. \n",
    "- train_data_split_seed: If validation data is not present, this fixes the random splitting of the input training data to training and validation data used by the algorithm. Must be an integer. Default: 0.\n",
    "- preprocessing_num_workers: The number of processes to use for the preprocessing. If None, main process is used for preprocessing. Default: \"None\"\n",
    "- lora_r: Lora R. Must be a positive integer. Default: 8.\n",
    "- lora_alpha: Lora Alpha. Must be a positive integer. Default: 32\n",
    "- lora_dropout: Lora Dropout. must be a positive float between 0 and 1. Default: 0.05. \n",
    "- int8_quantization: If True, model is loaded with 8 bit precision for training. Default for 7B/13B: False. Default for 70B: True.\n",
    "- enable_fsdp: If True, training uses Fully Sharded Data Parallelism. Default for 7B/13B: True. Default for 70B: False.\n",
    "\n",
    "Note 1: int8_quantization is not supported with FSDP. Also, int8_quantization = 'False' and enable_fsdp = 'False' is not supported due to CUDA memory issues for any of the g5 family instances. Thus, we recommend setting exactly one of int8_quantization or enable_fsdp to be 'True'\n",
    "Note 2: Due to the size of the model, 70B model can not be fine-tuned with enable_fsdp = 'True' for any of the supported instance types.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b6d023-3487-4571-8b52-f332790c1ad7",
   "metadata": {},
   "source": [
    "### 4. Supported Instance types\n",
    "\n",
    "---\n",
    "We have tested our scripts on the following instances types:\n",
    "\n",
    "- 7B: ml.g5.12xlarge, nl.g5.24xlarge, ml.g5.48xlarge, ml.p3dn.24xlarge\n",
    "- 13B: ml.g5.24xlarge, ml.g5.48xlarge, ml.p3dn.24xlarge\n",
    "- 70B: ml.g5.48xlarge\n",
    "\n",
    "Other instance types may also work to fine-tune. Note: When using p3 instances, training will be done with 32 bit precision as bfloat16 is not supported on these instances. Thus, training job would consume double the amount of CUDA memory when training on p3 instances compared to g5 instances.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770d4350-cf4d-40a0-be1c-eba44efd33ab",
   "metadata": {},
   "source": [
    "### Few notes about the fine-tuning method\n",
    "\n",
    "---\n",
    "- Fine-tuning scripts are based on [this repo](https://github.com/facebookresearch/llama-recipes/tree/main). \n",
    "- Instruction tuning dataset is first converted into domain adaptation dataset format before fine-tuning. \n",
    "- Fine-tuning scripts utilize Fully Sharded Data Parallel (FSDP) as well as Low Rank Adaptation (LoRA) method fine-tuning the models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909ce841-3a2c-4c08-a102-b94148036a5a",
   "metadata": {},
   "source": [
    "### Studio Kernel Dead/Creating JumpStart Model from the training Job\n",
    "---\n",
    "Due to the size of the Llama 70B model, training job may take several hours and the studio kernel may die during the training phase. However, during this time, training is still running in SageMaker. If this happens, you can still deploy the endpoint using the training job name with the following code:\n",
    "\n",
    "How to find the training job name? Go to Console -> SageMaker -> Training -> Training Jobs -> Identify the training job name and substitute in the following cell. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa60a66-1c2f-42df-8079-191319e28a65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "# training_job_name = <<training_job_name>>\n",
    "\n",
    "# attached_estimator = JumpStartEstimator.attach(training_job_name, model_id)\n",
    "# attached_estimator.logs()\n",
    "# attached_estimator.deploy()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
